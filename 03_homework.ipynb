{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c678e33-7efc-47da-bd84-890daf4b5beb",
   "metadata": {},
   "source": [
    "# Задание 1 (5 балла)\n",
    "\n",
    "Имплементируйте алгоритм Леска (описание есть в семинаре) и оцените качество его работы на датасете `data/corpus_wsd_50k.txt`\n",
    "\n",
    "В качестве метрики близости вы должны попробовать два подхода:\n",
    "\n",
    "1) Jaccard score на множествах слов (определений и контекста)\n",
    "2) Cosine distance на эмбедингах sentence_transformers\n",
    "\n",
    "В качестве метрики используйте accuracy (% правильных ответов). Предсказывайте только многозначные слова в датасете\n",
    "\n",
    "Контекст вы можете определить самостоятельно (окно вокруг целевого слова или все предложение). Также можете поэкспериментировать с предобработкой для обоих методов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74454823",
   "metadata": {},
   "source": [
    "## 1. Загрузка и подготовка данных\n",
    "\n",
    "Сначала скачаем корпус и преобразуем его в формат, с которым будет удобно работать. Так как нам нужны разные способы представления текстов для разных методов, сделаем список предложений, каждое из которых будет храниться в словаре со всеми необходимыми элементами. Добавим также словесные определения из Wordnet для многозначных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c449bd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-10-02 13:09:02--  https://github.com/mannefedov/compling_nlp_hse_course/raw/refs/heads/master/data/corpus_wsd_50k.txt.zip\n",
      "Resolving github.com (github.com)... 140.82.121.4\n",
      "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/refs/heads/master/data/corpus_wsd_50k.txt.zip [following]\n",
      "--2025-10-02 13:09:03--  https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/refs/heads/master/data/corpus_wsd_50k.txt.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4723095 (4.5M) [application/zip]\n",
      "Saving to: ‘corpus_wsd_50k.txt.zip’\n",
      "\n",
      "corpus_wsd_50k.txt. 100%[===================>]   4.50M  12.0MB/s    in 0.4s    \n",
      "\n",
      "2025-10-02 13:09:04 (12.0 MB/s) - ‘corpus_wsd_50k.txt.zip’ saved [4723095/4723095]\n",
      "\n",
      "Archive:  corpus_wsd_50k.txt.zip\n",
      "  inflating: corpus_wsd_50k.txt      \n",
      "  inflating: __MACOSX/._corpus_wsd_50k.txt  \n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/mannefedov/compling_nlp_hse_course/raw/refs/heads/master/data/corpus_wsd_50k.txt.zip\n",
    "!unzip -o corpus_wsd_50k.txt.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54f84c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm\n",
    "import string\n",
    "\n",
    "\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "086d5602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружено 49453 предложений\n"
     ]
    }
   ],
   "source": [
    "with open('corpus_wsd_50k.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus = f.read().split('\\n\\n')\n",
    "\n",
    "print(f'Загружено {len(corpus)} предложений')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dcae254",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_corpus = []\n",
    "ambiguous_word_count = 0  # для статистики\n",
    "punctuation_to_clean = {\n",
    "    ' ,': ',',\n",
    "    ' .': '.',\n",
    "    ' ?': '?',\n",
    "    ' !': '!',\n",
    "    ' :': ':',\n",
    "    ' ;': ';'}\n",
    "\n",
    "for sent in corpus:\n",
    "    if not sent:\n",
    "        continue\n",
    "\n",
    "    sentence_data = {'tokens': []}\n",
    "    word_forms = []\n",
    "\n",
    "    lines = sent.split('\\n')\n",
    "    for line in lines:\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) != 3:\n",
    "            continue\n",
    "\n",
    "        sense_key, lemma, word_form = parts\n",
    "        word_forms.append(word_form)\n",
    "\n",
    "        token_info = {\n",
    "            'word_form': word_form,\n",
    "            'lemma': lemma,\n",
    "            'sense': None\n",
    "        }\n",
    "\n",
    "        if sense_key:\n",
    "            ambiguous_word_count += 1\n",
    "            try:\n",
    "                synset = wn.lemma_from_key(sense_key).synset()\n",
    "                token_info['sense'] = {\n",
    "                    'key': sense_key,\n",
    "                    'definition': synset.definition()\n",
    "                }\n",
    "            except nltk.corpus.reader.wordnet.WordNetError:\n",
    "                # Если ключ вдруг не нашёлся, значения не будет\n",
    "                token_info['sense'] = {\n",
    "                    'key': sense_key,\n",
    "                    'definition': ''\n",
    "                }\n",
    "\n",
    "        sentence_data['tokens'].append(token_info)\n",
    "\n",
    "    # Собираем текст всего предложения\n",
    "    text = ' '.join(word_forms)\n",
    "    # Удаляем лишние пробелы перед пунктуацией\n",
    "    for punc_with_space, punc in punctuation_to_clean.items():\n",
    "        text = text.replace(punc_with_space, punc)\n",
    "    sentence_data['text'] = text\n",
    "\n",
    "    processed_corpus.append(sentence_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc6d8603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего предложений: 49452\n",
      "Всего неоднозначных слов: 239913\n"
     ]
    }
   ],
   "source": [
    "# Проверяем, что получилось\n",
    "\n",
    "print(f\"Всего предложений: {len(processed_corpus)}\")\n",
    "print(f\"Всего неоднозначных слов: {ambiguous_word_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bd806f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'How long has it been since you reviewed the objectives of your '\n",
      "          'benefit and service program?',\n",
      "  'tokens': [{'lemma': 'how', 'sense': None, 'word_form': 'How'},\n",
      "             {'lemma': 'long',\n",
      "              'sense': {'definition': 'primarily temporal sense; being or '\n",
      "                                      'indicating a relatively great or '\n",
      "                                      'greater than average duration or '\n",
      "                                      'passage of time or a duration as '\n",
      "                                      'specified',\n",
      "                        'key': 'long%3:00:02::'},\n",
      "              'word_form': 'long'},\n",
      "             {'lemma': 'have', 'sense': None, 'word_form': 'has'},\n",
      "             {'lemma': 'it', 'sense': None, 'word_form': 'it'},\n",
      "             {'lemma': 'be',\n",
      "              'sense': {'definition': 'have the quality of being; (copula, '\n",
      "                                      'used with an adjective or a predicate '\n",
      "                                      'noun)',\n",
      "                        'key': 'be%2:42:03::'},\n",
      "              'word_form': 'been'},\n",
      "             {'lemma': 'since', 'sense': None, 'word_form': 'since'},\n",
      "             {'lemma': 'you', 'sense': None, 'word_form': 'you'},\n",
      "             {'lemma': 'review',\n",
      "              'sense': {'definition': 'look at again; examine again',\n",
      "                        'key': 'review%2:31:00::'},\n",
      "              'word_form': 'reviewed'},\n",
      "             {'lemma': 'the', 'sense': None, 'word_form': 'the'},\n",
      "             {'lemma': 'objective',\n",
      "              'sense': {'definition': 'the goal intended to be attained (and '\n",
      "                                      'which is believed to be attainable)',\n",
      "                        'key': 'objective%1:09:00::'},\n",
      "              'word_form': 'objectives'},\n",
      "             {'lemma': 'of', 'sense': None, 'word_form': 'of'},\n",
      "             {'lemma': 'you', 'sense': None, 'word_form': 'your'},\n",
      "             {'lemma': 'benefit',\n",
      "              'sense': {'definition': 'financial assistance in time of need',\n",
      "                        'key': 'benefit%1:21:00::'},\n",
      "              'word_form': 'benefit'},\n",
      "             {'lemma': 'and', 'sense': None, 'word_form': 'and'},\n",
      "             {'lemma': 'service',\n",
      "              'sense': {'definition': 'employment in or work for another',\n",
      "                        'key': 'service%1:04:07::'},\n",
      "              'word_form': 'service'},\n",
      "             {'lemma': 'program',\n",
      "              'sense': {'definition': 'a system of projects or services '\n",
      "                                      'intended to meet a public need',\n",
      "                        'key': 'program%1:09:01::'},\n",
      "              'word_form': 'program'},\n",
      "             {'lemma': '?', 'sense': None, 'word_form': '?'}]},\n",
      " {'text': 'Have you permitted it to become a giveaway program rather than one '\n",
      "          'that has the goal of improved employee morale and, consequently, '\n",
      "          'increased productivity?',\n",
      "  'tokens': [{'lemma': 'have', 'sense': None, 'word_form': 'Have'},\n",
      "             {'lemma': 'you', 'sense': None, 'word_form': 'you'},\n",
      "             {'lemma': 'permit',\n",
      "              'sense': {'definition': 'make it possible through a specific '\n",
      "                                      'action or lack of action for something '\n",
      "                                      'to happen',\n",
      "                        'key': 'permit%2:41:00::'},\n",
      "              'word_form': 'permitted'},\n",
      "             {'lemma': 'it', 'sense': None, 'word_form': 'it'},\n",
      "             {'lemma': 'to', 'sense': None, 'word_form': 'to'},\n",
      "             {'lemma': 'become',\n",
      "              'sense': {'definition': 'undergo a change or development',\n",
      "                        'key': 'become%2:42:01::'},\n",
      "              'word_form': 'become'},\n",
      "             {'lemma': 'a', 'sense': None, 'word_form': 'a'},\n",
      "             {'lemma': 'giveaway',\n",
      "              'sense': {'definition': 'a gift of public land or resources for '\n",
      "                                      'the private gain of a limited group',\n",
      "                        'key': 'giveaway%1:21:00::'},\n",
      "              'word_form': 'giveaway'},\n",
      "             {'lemma': 'program',\n",
      "              'sense': {'definition': 'a system of projects or services '\n",
      "                                      'intended to meet a public need',\n",
      "                        'key': 'program%1:09:01::'},\n",
      "              'word_form': 'program'},\n",
      "             {'lemma': 'rather',\n",
      "              'sense': {'definition': 'on the contrary',\n",
      "                        'key': 'rather%4:02:02::'},\n",
      "              'word_form': 'rather'},\n",
      "             {'lemma': 'than', 'sense': None, 'word_form': 'than'},\n",
      "             {'lemma': 'one', 'sense': None, 'word_form': 'one'},\n",
      "             {'lemma': 'that', 'sense': None, 'word_form': 'that'},\n",
      "             {'lemma': 'have',\n",
      "              'sense': {'definition': 'have as a feature',\n",
      "                        'key': 'have%2:42:00::'},\n",
      "              'word_form': 'has'},\n",
      "             {'lemma': 'the', 'sense': None, 'word_form': 'the'},\n",
      "             {'lemma': 'goal',\n",
      "              'sense': {'definition': 'the state of affairs that a plan is '\n",
      "                                      'intended to achieve and that (when '\n",
      "                                      'achieved) terminates behavior intended '\n",
      "                                      'to achieve it',\n",
      "                        'key': 'goal%1:09:00::'},\n",
      "              'word_form': 'goal'},\n",
      "             {'lemma': 'of', 'sense': None, 'word_form': 'of'},\n",
      "             {'lemma': 'improved',\n",
      "              'sense': {'definition': 'made more desirable or valuable or '\n",
      "                                      'profitable; especially made ready for '\n",
      "                                      'use or marketing',\n",
      "                        'key': 'improved%3:00:00::'},\n",
      "              'word_form': 'improved'},\n",
      "             {'lemma': 'employee',\n",
      "              'sense': {'definition': 'a worker who is hired to perform a job',\n",
      "                        'key': 'employee%1:18:00::'},\n",
      "              'word_form': 'employee'},\n",
      "             {'lemma': 'morale',\n",
      "              'sense': {'definition': 'a state of individual psychological '\n",
      "                                      'well-being based upon a sense of '\n",
      "                                      'confidence and usefulness and purpose',\n",
      "                        'key': 'morale%1:26:00::'},\n",
      "              'word_form': 'morale'},\n",
      "             {'lemma': 'and', 'sense': None, 'word_form': 'and'},\n",
      "             {'lemma': ',', 'sense': None, 'word_form': ','},\n",
      "             {'lemma': 'consequently',\n",
      "              'sense': {'definition': '(sentence connectors) because of the '\n",
      "                                      'reason given',\n",
      "                        'key': 'consequently%4:02:00::'},\n",
      "              'word_form': 'consequently'},\n",
      "             {'lemma': ',', 'sense': None, 'word_form': ','},\n",
      "             {'lemma': 'increased',\n",
      "              'sense': {'definition': 'made greater in size or amount or '\n",
      "                                      'degree',\n",
      "                        'key': 'increased%3:00:00::'},\n",
      "              'word_form': 'increased'},\n",
      "             {'lemma': 'productivity',\n",
      "              'sense': {'definition': 'the quality of being productive or '\n",
      "                                      'having the power to produce',\n",
      "                        'key': 'productivity%1:07:00::'},\n",
      "              'word_form': 'productivity'},\n",
      "             {'lemma': '?', 'sense': None, 'word_form': '?'}]},\n",
      " {'text': 'What effort do you make to assess results of your program?',\n",
      "  'tokens': [{'lemma': 'what', 'sense': None, 'word_form': 'What'},\n",
      "             {'lemma': 'effort',\n",
      "              'sense': {'definition': 'earnest and conscientious activity '\n",
      "                                      'intended to do or accomplish something',\n",
      "                        'key': 'effort%1:04:00::'},\n",
      "              'word_form': 'effort'},\n",
      "             {'lemma': 'do', 'sense': None, 'word_form': 'do'},\n",
      "             {'lemma': 'you', 'sense': None, 'word_form': 'you'},\n",
      "             {'lemma': 'make',\n",
      "              'sense': {'definition': 'engage in', 'key': 'make%2:41:00::'},\n",
      "              'word_form': 'make'},\n",
      "             {'lemma': 'to', 'sense': None, 'word_form': 'to'},\n",
      "             {'lemma': 'assess',\n",
      "              'sense': {'definition': 'evaluate or estimate the nature, '\n",
      "                                      'quality, ability, extent, or '\n",
      "                                      'significance of',\n",
      "                        'key': 'assess%2:31:00::'},\n",
      "              'word_form': 'assess'},\n",
      "             {'lemma': 'result',\n",
      "              'sense': {'definition': 'something that results',\n",
      "                        'key': 'result%1:11:00::'},\n",
      "              'word_form': 'results'},\n",
      "             {'lemma': 'of', 'sense': None, 'word_form': 'of'},\n",
      "             {'lemma': 'you', 'sense': None, 'word_form': 'your'},\n",
      "             {'lemma': 'program',\n",
      "              'sense': {'definition': 'a system of projects or services '\n",
      "                                      'intended to meet a public need',\n",
      "                        'key': 'program%1:09:01::'},\n",
      "              'word_form': 'program'},\n",
      "             {'lemma': '?', 'sense': None, 'word_form': '?'}]},\n",
      " {'text': 'Do you measure its relation to reduced absenteeism, turnover, '\n",
      "          'accidents, and grievances, and to improved quality and output?',\n",
      "  'tokens': [{'lemma': 'do', 'sense': None, 'word_form': 'Do'},\n",
      "             {'lemma': 'you', 'sense': None, 'word_form': 'you'},\n",
      "             {'lemma': 'measure',\n",
      "              'sense': {'definition': 'evaluate or estimate the nature, '\n",
      "                                      'quality, ability, extent, or '\n",
      "                                      'significance of',\n",
      "                        'key': 'measure%2:31:01::'},\n",
      "              'word_form': 'measure'},\n",
      "             {'lemma': 'its', 'sense': None, 'word_form': 'its'},\n",
      "             {'lemma': 'relation', 'sense': None, 'word_form': 'relation'},\n",
      "             {'lemma': 'to', 'sense': None, 'word_form': 'to'},\n",
      "             {'lemma': 'reduced',\n",
      "              'sense': {'definition': 'made less in size or amount or degree',\n",
      "                        'key': 'reduced%3:00:04::'},\n",
      "              'word_form': 'reduced'},\n",
      "             {'lemma': 'absenteeism',\n",
      "              'sense': {'definition': 'habitual absence from work',\n",
      "                        'key': 'absenteeism%1:04:00::'},\n",
      "              'word_form': 'absenteeism'},\n",
      "             {'lemma': ',', 'sense': None, 'word_form': ','},\n",
      "             {'lemma': 'turnover',\n",
      "              'sense': {'definition': 'the ratio of the number of workers that '\n",
      "                                      'had to be replaced in a given time '\n",
      "                                      'period to the average number of workers',\n",
      "                        'key': 'turnover%1:24:00::'},\n",
      "              'word_form': 'turnover'},\n",
      "             {'lemma': ',', 'sense': None, 'word_form': ','},\n",
      "             {'lemma': 'accident',\n",
      "              'sense': {'definition': 'an unfortunate mishap; especially one '\n",
      "                                      'causing damage or injury',\n",
      "                        'key': 'accident%1:11:01::'},\n",
      "              'word_form': 'accidents'},\n",
      "             {'lemma': ',', 'sense': None, 'word_form': ','},\n",
      "             {'lemma': 'and', 'sense': None, 'word_form': 'and'},\n",
      "             {'lemma': 'grievance',\n",
      "              'sense': {'definition': 'an allegation that something imposes an '\n",
      "                                      'illegal obligation or denies some legal '\n",
      "                                      'right or causes injustice',\n",
      "                        'key': 'grievance%1:10:01::'},\n",
      "              'word_form': 'grievances'},\n",
      "             {'lemma': ',', 'sense': None, 'word_form': ','},\n",
      "             {'lemma': 'and', 'sense': None, 'word_form': 'and'},\n",
      "             {'lemma': 'to', 'sense': None, 'word_form': 'to'},\n",
      "             {'lemma': 'improved',\n",
      "              'sense': {'definition': 'become or made better in quality',\n",
      "                        'key': 'improved%5:00:00:better:00'},\n",
      "              'word_form': 'improved'},\n",
      "             {'lemma': 'quality',\n",
      "              'sense': {'definition': 'an essential and distinguishing '\n",
      "                                      'attribute of something or someone; '\n",
      "                                      '--Shakespeare',\n",
      "                        'key': 'quality%1:07:00::'},\n",
      "              'word_form': 'quality'},\n",
      "             {'lemma': 'and', 'sense': None, 'word_form': 'and'},\n",
      "             {'lemma': 'output',\n",
      "              'sense': {'definition': 'production of a certain amount',\n",
      "                        'key': 'output%1:04:00::'},\n",
      "              'word_form': 'output'},\n",
      "             {'lemma': '?', 'sense': None, 'word_form': '?'}]},\n",
      " {'text': 'Have you set specific objectives for your employee publication?',\n",
      "  'tokens': [{'lemma': 'have', 'sense': None, 'word_form': 'Have'},\n",
      "             {'lemma': 'you', 'sense': None, 'word_form': 'you'},\n",
      "             {'lemma': 'set',\n",
      "              'sense': {'definition': 'decide upon or fix definitely',\n",
      "                        'key': 'set%2:32:00::'},\n",
      "              'word_form': 'set'},\n",
      "             {'lemma': 'specific',\n",
      "              'sense': {'definition': \"(sometimes followed by `to') applying \"\n",
      "                                      'to or characterized by or '\n",
      "                                      'distinguishing something particular or '\n",
      "                                      'special or unique',\n",
      "                        'key': 'specific%3:00:00::'},\n",
      "              'word_form': 'specific'},\n",
      "             {'lemma': 'objective',\n",
      "              'sense': {'definition': 'the goal intended to be attained (and '\n",
      "                                      'which is believed to be attainable)',\n",
      "                        'key': 'objective%1:09:00::'},\n",
      "              'word_form': 'objectives'},\n",
      "             {'lemma': 'for', 'sense': None, 'word_form': 'for'},\n",
      "             {'lemma': 'you', 'sense': None, 'word_form': 'your'},\n",
      "             {'lemma': 'employee',\n",
      "              'sense': {'definition': 'a worker who is hired to perform a job',\n",
      "                        'key': 'employee%1:18:00::'},\n",
      "              'word_form': 'employee'},\n",
      "             {'lemma': 'publication',\n",
      "              'sense': {'definition': 'a copy of a printed work offered for '\n",
      "                                      'distribution',\n",
      "                        'key': 'publication%1:10:00::'},\n",
      "              'word_form': 'publication'},\n",
      "             {'lemma': '?', 'sense': None, 'word_form': '?'}]}]\n"
     ]
    }
   ],
   "source": [
    "# Примеры\n",
    "pprint(processed_corpus[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf3771a",
   "metadata": {},
   "source": [
    "## 2. Алгоритм Леска с мерой Жаккарта\n",
    "\n",
    "Здесь поэкспериментируем с разным размером окна, а также с удалением стоп-слов. Кроме того, проверим, что работает лучше: леммы или словоформы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03e76d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c9b09fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(set1, set2):\n",
    "    \"\"\"Вычисляет меру Джаккарда между двумя наборами слов.\"\"\"\n",
    "    if not set1 and not set2:\n",
    "        return 0.0\n",
    "\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "\n",
    "    return len(intersection) / len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6701fc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disambiguate_sentence_jaccard(tokens,\n",
    "                                  remove_stop_words=False,\n",
    "                                  window_size=None,\n",
    "                                  use_lemmas=False):\n",
    "    \"\"\"\n",
    "    Предсказывает значение для многозначных слов в предложении с использованием алгоритма Леска\n",
    "    и меры Джаккарда.\n",
    "    \"\"\"\n",
    "\n",
    "    def _preprocess_definition(text, remove_stops=False, lemmatize=False):\n",
    "        # Внутренняя функция для предобработки текста определения\n",
    "        # Токенизация, очистка от пунктуации\n",
    "        text_tokens = nltk.word_tokenize(text)\n",
    "        clean_tokens = [\n",
    "            token.lower() for token in text_tokens if token not in punctuation]\n",
    "\n",
    "        # Лемматизация (если требуется)\n",
    "        if lemmatize:\n",
    "            processed_tokens = [lemmatizer.lemmatize(\n",
    "                token) for token in clean_tokens]\n",
    "        else:\n",
    "            processed_tokens = clean_tokens\n",
    "\n",
    "        # Удаление стоп-слов (если требуется)\n",
    "        if remove_stops:\n",
    "            processed_tokens = [\n",
    "                token for token in processed_tokens if token not in english_stopwords]\n",
    "\n",
    "        return set(processed_tokens)\n",
    "\n",
    "    # Проходим по всем токенам в предложении\n",
    "    for i, token in enumerate(tokens):\n",
    "        # Обрабатываем только многозначные слова\n",
    "        if not (token.get('sense') and token['sense'].get('key')):\n",
    "            continue\n",
    "\n",
    "        # Определяем контекст\n",
    "        context_words = set()\n",
    "        token_key = 'lemma' if use_lemmas else 'word_form'\n",
    "\n",
    "        if window_size is None:\n",
    "            # Если размер окна не задан, контекст - все предложение\n",
    "            context_words_list = [\n",
    "                t[token_key].lower() for t in tokens if t[token_key] not in punctuation]\n",
    "            if remove_stop_words:\n",
    "                context_words_list = [\n",
    "                    w for w in context_words_list if w not in english_stopwords]\n",
    "            context_words = set(context_words_list) - \\\n",
    "                {token[token_key].lower()}\n",
    "        else:\n",
    "            # Если окно задано, сначала отбираем чистые слова, потом берем окно\n",
    "            context_words_list = []\n",
    "\n",
    "            # Идем влево от целевого слова, собирая n чистых слов\n",
    "            words_count_left = 0\n",
    "            for j in range(i - 1, -1, -1):\n",
    "                if words_count_left == window_size:\n",
    "                    break\n",
    "\n",
    "                word = tokens[j][token_key].lower()\n",
    "\n",
    "                if word in punctuation:\n",
    "                    continue\n",
    "                if remove_stop_words and word in english_stopwords:\n",
    "                    continue\n",
    "\n",
    "                context_words_list.append(word)\n",
    "                words_count_left += 1\n",
    "\n",
    "            # Идем вправо от целевого слова, собирая n чистых слов\n",
    "            words_count_right = 0\n",
    "            for j in range(i + 1, len(tokens)):\n",
    "                if words_count_right == window_size:\n",
    "                    break\n",
    "\n",
    "                word = tokens[j][token_key].lower()\n",
    "\n",
    "                if word in punctuation:\n",
    "                    continue\n",
    "                if remove_stop_words and word in english_stopwords:\n",
    "                    continue\n",
    "\n",
    "                context_words_list.append(word)\n",
    "                words_count_right += 1\n",
    "            context_words = set(context_words_list)\n",
    "\n",
    "        # Получаем все возможные значения для леммы\n",
    "        possible_synsets = wn.synsets(token['lemma'])\n",
    "        if not possible_synsets:\n",
    "            continue\n",
    "\n",
    "        best_sense = None\n",
    "        max_score = -1\n",
    "\n",
    "        # Находим ближайшее значение, сравнивая контекст с определениями\n",
    "        for synset in possible_synsets:\n",
    "            definition_words = _preprocess_definition(\n",
    "                synset.definition(),\n",
    "                remove_stops=remove_stop_words,\n",
    "                lemmatize=use_lemmas\n",
    "            )\n",
    "            score = jaccard_similarity(context_words, definition_words)\n",
    "\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                best_sense = synset\n",
    "\n",
    "        # Сохраняем предсказанное значение\n",
    "        if best_sense:\n",
    "            token['predicted_sense'] = {\n",
    "                'name': best_sense.name(),\n",
    "                'definition': best_sense.definition()\n",
    "            }\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fbbf71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_corpus_jaccard(\n",
    "        corpus,\n",
    "        remove_stop_words=False,\n",
    "        window_size=None,\n",
    "        use_lemmas=False):\n",
    "    \"\"\"\n",
    "    Применяет функцию дизамбигуации на основе Джаккарда для всех предложений в корпусе\n",
    "    \"\"\"\n",
    "    processed_with_predictions = []\n",
    "    for sentence in tqdm(\n",
    "            corpus,\n",
    "            desc=f\"Обработка (window={window_size}, stopwords={not remove_stop_words}, lemmas={use_lemmas})\"):\n",
    "        sentence_copy = {\n",
    "            'text': sentence['text'],\n",
    "            'tokens': [t.copy() for t in sentence['tokens']]\n",
    "        }\n",
    "\n",
    "        disambiguated_tokens = disambiguate_sentence_jaccard(\n",
    "            sentence_copy['tokens'],\n",
    "            remove_stop_words,\n",
    "            window_size,\n",
    "            use_lemmas\n",
    "        )\n",
    "        sentence_copy['tokens'] = disambiguated_tokens\n",
    "        processed_with_predictions.append(sentence_copy)\n",
    "\n",
    "    return processed_with_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4a27d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(corpus_with_predictions):\n",
    "    \"\"\"\n",
    "    Вычисляет accuracy и сохраняет примеры с ошибками\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    errors = []\n",
    "\n",
    "    for sentence in corpus_with_predictions:\n",
    "        for token in sentence['tokens']:\n",
    "            if token.get('sense') and token['sense'].get(\n",
    "                    'key') and token.get('predicted_sense'):\n",
    "                total += 1\n",
    "\n",
    "                gold_key = token['sense']['key']\n",
    "                predicted_name = token['predicted_sense']['name']\n",
    "\n",
    "                try:\n",
    "                    gold_synset = wn.lemma_from_key(gold_key).synset()\n",
    "                    predicted_synset = wn.synset(predicted_name)\n",
    "\n",
    "                    if gold_synset == predicted_synset:\n",
    "                        correct += 1\n",
    "                    else:\n",
    "                        errors.append({\n",
    "                            'text': sentence['text'],\n",
    "                            'word': token['word_form'],\n",
    "                            'lemma': token['lemma'],\n",
    "                            'gold_sense': {'key': gold_key, 'definition': token['sense']['definition']},\n",
    "                            'predicted_sense': token['predicted_sense']\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return {'average_accuracy': accuracy, 'errors': errors}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e98b62b",
   "metadata": {},
   "source": [
    "### Эксперимент 1: всё предложение, не убираем стоп-слова и не лемматизируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dca078b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed0a9e22b2644cc5a715491b3660b2cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Обработка (window=None, stopwords=True, lemmas=False):   0%|          | 0/49452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (полный контекст, со стоп-словами, без лемматизации): 0.3025\n"
     ]
    }
   ],
   "source": [
    "corpus_pred_1 = process_corpus_jaccard(\n",
    "    processed_corpus,\n",
    "    remove_stop_words=False,\n",
    "    window_size=None,\n",
    "    use_lemmas=False)\n",
    "results_1 = evaluate(corpus_pred_1)\n",
    "print(\n",
    "    f\"Accuracy (полный контекст, со стоп-словами, без лемматизации): {results_1['average_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f09a99",
   "metadata": {},
   "source": [
    "### Эксперимент 2: всё предложение, убираем стоп-слова, не лемматизируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b71261d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4b10e9801549149610bac0f64f27a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Обработка (window=None, stopwords=False, lemmas=False):   0%|          | 0/49452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (полный контекст, без стоп-слов, без лемматизации): 0.4532\n"
     ]
    }
   ],
   "source": [
    "corpus_pred_2 = process_corpus_jaccard(\n",
    "    processed_corpus,\n",
    "    remove_stop_words=True,\n",
    "    window_size=None,\n",
    "    use_lemmas=False)\n",
    "results_2 = evaluate(corpus_pred_2)\n",
    "print(\n",
    "    f\"Accuracy (полный контекст, без стоп-слов, без лемматизации): {results_2['average_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fa4f35",
   "metadata": {},
   "source": [
    "### Эксперимент 3: всё предложение, убираем стоп-слова, лемматизируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51f9ea5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68abeb19d6d14fdaad1fbb6b767c8554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Обработка (window=None, stopwords=False, lemmas=True):   0%|          | 0/49452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (полный контекст, без стоп-слов, с лемматизацией): 0.4532\n"
     ]
    }
   ],
   "source": [
    "corpus_pred_3 = process_corpus_jaccard(\n",
    "    processed_corpus,\n",
    "    remove_stop_words=True,\n",
    "    window_size=None,\n",
    "    use_lemmas=True)\n",
    "results_3 = evaluate(corpus_pred_3)\n",
    "print(\n",
    "    f\"Accuracy (полный контекст, без стоп-слов, с лемматизацией): {results_2['average_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad0fcfa",
   "metadata": {},
   "source": [
    "### Эксперименты с разным размером контекстного окна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b448a5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8bb56a9a304754af1f26b5a4fd23d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Обработка (window=3, stopwords=True, lemmas=False):   0%|          | 0/49452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (window=3, remove_stopwords=False, use_lemmas=False): 0.3366\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5046ca06fcb74c5db853c6ed966278f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Обработка (window=3, stopwords=True, lemmas=True):   0%|          | 0/49452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (window=3, remove_stopwords=False, use_lemmas=True): 0.3338\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca62e9710e44769a1d2d6fe094ac7da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Обработка (window=3, stopwords=False, lemmas=False):   0%|          | 0/49452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (window=3, remove_stopwords=True, use_lemmas=False): 0.4659\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83842f32133940fe90bec567f05b65e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Обработка (window=3, stopwords=False, lemmas=True):   0%|          | 0/49452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (window=3, remove_stopwords=True, use_lemmas=True): 0.4595\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "826b53ff55064a19bb59899ab77866b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Обработка (window=4, stopwords=True, lemmas=False):   0%|          | 0/49452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (window=4, remove_stopwords=False, use_lemmas=False): 0.3268\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353f6d1c763d47dca053cce018b315e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Обработка (window=4, stopwords=True, lemmas=True):   0%|          | 0/49452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (window=4, remove_stopwords=False, use_lemmas=True): 0.3238\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6871149a464b299126440e517fcac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Обработка (window=4, stopwords=False, lemmas=False):   0%|          | 0/49452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (window=4, remove_stopwords=True, use_lemmas=False): 0.4630\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35dd001f8a4a4d848569abd6dfd711a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Обработка (window=4, stopwords=False, lemmas=True):   0%|          | 0/49452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (window=4, remove_stopwords=True, use_lemmas=True): 0.4550\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "144f19f0e12a40dabd0ce154ac3db824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Обработка (window=5, stopwords=True, lemmas=False):   0%|          | 0/49452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (window=5, remove_stopwords=False, use_lemmas=False): 0.3205\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c72222d20d4c3c8ea7137a7ad8a770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Обработка (window=5, stopwords=True, lemmas=True):   0%|          | 0/49452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (window=5, remove_stopwords=False, use_lemmas=True): 0.3173\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6635b111e71f4ebc82a5fef996d0f614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Обработка (window=5, stopwords=False, lemmas=False):   0%|          | 0/49452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (window=5, remove_stopwords=True, use_lemmas=False): 0.4602\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2333a77900b5403287d2132a5b2e8072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Обработка (window=5, stopwords=False, lemmas=True):   0%|          | 0/49452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (window=5, remove_stopwords=True, use_lemmas=True): 0.4513\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da53acbedf04f46844cd8e19bac2b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Обработка (window=6, stopwords=True, lemmas=False):   0%|          | 0/49452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (window=6, remove_stopwords=False, use_lemmas=False): 0.3157\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2687fbc93c4429188a867a546f4c54a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Обработка (window=6, stopwords=True, lemmas=True):   0%|          | 0/49452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (window=6, remove_stopwords=False, use_lemmas=True): 0.3126\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44bd86696ed24914b3968a9315417442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Обработка (window=6, stopwords=False, lemmas=False):   0%|          | 0/49452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (window=6, remove_stopwords=True, use_lemmas=False): 0.4582\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9941592b7934c6cb86b7c51b96ca8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Обработка (window=6, stopwords=False, lemmas=True):   0%|          | 0/49452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (window=6, remove_stopwords=True, use_lemmas=True): 0.4485\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fe54f16bc1e4ed8a26f6fe4869cd988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Обработка (window=7, stopwords=True, lemmas=False):   0%|          | 0/49452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (window=7, remove_stopwords=False, use_lemmas=False): 0.3126\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f70471337e4a3b9ed628b0fc20514a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Обработка (window=7, stopwords=True, lemmas=True):   0%|          | 0/49452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (window=7, remove_stopwords=False, use_lemmas=True): 0.3094\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12b4d41bb8f4e939369e033fb8d6792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Обработка (window=7, stopwords=False, lemmas=False):   0%|          | 0/49452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (window=7, remove_stopwords=True, use_lemmas=False): 0.4565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adaf65590e2447b18823914185270539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Обработка (window=7, stopwords=False, lemmas=True):   0%|          | 0/49452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (window=7, remove_stopwords=True, use_lemmas=True): 0.4464\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = -1\n",
    "best_params = {}\n",
    "best_results = {}\n",
    "\n",
    "for window in range(3, 8):\n",
    "    for remove_stops in [False, True]:\n",
    "        for use_lemmas in [False, True]:\n",
    "            corpus_pred = process_corpus_jaccard(\n",
    "                processed_corpus,\n",
    "                remove_stop_words=remove_stops,\n",
    "                window_size=window,\n",
    "                use_lemmas=use_lemmas)\n",
    "            results = evaluate(corpus_pred)\n",
    "            accuracy = results['average_accuracy']\n",
    "\n",
    "            print(\n",
    "                f\"Accuracy (window={window}, remove_stopwords={remove_stops}, use_lemmas={use_lemmas}): {accuracy:.4f}\")\n",
    "\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_params = {\n",
    "                    'window_size': window,\n",
    "                    'remove_stop_words': remove_stops,\n",
    "                    'use_lemmas': use_lemmas}\n",
    "                best_results = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90a1687f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Лучший эксперимент ---\n",
      "Лучшая Accuracy: 0.4659\n",
      "Лучшие параметры: {'window_size': 3, 'remove_stop_words': True, 'use_lemmas': False}\n",
      "\n",
      "--- 10 ошибок для лучшей модели ---\n",
      "Ошибка #1\n",
      "Контекст: ...How long has it been since you reviewed the objectives of your benefit and service program?...\n",
      "Слово: 'long' (лемма: 'long')\n",
      "  - Настоящее значение     : [long%3:00:02::] primarily temporal sense; being or indicating a relatively great or greater than average duration or passage of time or a duration as specified\n",
      "  - Предсказанное: [hanker.v.01] desire strongly or persistently\n",
      "\n",
      "Ошибка #2\n",
      "Контекст: ...How long has it been since you reviewed the objectives of your benefit and service program?...\n",
      "Слово: 'been' (лемма: 'be')\n",
      "  - Настоящее значение     : [be%2:42:03::] have the quality of being; (copula, used with an adjective or a predicate noun)\n",
      "  - Предсказанное: [beryllium.n.01] a light strong brittle grey toxic bivalent metallic element\n",
      "\n",
      "Ошибка #3\n",
      "Контекст: ...How long has it been since you reviewed the objectives of your benefit and service program?...\n",
      "Слово: 'reviewed' (лемма: 'review')\n",
      "  - Настоящее значение     : [review%2:31:00::] look at again; examine again\n",
      "  - Предсказанное: [review.n.04] (accounting) a service (less exhaustive than an audit) that provides some assurance to interested parties as to the reliability of financial data\n",
      "\n",
      "Ошибка #4\n",
      "Контекст: ...How long has it been since you reviewed the objectives of your benefit and service program?...\n",
      "Слово: 'service' (лемма: 'service')\n",
      "  - Настоящее значение     : [service%1:04:07::] employment in or work for another\n",
      "  - Предсказанное: [service.n.11] (law) the acts performed by an English feudal tenant for the benefit of his lord which formed the consideration for the property granted to him\n",
      "\n",
      "Ошибка #5\n",
      "Контекст: ...How long has it been since you reviewed the objectives of your benefit and service program?...\n",
      "Слово: 'program' (лемма: 'program')\n",
      "  - Настоящее значение     : [program%1:09:01::] a system of projects or services intended to meet a public need\n",
      "  - Предсказанное: [plan.n.01] a series of steps to be carried out or goals to be accomplished\n",
      "\n",
      "Ошибка #6\n",
      "Контекст: ...Have you permitted it to become a giveaway program rather than one that has the goal of improved employee morale and, consequently, increased productivity?...\n",
      "Слово: 'permitted' (лемма: 'permit')\n",
      "  - Настоящее значение     : [permit%2:41:00::] make it possible through a specific action or lack of action for something to happen\n",
      "  - Предсказанное: [license.n.01] a legal document giving official permission to do something\n",
      "\n",
      "Ошибка #7\n",
      "Контекст: ...Have you permitted it to become a giveaway program rather than one that has the goal of improved employee morale and, consequently, increased productivity?...\n",
      "Слово: 'become' (лемма: 'become')\n",
      "  - Настоящее значение     : [become%2:42:01::] undergo a change or development\n",
      "  - Предсказанное: [become.v.01] enter or assume a certain state or condition\n",
      "\n",
      "Ошибка #8\n",
      "Контекст: ...Have you permitted it to become a giveaway program rather than one that has the goal of improved employee morale and, consequently, increased productivity?...\n",
      "Слово: 'giveaway' (лемма: 'giveaway')\n",
      "  - Настоящее значение     : [giveaway%1:21:00::] a gift of public land or resources for the private gain of a limited group\n",
      "  - Предсказанное: [game_show.n.01] a television or radio program in which contestants compete for awards\n",
      "\n",
      "Ошибка #9\n",
      "Контекст: ...Have you permitted it to become a giveaway program rather than one that has the goal of improved employee morale and, consequently, increased productivity?...\n",
      "Слово: 'program' (лемма: 'program')\n",
      "  - Настоящее значение     : [program%1:09:01::] a system of projects or services intended to meet a public need\n",
      "  - Предсказанное: [plan.n.01] a series of steps to be carried out or goals to be accomplished\n",
      "\n",
      "Ошибка #10\n",
      "Контекст: ...Have you permitted it to become a giveaway program rather than one that has the goal of improved employee morale and, consequently, increased productivity?...\n",
      "Слово: 'has' (лемма: 'have')\n",
      "  - Настоящее значение     : [have%2:42:00::] have as a feature\n",
      "  - Предсказанное: [have.v.17] achieve a point or goal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Лучший эксперимент ---\")\n",
    "print(f\"Лучшая Accuracy: {best_accuracy:.4f}\")\n",
    "print(f\"Лучшие параметры: {best_params}\")\n",
    "\n",
    "print(\"\\n--- 10 ошибок для лучшей модели ---\")\n",
    "for i, error in enumerate(best_results['errors'][:10]):\n",
    "    print(f\"Ошибка #{i+1}\")\n",
    "    print(f\"Контекст: ...{error['text']}...\")\n",
    "    print(f\"Слово: '{error['word']}' (лемма: '{error['lemma']}')\")\n",
    "    print(\n",
    "        f\"  - Настоящее значение     : [{error['gold_sense']['key']}] {error['gold_sense']['definition']}\")\n",
    "    print(\n",
    "        f\"  - Предсказанное: [{error['predicted_sense']['name']}] {error['predicted_sense']['definition']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519a927e",
   "metadata": {},
   "source": [
    "Во-первых, удаление стоп-слов существенно улучшает результат (больше чем на 15%), что для lexical-based-подхода более чем ожидаемо. А вот использование лемм вместо словоформ, наоборот, результат ухудшает. Связано это, видимо, с плохим качеством лемматизатора на основе Wordnet (который использовался, чтобы лемматизировать токены для определений синсетов), а также с тем, что тексты на английском языке из-за аналитической природы языка не нуждаются в этом шаге препроцессинга в той же степени, как, например, русскоязычные.\n",
    "\n",
    "Также мы увидели, что при использовании данного алгоритма самым эффективным оказалось брать минимальное контекстное окно (3), хотя, судя по метрикам, его размер вообще не особо значительно влиял на результат — решающее значение имели только стоп-слова.\n",
    "\n",
    "В целом, результат довольно плохой, мы не достигли даже 50% точности. Видно, что, если пересечений контекста и определения нет вообще, выбирается самое короткое, а иначе мы полагаемся на просто какие-то случайные слова, которым повезло попасть в пересечение (чаще всего, 1-2). Однако учитывая то, что это даже не бинарная, а многоклассовая классификация, результат совсем уж бессмысленным назвать нельзя."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed74856",
   "metadata": {},
   "source": [
    "## 3. Алгоритм Леска с семантическими эмбеддингами\n",
    "\n",
    "Возьмём три разные модели типа Sentence-Transformers и проверим, как они справляются с нашей задачей. Никакой предобработки не будет, так как эти модели учились на реальных текстах и не видели текста без пунктуации, стоп-слов или с одними леммами. Окно тоже будет только всё предложение, опять же, из-за природы текстов, на которых такие модели обучались."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f98fd65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-03 12:24:13.616357: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759483453.752394    5477 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1759483453.789776    5477 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1759483454.066813    5477 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759483454.066874    5477 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759483454.066879    5477 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759483454.066881    5477 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-10-03 12:24:14.095173: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a653a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(corpus, model_name, query_prefix=None, doc_prefix=None):\n",
    "    \"\"\"\n",
    "    Считает эмбеддинги для всех предложений и всех определений многозначных слов в корпусе.\n",
    "    \"\"\"\n",
    "    # Загружаем модель\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    # Получаем эмбеддинги для всех предложений\n",
    "    sentence_texts = [sentence['text'] for sentence in corpus]\n",
    "    if query_prefix:\n",
    "        sentence_texts = [f\"{query_prefix}{text}\" for text in sentence_texts]\n",
    "\n",
    "    sentence_embeddings = model.encode(\n",
    "        sentence_texts,\n",
    "        show_progress_bar=True,\n",
    "        normalize_embeddings=True)\n",
    "\n",
    "    # Добавляем эмбеддинги в исходный корпус\n",
    "    for i, sentence in enumerate(corpus):\n",
    "        sentence['embedding'] = sentence_embeddings[i]\n",
    "\n",
    "    # Собираем все уникальные многозначные слова и их значения\n",
    "    unique_polysemous_lemmas = set()\n",
    "    for sentence in corpus:\n",
    "        for token in sentence['tokens']:\n",
    "            if token.get('sense') and token['sense'].get('key'):\n",
    "                unique_polysemous_lemmas.add(token['lemma'])\n",
    "\n",
    "    # Получаем эмбеддинги для каждого значения\n",
    "    sense_data = defaultdict(list)\n",
    "    definitions_to_encode = []\n",
    "    sense_order_map = []\n",
    "\n",
    "    for lemma in tqdm(unique_polysemous_lemmas, desc=\"Сбор толкований\"):\n",
    "        synsets = wn.synsets(lemma)\n",
    "        for synset in synsets:\n",
    "            definitions_to_encode.append(synset.definition())\n",
    "            sense_order_map.append({'lemma': lemma, 'synset': synset})\n",
    "\n",
    "    if doc_prefix:\n",
    "        definitions_to_encode = [\n",
    "            f\"{doc_prefix}{text}\" for text in definitions_to_encode]\n",
    "\n",
    "    definition_embeddings = model.encode(\n",
    "        definitions_to_encode,\n",
    "        show_progress_bar=True,\n",
    "        normalize_embeddings=True)\n",
    "\n",
    "    # Сопоставляем эмбеддинги с их значениями\n",
    "    for i, embedding in enumerate(definition_embeddings):\n",
    "        info = sense_order_map[i]\n",
    "        sense_data[info['lemma']].append({\n",
    "            'synset': info['synset'],\n",
    "            'embedding': embedding\n",
    "        })\n",
    "\n",
    "    return corpus, sense_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b431357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disambiguate_with_embeddings(corpus_with_embeddings, sense_embeddings):\n",
    "    \"\"\"\n",
    "    Предсказывает значения для многозначных слов в корпусе, используя алгоритм Леска и косинусное сходство эмбеддингов.\n",
    "    \"\"\"\n",
    "    for sentence in tqdm(\n",
    "            corpus_with_embeddings,\n",
    "            desc=\"Disambiguating sentences\"):\n",
    "        sentence_emb = sentence['embedding'].reshape(1, -1)\n",
    "\n",
    "        for token in sentence['tokens']:\n",
    "            if token.get('sense') and token['sense'].get('key'):\n",
    "                lemma = token['lemma']\n",
    "\n",
    "                # Проверяем, есть ли у нас эмбеддинги для значений этого слова\n",
    "                if lemma not in sense_embeddings or not sense_embeddings[lemma]:\n",
    "                    continue\n",
    "\n",
    "                possible_senses = sense_embeddings[lemma]\n",
    "\n",
    "                # Собираем эмбеддинги всех возможных значений в матрицу\n",
    "                definition_embs = np.array(\n",
    "                    [sense['embedding'] for sense in possible_senses])\n",
    "\n",
    "                # Рассчитываем косинусную близость между эмбеддингом предложения\n",
    "                # и эмбеддингами всех определений\n",
    "                similarities = cosine_similarity(sentence_emb, definition_embs)\n",
    "\n",
    "                # Находим значение с наибольшей близостью\n",
    "                best_sense_index = np.argmax(similarities)\n",
    "                best_sense = possible_senses[best_sense_index]['synset']\n",
    "\n",
    "                # Сохраняем предсказанное значение\n",
    "                token['predicted_sense'] = {\n",
    "                    'name': best_sense.name(),\n",
    "                    'definition': best_sense.definition()\n",
    "                }\n",
    "\n",
    "    return corpus_with_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b19276",
   "metadata": {},
   "source": [
    "### Эксперимент 1: all-mpnet-base-v2\n",
    "\n",
    "Оригинальная моделька от авторов Sentence-Transformers, работает с некоторым качеством, но в 2025 году уже есть масса сильно лучших (и больших) моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50af7d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f557e6a4886a4507b4dffee3171fd969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1546 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b516eb1b05ec4c81b55dd5ddbaa36789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Сбор толкований:   0%|          | 0/20387 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "032793eeefd5480594b6bf1a56c7e4a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2073 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4976e199443946a8b5a3061e6d856e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Disambiguating sentences:   0%|          | 0/49452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy для sentence-transformers/all-mpnet-base-v2: 0.3708\n",
      "\n",
      "--- 10 ошибок ---\n",
      "Ошибка #1\n",
      "Контекст: ...How long has it been since you reviewed the objectives of your benefit and service program?...\n",
      "Слово: 'long' (lemma: 'long')\n",
      "  - Настоящее значение: [long%3:00:02::] primarily temporal sense; being or indicating a relatively great or greater than average duration or passage of time or a duration as specified\n",
      "  - Предсказанное значение: [long.r.01] for an extended time or at a distant time\n",
      "\n",
      "Ошибка #2\n",
      "Контекст: ...How long has it been since you reviewed the objectives of your benefit and service program?...\n",
      "Слово: 'been' (lemma: 'be')\n",
      "  - Настоящее значение: [be%2:42:03::] have the quality of being; (copula, used with an adjective or a predicate noun)\n",
      "  - Предсказанное значение: [be.v.10] spend or use time\n",
      "\n",
      "Ошибка #3\n",
      "Контекст: ...How long has it been since you reviewed the objectives of your benefit and service program?...\n",
      "Слово: 'reviewed' (lemma: 'review')\n",
      "  - Настоящее значение: [review%2:31:00::] look at again; examine again\n",
      "  - Предсказанное значение: [reappraisal.n.01] a new appraisal or evaluation\n",
      "\n",
      "Ошибка #4\n",
      "Контекст: ...How long has it been since you reviewed the objectives of your benefit and service program?...\n",
      "Слово: 'benefit' (lemma: 'benefit')\n",
      "  - Настоящее значение: [benefit%1:21:00::] financial assistance in time of need\n",
      "  - Предсказанное значение: [profit.v.01] derive a benefit from\n",
      "\n",
      "Ошибка #5\n",
      "Контекст: ...How long has it been since you reviewed the objectives of your benefit and service program?...\n",
      "Слово: 'service' (lemma: 'service')\n",
      "  - Настоящее значение: [service%1:04:07::] employment in or work for another\n",
      "  - Предсказанное значение: [service.n.01] work done by one person or group that benefits another\n",
      "\n",
      "Ошибка #6\n",
      "Контекст: ...Have you permitted it to become a giveaway program rather than one that has the goal of improved employee morale and, consequently, increased productivity?...\n",
      "Слово: 'permitted' (lemma: 'permit')\n",
      "  - Настоящее значение: [permit%2:41:00::] make it possible through a specific action or lack of action for something to happen\n",
      "  - Предсказанное значение: [license.n.04] the act of giving a formal (usually written) authorization\n",
      "\n",
      "Ошибка #7\n",
      "Контекст: ...Have you permitted it to become a giveaway program rather than one that has the goal of improved employee morale and, consequently, increased productivity?...\n",
      "Слово: 'giveaway' (lemma: 'giveaway')\n",
      "  - Настоящее значение: [giveaway%1:21:00::] a gift of public land or resources for the private gain of a limited group\n",
      "  - Предсказанное значение: [giveaway.n.02] an unintentional disclosure\n",
      "\n",
      "Ошибка #8\n",
      "Контекст: ...Have you permitted it to become a giveaway program rather than one that has the goal of improved employee morale and, consequently, increased productivity?...\n",
      "Слово: 'has' (lemma: 'have')\n",
      "  - Настоящее значение: [have%2:42:00::] have as a feature\n",
      "  - Предсказанное значение: [have.v.17] achieve a point or goal\n",
      "\n",
      "Ошибка #9\n",
      "Контекст: ...Have you permitted it to become a giveaway program rather than one that has the goal of improved employee morale and, consequently, increased productivity?...\n",
      "Слово: 'improved' (lemma: 'improved')\n",
      "  - Настоящее значение: [improved%3:00:00::] made more desirable or valuable or profitable; especially made ready for use or marketing\n",
      "  - Предсказанное значение: [better.v.02] to make better\n",
      "\n",
      "Ошибка #10\n",
      "Контекст: ...Have you permitted it to become a giveaway program rather than one that has the goal of improved employee morale and, consequently, increased productivity?...\n",
      "Слово: 'morale' (lemma: 'morale')\n",
      "  - Настоящее значение: [morale%1:26:00::] a state of individual psychological well-being based upon a sense of confidence and usefulness and purpose\n",
      "  - Предсказанное значение: [esprit_de_corps.n.01] the spirit of a group that makes the members want the group to succeed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name_1 = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# Получаем эмбеддинги данной моделью\n",
    "corpus_with_embs_1, senses_with_embs_1 = get_embeddings(\n",
    "    processed_corpus,\n",
    "    model_name=model_name_1,\n",
    "    query_prefix=None,\n",
    "    doc_prefix=None\n",
    ")\n",
    "\n",
    "# 2. Предсказываем значения, используя эмбеддинги\n",
    "corpus_pred_1 = disambiguate_with_embeddings(\n",
    "    corpus_with_embs_1, senses_with_embs_1)\n",
    "\n",
    "# Оцениваем\n",
    "results_1 = evaluate(corpus_pred_1)\n",
    "print(f\"\\nAccuracy для {model_name_1}: {results_1['average_accuracy']:.4f}\\n\")\n",
    "\n",
    "print(\"--- 10 ошибок ---\")\n",
    "for i, error in enumerate(results_1['errors'][:10]):\n",
    "    print(f\"Ошибка #{i+1}\")\n",
    "    print(f\"Контекст: ...{error['text']}...\")\n",
    "    print(f\"Слово: '{error['word']}' (lemma: '{error['lemma']}')\")\n",
    "    print(\n",
    "        f\"  - Настоящее значение: [{error['gold_sense']['key']}] {error['gold_sense']['definition']}\")\n",
    "    print(\n",
    "        f\"  - Предсказанное значение: [{error['predicted_sense']['name']}] {error['predicted_sense']['definition']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c0e819",
   "metadata": {},
   "source": [
    "### Эксперимент 2: e5-large-v2\n",
    "\n",
    "Хорошая, индустриальная модель, которая ещё в конце 2024 года была SOTA для векторного поиска (для английского языка), да и сейчас остаётся очень хорошей для индустриальных задач."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4542380e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01a1198e68a4105935e8e1ce094d9e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783ece9176464b8b8c16fb654bd6cc0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776b91b077bf4de297d872a14dc8d74b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d7d2115050d40b38b6afe069b0560a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b96a962a7d4a31b0aad799e18e2979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319c3ed97fa1438ea855466d34d311bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515a962f1cb84730bed488b64b1a2a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa5f51b7f6d42c99e1a2346d2fc50b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a094837ce54b5d8c94caf9a3eaa123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b1cb28689db406aa5f836571d82ef52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/201 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c60ac2ea2ed145cfa7c44a347d878f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1546 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "366cc2d8d40b44b2aef4453db4f5c345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Сбор толкований:   0%|          | 0/20387 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0876973732374ac2bbba09ac36b7b209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2073 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fa4a9809fe64d41b5430f536da6d622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Disambiguating sentences:   0%|          | 0/49452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy для intfloat/e5-large-v2: 0.3647\n",
      "\n",
      "--- 10 ошибок ---\n",
      "Ошибка #1\n",
      "Контекст: ...How long has it been since you reviewed the objectives of your benefit and service program?...\n",
      "Слово: 'long' (lemma: 'long')\n",
      "  - Настоящее значение: [long%3:00:02::] primarily temporal sense; being or indicating a relatively great or greater than average duration or passage of time or a duration as specified\n",
      "  - Предсказанное значение: [long.r.01] for an extended time or at a distant time\n",
      "\n",
      "Ошибка #2\n",
      "Контекст: ...How long has it been since you reviewed the objectives of your benefit and service program?...\n",
      "Слово: 'been' (lemma: 'be')\n",
      "  - Настоящее значение: [be%2:42:03::] have the quality of being; (copula, used with an adjective or a predicate noun)\n",
      "  - Предсказанное значение: [be.v.10] spend or use time\n",
      "\n",
      "Ошибка #3\n",
      "Контекст: ...How long has it been since you reviewed the objectives of your benefit and service program?...\n",
      "Слово: 'reviewed' (lemma: 'review')\n",
      "  - Настоящее значение: [review%2:31:00::] look at again; examine again\n",
      "  - Предсказанное значение: [reappraisal.n.01] a new appraisal or evaluation\n",
      "\n",
      "Ошибка #4\n",
      "Контекст: ...How long has it been since you reviewed the objectives of your benefit and service program?...\n",
      "Слово: 'benefit' (lemma: 'benefit')\n",
      "  - Настоящее значение: [benefit%1:21:00::] financial assistance in time of need\n",
      "  - Предсказанное значение: [profit.v.01] derive a benefit from\n",
      "\n",
      "Ошибка #5\n",
      "Контекст: ...How long has it been since you reviewed the objectives of your benefit and service program?...\n",
      "Слово: 'service' (lemma: 'service')\n",
      "  - Настоящее значение: [service%1:04:07::] employment in or work for another\n",
      "  - Предсказанное значение: [service.n.01] work done by one person or group that benefits another\n",
      "\n",
      "Ошибка #6\n",
      "Контекст: ...How long has it been since you reviewed the objectives of your benefit and service program?...\n",
      "Слово: 'program' (lemma: 'program')\n",
      "  - Настоящее значение: [program%1:09:01::] a system of projects or services intended to meet a public need\n",
      "  - Предсказанное значение: [plan.n.01] a series of steps to be carried out or goals to be accomplished\n",
      "\n",
      "Ошибка #7\n",
      "Контекст: ...Have you permitted it to become a giveaway program rather than one that has the goal of improved employee morale and, consequently, increased productivity?...\n",
      "Слово: 'program' (lemma: 'program')\n",
      "  - Настоящее значение: [program%1:09:01::] a system of projects or services intended to meet a public need\n",
      "  - Предсказанное значение: [program.v.01] arrange a program of or for\n",
      "\n",
      "Ошибка #8\n",
      "Контекст: ...Have you permitted it to become a giveaway program rather than one that has the goal of improved employee morale and, consequently, increased productivity?...\n",
      "Слово: 'rather' (lemma: 'rather')\n",
      "  - Настоящее значение: [rather%4:02:02::] on the contrary\n",
      "  - Предсказанное значение: [quite.r.01] to a degree (not used with a negative)\n",
      "\n",
      "Ошибка #9\n",
      "Контекст: ...Have you permitted it to become a giveaway program rather than one that has the goal of improved employee morale and, consequently, increased productivity?...\n",
      "Слово: 'has' (lemma: 'have')\n",
      "  - Настоящее значение: [have%2:42:00::] have as a feature\n",
      "  - Предсказанное значение: [accept.v.02] receive willingly something given or offered\n",
      "\n",
      "Ошибка #10\n",
      "Контекст: ...Have you permitted it to become a giveaway program rather than one that has the goal of improved employee morale and, consequently, increased productivity?...\n",
      "Слово: 'improved' (lemma: 'improved')\n",
      "  - Настоящее значение: [improved%3:00:00::] made more desirable or valuable or profitable; especially made ready for use or marketing\n",
      "  - Предсказанное значение: [improved.s.02] become or made better in quality\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name_2 = \"intfloat/e5-large-v2\"\n",
    "\n",
    "corpus_with_embs_2, senses_with_embs_2 = get_embeddings(\n",
    "    processed_corpus,\n",
    "    model_name=model_name_2,\n",
    "    query_prefix=\"query: \",\n",
    "    doc_prefix=\"passage: \"\n",
    ")\n",
    "\n",
    "corpus_pred_2 = disambiguate_with_embeddings(\n",
    "    corpus_with_embs_2, senses_with_embs_2)\n",
    "\n",
    "results_2 = evaluate(corpus_pred_2)\n",
    "print(f\"\\nAccuracy для {model_name_2}: {results_2['average_accuracy']:.4f}\\n\")\n",
    "\n",
    "print(\"--- 10 ошибок ---\")\n",
    "for i, error in enumerate(results_2['errors'][:10]):\n",
    "    print(f\"Ошибка #{i+1}\")\n",
    "    print(f\"Контекст: ...{error['text']}...\")\n",
    "    print(f\"Слово: '{error['word']}' (lemma: '{error['lemma']}')\")\n",
    "    print(\n",
    "        f\"  - Настоящее значение: [{error['gold_sense']['key']}] {error['gold_sense']['definition']}\")\n",
    "    print(\n",
    "        f\"  - Предсказанное значение: [{error['predicted_sense']['name']}] {error['predicted_sense']['definition']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa96258",
   "metadata": {},
   "source": [
    "### Эксперимент 3: multilingual-e5-large-instruct\n",
    "\n",
    "Кажется, лучшее, что есть в серии E5, не считая огромных LLM-based-моделек. Особенность модели в том, что она поддерживает короткие инструкции, которые, в теории, должны модифицировать её эмбеддинги под задачу. Здесь мы используем этот факт и составляем инструкцию для задачи WSD (в стиле тех, на которых модель училась)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16f6a93a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740104add4e04219b56135fd2fc4dfe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7747ac04c524c4aa5af4ad5e5c4c77a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1546 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac249e62599e4b9391ef96ce3666bd02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Сбор толкований:   0%|          | 0/20387 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a858aea0f6466dbbde9c375a174efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2073 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ebf89b908b49869bd921472945d448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Disambiguating sentences:   0%|          | 0/49452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy для intfloat/multilingual-e5-large-instruct: 0.4061\n",
      "\n",
      "--- 10 ошибок ---\n",
      "Ошибка #1\n",
      "Контекст: ...How long has it been since you reviewed the objectives of your benefit and service program?...\n",
      "Слово: 'reviewed' (lemma: 'review')\n",
      "  - Настоящее значение: [review%2:31:00::] look at again; examine again\n",
      "  - Предсказанное значение: [review.v.05] look back upon (a period of time, sequence of events); remember\n",
      "\n",
      "Ошибка #2\n",
      "Контекст: ...How long has it been since you reviewed the objectives of your benefit and service program?...\n",
      "Слово: 'objectives' (lemma: 'objective')\n",
      "  - Настоящее значение: [objective%1:09:00::] the goal intended to be attained (and which is believed to be attainable)\n",
      "  - Предсказанное значение: [objective.a.02] serving as or indicating the object of a verb or of certain prepositions and used for certain other purposes\n",
      "\n",
      "Ошибка #3\n",
      "Контекст: ...How long has it been since you reviewed the objectives of your benefit and service program?...\n",
      "Слово: 'benefit' (lemma: 'benefit')\n",
      "  - Настоящее значение: [benefit%1:21:00::] financial assistance in time of need\n",
      "  - Предсказанное значение: [profit.v.01] derive a benefit from\n",
      "\n",
      "Ошибка #4\n",
      "Контекст: ...How long has it been since you reviewed the objectives of your benefit and service program?...\n",
      "Слово: 'service' (lemma: 'service')\n",
      "  - Настоящее значение: [service%1:04:07::] employment in or work for another\n",
      "  - Предсказанное значение: [service.v.01] be used by; as of a utility\n",
      "\n",
      "Ошибка #5\n",
      "Контекст: ...How long has it been since you reviewed the objectives of your benefit and service program?...\n",
      "Слово: 'program' (lemma: 'program')\n",
      "  - Настоящее значение: [program%1:09:01::] a system of projects or services intended to meet a public need\n",
      "  - Предсказанное значение: [plan.n.01] a series of steps to be carried out or goals to be accomplished\n",
      "\n",
      "Ошибка #6\n",
      "Контекст: ...Have you permitted it to become a giveaway program rather than one that has the goal of improved employee morale and, consequently, increased productivity?...\n",
      "Слово: 'program' (lemma: 'program')\n",
      "  - Настоящее значение: [program%1:09:01::] a system of projects or services intended to meet a public need\n",
      "  - Предсказанное значение: [program.v.01] arrange a program of or for\n",
      "\n",
      "Ошибка #7\n",
      "Контекст: ...Have you permitted it to become a giveaway program rather than one that has the goal of improved employee morale and, consequently, increased productivity?...\n",
      "Слово: 'rather' (lemma: 'rather')\n",
      "  - Настоящее значение: [rather%4:02:02::] on the contrary\n",
      "  - Предсказанное значение: [quite.r.01] to a degree (not used with a negative)\n",
      "\n",
      "Ошибка #8\n",
      "Контекст: ...Have you permitted it to become a giveaway program rather than one that has the goal of improved employee morale and, consequently, increased productivity?...\n",
      "Слово: 'has' (lemma: 'have')\n",
      "  - Настоящее значение: [have%2:42:00::] have as a feature\n",
      "  - Предсказанное значение: [induce.v.02] cause to do; cause to act in a specified manner\n",
      "\n",
      "Ошибка #9\n",
      "Контекст: ...Have you permitted it to become a giveaway program rather than one that has the goal of improved employee morale and, consequently, increased productivity?...\n",
      "Слово: 'morale' (lemma: 'morale')\n",
      "  - Настоящее значение: [morale%1:26:00::] a state of individual psychological well-being based upon a sense of confidence and usefulness and purpose\n",
      "  - Предсказанное значение: [esprit_de_corps.n.01] the spirit of a group that makes the members want the group to succeed\n",
      "\n",
      "Ошибка #10\n",
      "Контекст: ...Have you permitted it to become a giveaway program rather than one that has the goal of improved employee morale and, consequently, increased productivity?...\n",
      "Слово: 'productivity' (lemma: 'productivity')\n",
      "  - Настоящее значение: [productivity%1:07:00::] the quality of being productive or having the power to produce\n",
      "  - Предсказанное значение: [productivity.n.02] (economics) the ratio of the quantity and quality of units produced to the labor per unit of time\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name_3 = \"intfloat/multilingual-e5-large-instruct\"\n",
    "instruction_3 = \"Given a sentence, retrieve the definition of a word as it is used in that sentence.\"\n",
    "\n",
    "corpus_with_embs_3, senses_with_embs_3 = get_embeddings(\n",
    "    processed_corpus,\n",
    "    model_name=model_name_3,\n",
    "    query_prefix=f\"Instruct: {instruction_3}\\nQuery: \",\n",
    "    doc_prefix=None\n",
    ")\n",
    "\n",
    "corpus_pred_3 = disambiguate_with_embeddings(\n",
    "    corpus_with_embs_3, senses_with_embs_3)\n",
    "\n",
    "results_3 = evaluate(corpus_pred_3)\n",
    "print(f\"\\nAccuracy для {model_name_3}: {results_3['average_accuracy']:.4f}\\n\")\n",
    "\n",
    "print(\"--- 10 ошибок ---\")\n",
    "for i, error in enumerate(results_3['errors'][:10]):\n",
    "    print(f\"Ошибка #{i+1}\")\n",
    "    print(f\"Контекст: ...{error['text']}...\")\n",
    "    print(f\"Слово: '{error['word']}' (lemma: '{error['lemma']}')\")\n",
    "    print(\n",
    "        f\"  - Настоящее значение: [{error['gold_sense']['key']}] {error['gold_sense']['definition']}\")\n",
    "    print(\n",
    "        f\"  - Предсказанное значение: [{error['predicted_sense']['name']}] {error['predicted_sense']['definition']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc5abd0",
   "metadata": {},
   "source": [
    "Результаты очень интересные. Получилось, что все наши модели дают худший результат, чем мера Жаккарда. Правда, если посмотреть на примеры ошибок, видно, что здесь намного чаще выбираются не совсем «левые» толкования, а похожие (иногда даже спорно и непонятно, не права ли модель на самом деле).\n",
    "\n",
    "Так, например, для слова *long* в контексте *How long has it been since you reviewed the objectives of your benefit and service program?* эталонное значение такое: 'primarily temporal sense; being or indicating a relatively great or greater than average duration or passage of time or a duration as specified'. Модель предсказала другое: 'for an extended time or at a distant time' — и я не уверен, что не могу с ней не согласиться. То же, например, можно сказать и для слова `review`.\n",
    "\n",
    "Интересно, что хоть instruct-модель и обогнала все другие, обычная E5-Large-V2 показала худший результат, чем более маленькая и старая all-mpnet-base-v2. Возможно, дело в том, что множество индустриальных бенчмарков ухудшили непосредственно NLI модели, ну или в чём-то ещё.\n",
    "\n",
    "Однако в целом видно, что все эти модели не обучены сопоставлять контексты с толкованиями слов, да ещё и когда целевое слово, в общем-то, неизвестно (может быть, надо было добавлять его в начало или конец текста или в инструкцию в последнем случае, но эта мысль слишком поздно пришла мне в голову).\n",
    "\n",
    "Однако нам никто не мешает дообучить любую из этих моделек, сближая эмбеддинги контекстов и соответствующих толкований (из любого словаря), и тогда я больше чем уверен, что результат будет очень хорошим. То есть, выходит, что мера Жаккарда хороша для unsupervised-случаев, когда у нас нет соответствующих размеченных данных (или ресурсов для дообучения), но и качество этот метод даёт невысокое, то, что мы получили, в каком-то смысле потолок, непонятно, как улучшить результат. Семантические эмбеддеры, несмотря на более низкий результат для предобученных моделях, имеют гораздо больший потенциал, если у нас есть данные и ресурсы, чтобы их дообучить."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39efde9a-af0b-4c94-bfd0-249e7054562f",
   "metadata": {},
   "source": [
    "# Задание 2 (5 балла)\n",
    "Попробуйте разные алгоритмы кластеризации на датасете - `https://github.com/nlpub/russe-wsi-kit/blob/initial/data/main/wiki-wiki/train.csv`\n",
    "\n",
    "Используйте код из семинара как основу. Используйте ARI как метрику качества.\n",
    "\n",
    "Попробуйте все 4 алгоритма кластеризации, про которые говорилось на семинаре. Для каждого из алгоритмов попробуйте настраивать гиперпараметры (посмотрите их в документации). Прогоните как минимум 5 экспериментов (не обязательно успешных) с разными параметрами на каждый алгоритме кластеризации и оцените: качество кластеризации, скорость работы, интуитивность параметров.\n",
    "\n",
    "Помимо этого также выберите 1 дополнительный алгоритм кластеризации отсюда - https://scikit-learn.org/stable/modules/clustering.html , опишите своими словами принцип его работы  и проделайте аналогичные эксперименты. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406ae377",
   "metadata": {},
   "source": [
    "## 1. Загружаем данные и смотрим статистику"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eab8e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/nlpub/russe-wsi-kit/initial/data/main/wiki-wiki/train.csv'\n",
    "df = pd.read_csv(\n",
    "    url,\n",
    "    sep='\\t',\n",
    "    header=0,\n",
    "    names=[\n",
    "        'context_id',\n",
    "        'word',\n",
    "        'gold_sense_id',\n",
    "        'predict_sense_id',\n",
    "        'positions',\n",
    "        'context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d72a04e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Статистика датасета ---\n",
      "Всего контекстов: 439\n",
      "Количество многозначных слов: 4\n",
      "Среднее количество контекстов на слово: 109.75\n",
      "Среднее количество значений для слова: 2.00\n"
     ]
    }
   ],
   "source": [
    "total_contexts = len(df)\n",
    "unique_words = df['word'].unique()\n",
    "num_unique_words = len(unique_words)\n",
    "\n",
    "# Группируем по слову, чтобы найти количество уникальных значений для\n",
    "# каждого слова\n",
    "senses_per_word = df.groupby('word')['gold_sense_id'].nunique()\n",
    "\n",
    "avg_meanings_per_word = senses_per_word.mean()\n",
    "avg_contexts_per_word = total_contexts / num_unique_words\n",
    "\n",
    "print(\"--- Статистика датасета ---\")\n",
    "print(f\"Всего контекстов: {total_contexts}\")\n",
    "print(f\"Количество многозначных слов: {num_unique_words}\")\n",
    "print(f\"Среднее количество контекстов на слово: {avg_contexts_per_word:.2f}\")\n",
    "print(f\"Среднее количество значений для слова: {avg_meanings_per_word:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ead4bc",
   "metadata": {},
   "source": [
    "## 2. Экспериментируем с кластеризацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bca4bf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ba236b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(df, model_name, instruction=None):\n",
    "    \"\"\"\n",
    "    Вычисляет эмбеддинги для всех контекстов в датафрейме с использованием указанной модели.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    contexts = df['context'].tolist()\n",
    "\n",
    "    if instruction:\n",
    "        contexts = [\n",
    "            f\"Instruct: {instruction}\\nQuery: {ctx}\" for ctx in contexts]\n",
    "\n",
    "    embeddings = model.encode(\n",
    "        contexts,\n",
    "        show_progress_bar=True,\n",
    "        normalize_embeddings=True)\n",
    "\n",
    "    df_embedded = df.copy()\n",
    "    df_embedded['embedding'] = list(embeddings)\n",
    "\n",
    "    return df_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5a16e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbec7d4b6e1c4af89e96a03c6b8354f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wsi_instruction = \"Identify the meaning of a word based on the sentence it appears in\"\n",
    "\n",
    "df_embedded = generate_embeddings(\n",
    "    df,\n",
    "    'intfloat/multilingual-e5-large-instruct',\n",
    "    instruction=wsi_instruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3118ba",
   "metadata": {},
   "source": [
    "### Эксперимент 1: K-means\n",
    "\n",
    "Здесь мы можем задать количество кластеров (подсказано данными), а также количество запусков. По идеи, чем больше запусков, тем больше шансов на лучший исход, но и работать будет дольше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3366f2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running K-Means Experiments ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f018a2f47a4429a878408bda478fa56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "K-Means Hyperparameters:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing params: {'n_clusters': 2, 'n_init': 'auto', 'random_state': 42} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 69.0\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 55.0\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 67.5\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 28.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.7200 | Total Time: 0.0142s\n",
      "\n",
      "--- Testing params: {'n_clusters': 2, 'n_init': 'auto', 'algorithm': 'elkan', 'random_state': 42} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 69.0\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 55.0\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 67.5\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 28.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.7200 | Total Time: 0.0183s\n",
      "\n",
      "--- Testing params: {'n_clusters': 3, 'n_init': 'auto', 'random_state': 42} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 3\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 46.0\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 3\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 36.7\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 3\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 45.0\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 3\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 18.7\n",
      "  --------------------\n",
      "  Avg ARI: 0.6225 | Total Time: 0.0230s\n",
      "\n",
      "--- Testing params: {'n_clusters': 4, 'n_init': 'auto', 'random_state': 42} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 4\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 34.5\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 4\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 27.5\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 4\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 33.8\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 4\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 14.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.4654 | Total Time: 0.0192s\n",
      "\n",
      "--- Testing params: {'n_clusters': 2, 'n_init': 1, 'init': 'random', 'random_state': 42} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 69.0\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 55.0\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 67.5\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 28.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.7254 | Total Time: 0.0082s\n",
      "\n",
      "--- Testing params: {'n_clusters': 2, 'n_init': 10, 'init': 'random', 'random_state': 42} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 69.0\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 55.0\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 67.5\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 28.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.8374 | Total Time: 0.0378s\n",
      "\n",
      "--- Testing params: {'n_clusters': 2, 'n_init': 20, 'random_state': 42} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 69.0\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 55.0\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 67.5\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 28.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.8374 | Total Time: 0.1230s\n",
      "\n",
      "--- Testing params: {'n_clusters': 2, 'n_init': 30, 'random_state': 42} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 69.0\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 55.0\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 67.5\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 28.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.8374 | Total Time: 0.1759s\n",
      "\n",
      "--- K-Means Final Summary ---\n",
      "Best ARI Score: 0.8374 with params: {'n_clusters': 2, 'n_init': 10, 'init': 'random', 'random_state': 42}\n",
      "Fastest Run: 0.0082s with params: {'n_clusters': 2, 'n_init': 1, 'init': 'random', 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "print(\"--- Running K-Means Experiments ---\")\n",
    "\n",
    "# Гиперпараметры\n",
    "kmeans_params_to_test = [\n",
    "    {'n_clusters': 2, 'n_init': 'auto', 'random_state': 42},\n",
    "    {'n_clusters': 2, 'n_init': 'auto', 'algorithm': 'elkan', 'random_state': 42},\n",
    "    {'n_clusters': 3, 'n_init': 'auto', 'random_state': 42},\n",
    "    {'n_clusters': 4, 'n_init': 'auto', 'random_state': 42},\n",
    "    {'n_clusters': 2, 'n_init': 1, 'init': 'random', 'random_state': 42},\n",
    "    {'n_clusters': 2, 'n_init': 10, 'init': 'random', 'random_state': 42},\n",
    "    {'n_clusters': 2, 'n_init': 20, 'random_state': 42},\n",
    "    {'n_clusters': 2, 'n_init': 30, 'random_state': 42}\n",
    "]\n",
    "\n",
    "unique_words = df_embedded['word'].unique()\n",
    "kmeans_results = []\n",
    "\n",
    "for params in tqdm(kmeans_params_to_test, desc=\"K-Means Hyperparameters\"):\n",
    "    print(f\"\\n--- Testing params: {params} ---\")\n",
    "    total_ari = 0\n",
    "    total_time = 0\n",
    "\n",
    "    for word in unique_words:\n",
    "        word_df = df_embedded[df_embedded['word'] == word]\n",
    "        embeddings = np.vstack(word_df['embedding'].values)\n",
    "        gold_labels = word_df['gold_sense_id'].values\n",
    "\n",
    "        clusterer = KMeans(**params)\n",
    "\n",
    "        start_time = time.time()\n",
    "        predicted_labels = clusterer.fit_predict(embeddings)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Статистика по каждому слову\n",
    "        gold_counts = collections.Counter(gold_labels)\n",
    "        pred_counts = collections.Counter(predicted_labels)\n",
    "        print(f\"  Word: '{word}'\")\n",
    "        print(\n",
    "            f\"    - Clusters (Gold/Pred): {len(gold_counts)} / {len(pred_counts)}\")\n",
    "        print(\n",
    "            f\"    - Avg. Contexts/Cluster (Gold/Pred): {np.mean(list(gold_counts.values())):.1f} / {np.mean(list(pred_counts.values())):.1f}\")\n",
    "\n",
    "        total_time += (end_time - start_time)\n",
    "        total_ari += adjusted_rand_score(gold_labels, predicted_labels)\n",
    "\n",
    "    avg_ari = total_ari / len(unique_words)\n",
    "    kmeans_results.append(\n",
    "        {'params': params, 'ari': avg_ari, 'time': total_time})\n",
    "    print(\n",
    "        f\"  --------------------\\n  Avg ARI: {avg_ari:.4f} | Total Time: {total_time:.4f}s\")\n",
    "\n",
    "# Финальные результаты\n",
    "best_ari_run = max(kmeans_results, key=lambda x: x['ari'])\n",
    "fastest_run = min(kmeans_results, key=lambda x: x['time'])\n",
    "\n",
    "print(\"\\n--- K-Means Final Summary ---\")\n",
    "print(\n",
    "    f\"Best ARI Score: {best_ari_run['ari']:.4f} with params: {best_ari_run['params']}\")\n",
    "print(\n",
    "    f\"Fastest Run: {fastest_run['time']:.4f}s with params: {fastest_run['params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e4f17f",
   "metadata": {},
   "source": [
    "### Эксперимент 2: DBSCAN\n",
    "\n",
    "Здесь ключевой гиперпараметр — `eps`, который по сути влияет на то, какой разброс дистанции будет учитываться при кластеризации. Учитывая, что наша моделька обучена так, что значения косинусной близости в основном разбросаны в диапазоне 0.7-1.0 (из документации), имеет смысл брать очень маленькие значения. Ну и ещё можно настроить минимальное количество сэмплов в каждом кластере."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f196bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running DBSCAN Experiments ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad8def793c048d99efd463801ca9793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DBSCAN Hyperparameters:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing params: {'eps': 0.05, 'min_samples': 3, 'metric': 'cosine'} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 1 (+ 2 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 136.0\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 2 (+ 6 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 52.0\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 11 (+ 11 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 11.3\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 4 (+ 6 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 12.5\n",
      "  --------------------\n",
      "  Avg ARI: 0.4018 | Total Time: 0.0138s\n",
      "\n",
      "--- Testing params: {'eps': 0.1, 'min_samples': 3, 'metric': 'cosine'} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 1 (+ 0 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 138.0\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 1 (+ 0 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 110.0\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 1 (+ 0 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 135.0\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 1 (+ 0 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 56.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.0000 | Total Time: 0.0102s\n",
      "\n",
      "--- Testing params: {'eps': 0.12, 'min_samples': 3, 'metric': 'cosine'} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 1 (+ 0 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 138.0\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 1 (+ 0 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 110.0\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 1 (+ 0 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 135.0\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 1 (+ 0 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 56.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.0000 | Total Time: 0.0105s\n",
      "\n",
      "--- Testing params: {'eps': 0.05, 'min_samples': 5, 'metric': 'cosine'} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 1 (+ 2 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 136.0\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 2 (+ 6 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 52.0\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 7 (+ 25 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 15.7\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 2 (+ 12 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 22.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.4109 | Total Time: 0.0115s\n",
      "\n",
      "--- Testing params: {'eps': 0.1, 'min_samples': 5, 'metric': 'cosine'} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 1 (+ 0 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 138.0\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 1 (+ 0 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 110.0\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 1 (+ 0 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 135.0\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 1 (+ 0 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 56.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.0000 | Total Time: 0.0109s\n",
      "\n",
      "--- DBSCAN Final Summary ---\n",
      "Best ARI Score: 0.4109 with params: {'eps': 0.05, 'min_samples': 5, 'metric': 'cosine'}\n",
      "Fastest Run: 0.0102s with params: {'eps': 0.1, 'min_samples': 3, 'metric': 'cosine'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "print(\"\\n--- Running DBSCAN Experiments ---\")\n",
    "\n",
    "# Гиперпараметры\n",
    "dbscan_params_to_test = [\n",
    "    {'eps': 0.05, 'min_samples': 3, 'metric': 'cosine'},\n",
    "    {'eps': 0.1, 'min_samples': 3, 'metric': 'cosine'},\n",
    "    {'eps': 0.12, 'min_samples': 3, 'metric': 'cosine'},\n",
    "    {'eps': 0.05, 'min_samples': 5, 'metric': 'cosine'},\n",
    "    {'eps': 0.1, 'min_samples': 5, 'metric': 'cosine'},\n",
    "]\n",
    "\n",
    "dbscan_results = []\n",
    "\n",
    "for params in tqdm(dbscan_params_to_test, desc=\"DBSCAN Hyperparameters\"):\n",
    "    print(f\"\\n--- Testing params: {params} ---\")\n",
    "    total_ari = 0\n",
    "    total_time = 0\n",
    "\n",
    "    for word in unique_words:\n",
    "        word_df = df_embedded[df_embedded['word'] == word]\n",
    "        embeddings = np.vstack(word_df['embedding'].values)\n",
    "        gold_labels = word_df['gold_sense_id'].values\n",
    "\n",
    "        clusterer = DBSCAN(**params)\n",
    "\n",
    "        start_time = time.time()\n",
    "        predicted_labels = clusterer.fit_predict(embeddings)\n",
    "        end_time = time.time()\n",
    "\n",
    "        gold_counts = collections.Counter(gold_labels)\n",
    "        pred_counts = collections.Counter(predicted_labels)\n",
    "        num_pred_clusters = len(pred_counts) - (1 if -1 in pred_counts else 0)\n",
    "\n",
    "        print(f\"  Word: '{word}'\")\n",
    "        print(\n",
    "            f\"    - Clusters (Gold/Pred): {len(gold_counts)} / {num_pred_clusters} (+ {pred_counts.get(-1, 0)} noise points)\")\n",
    "        print(\n",
    "            f\"    - Avg. Contexts/Cluster (Gold/Pred): {np.mean(list(gold_counts.values())):.1f} / {np.mean([c for k, c in pred_counts.items() if k != -1]) if num_pred_clusters > 0 else 0:.1f}\")\n",
    "\n",
    "        total_time += (end_time - start_time)\n",
    "        if len(set(predicted_labels)) > 1:\n",
    "            total_ari += adjusted_rand_score(gold_labels, predicted_labels)\n",
    "        else:\n",
    "            total_ari += 0\n",
    "\n",
    "    avg_ari = total_ari / len(unique_words)\n",
    "    dbscan_results.append(\n",
    "        {'params': params, 'ari': avg_ari, 'time': total_time})\n",
    "    print(\n",
    "        f\"  --------------------\\n  Avg ARI: {avg_ari:.4f} | Total Time: {total_time:.4f}s\")\n",
    "\n",
    "# Финальные результаты\n",
    "best_ari_run_db = max(dbscan_results, key=lambda x: x['ari'])\n",
    "fastest_run_db = min(dbscan_results, key=lambda x: x['time'])\n",
    "\n",
    "print(\"\\n--- DBSCAN Final Summary ---\")\n",
    "print(\n",
    "    f\"Best ARI Score: {best_ari_run_db['ari']:.4f} with params: {best_ari_run_db['params']}\")\n",
    "print(\n",
    "    f\"Fastest Run: {fastest_run_db['time']:.4f}s with params: {fastest_run_db['params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e60b603",
   "metadata": {},
   "source": [
    "### Эксперимент 3: Affinity Propagation\n",
    "\n",
    "Здесь два ключевых параметра: `damping` и `preference`. Последний непосредственно влияет на количество кластеров: если ставить отрицательные значения, кластеров будет поменьше, что нам как раз очень надо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4163a1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Affinity Propagation Experiments ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0538f97764c489da7f531875667079d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "AffinityProp Hyperparameters:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing params: {'damping': 0.5, 'random_state': 42} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 27\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 5.1\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 22\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 5.0\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 25\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 5.4\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 15\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 3.7\n",
      "  --------------------\n",
      "  Avg ARI: 0.0685 | Total Time: 0.0808s\n",
      "\n",
      "--- Testing params: {'damping': 0.7, 'random_state': 42} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 26\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 5.3\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 23\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 4.8\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 25\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 5.4\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 15\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 3.7\n",
      "  --------------------\n",
      "  Avg ARI: 0.0677 | Total Time: 0.0597s\n",
      "\n",
      "--- Testing params: {'damping': 0.9, 'random_state': 42} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 26\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 5.3\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 23\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 4.8\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 25\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 5.4\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 15\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 3.7\n",
      "  --------------------\n",
      "  Avg ARI: 0.0669 | Total Time: 0.1352s\n",
      "\n",
      "--- Testing params: {'damping': 0.9, 'preference': -5, 'random_state': 42} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 138.0\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 110.0\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 67.5\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 56.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.1120 | Total Time: 0.0609s\n",
      "\n",
      "--- Testing params: {'damping': 0.9, 'preference': -10, 'random_state': 42} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 138.0\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 110.0\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 135.0\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 56.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.0000 | Total Time: 0.0721s\n",
      "\n",
      "--- Testing params: {'damping': 0.95, 'preference': -20, 'random_state': 42} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 138.0\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 110.0\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 135.0\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 56.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.0000 | Total Time: 0.0945s\n",
      "\n",
      "--- Testing params: {'damping': 0.95, 'preference': -25, 'random_state': 42} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 138.0\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 110.0\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 135.0\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 56.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.0000 | Total Time: 0.1127s\n",
      "\n",
      "--- Testing params: {'damping': 0.95, 'preference': -30, 'random_state': 42} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 138.0\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 110.0\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 135.0\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 56.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.0000 | Total Time: 0.1041s\n",
      "\n",
      "--- Testing params: {'damping': 0.9, 'preference': -50, 'random_state': 42} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 138.0\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 110.0\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 135.0\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 56.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.0000 | Total Time: 0.0696s\n",
      "\n",
      "--- Testing params: {'damping': 0.9, 'preference': -100, 'random_state': 42} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 138.0\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 110.0\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 135.0\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 56.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.0000 | Total Time: 0.0702s\n",
      "\n",
      "--- Affinity Propagation Final Summary ---\n",
      "Best ARI Score: 0.1120 with params: {'damping': 0.9, 'preference': -5, 'random_state': 42}\n",
      "Fastest Run: 0.0597s with params: {'damping': 0.7, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "print(\"\\n--- Running Affinity Propagation Experiments ---\")\n",
    "\n",
    "# Гиперпараметры\n",
    "ap_params_to_test = [\n",
    "    {'damping': 0.5, 'random_state': 42},\n",
    "    {'damping': 0.7, 'random_state': 42},\n",
    "    {'damping': 0.9, 'random_state': 42},\n",
    "    {'damping': 0.9, 'preference': -5, 'random_state': 42},\n",
    "    {'damping': 0.9, 'preference': -10, 'random_state': 42},\n",
    "    {'damping': 0.95, 'preference': -20, 'random_state': 42},\n",
    "    {'damping': 0.95, 'preference': -25, 'random_state': 42},\n",
    "    {'damping': 0.95, 'preference': -30, 'random_state': 42},\n",
    "    {'damping': 0.9, 'preference': -50, 'random_state': 42},\n",
    "    {'damping': 0.9, 'preference': -100, 'random_state': 42},\n",
    "]\n",
    "\n",
    "ap_results = []\n",
    "\n",
    "for params in tqdm(ap_params_to_test, desc=\"AffinityProp Hyperparameters\"):\n",
    "    print(f\"\\n--- Testing params: {params} ---\")\n",
    "    total_ari = 0\n",
    "    total_time = 0\n",
    "\n",
    "    for word in unique_words:\n",
    "        word_df = df_embedded[df_embedded['word'] == word]\n",
    "        embeddings = np.vstack(word_df['embedding'].values)\n",
    "        gold_labels = word_df['gold_sense_id'].values\n",
    "\n",
    "        clusterer = AffinityPropagation(**params)\n",
    "\n",
    "        start_time = time.time()\n",
    "        predicted_labels = clusterer.fit_predict(embeddings)\n",
    "        end_time = time.time()\n",
    "\n",
    "        gold_counts = collections.Counter(gold_labels)\n",
    "        pred_counts = collections.Counter(predicted_labels)\n",
    "        print(f\"  Word: '{word}'\")\n",
    "        print(\n",
    "            f\"    - Clusters (Gold/Pred): {len(gold_counts)} / {len(pred_counts)}\")\n",
    "        print(\n",
    "            f\"    - Avg. Contexts/Cluster (Gold/Pred): {np.mean(list(gold_counts.values())):.1f} / {np.mean(list(pred_counts.values())):.1f}\")\n",
    "\n",
    "        total_time += (end_time - start_time)\n",
    "        if len(set(predicted_labels)) > 1:\n",
    "            total_ari += adjusted_rand_score(gold_labels, predicted_labels)\n",
    "        else:\n",
    "            total_ari += 0\n",
    "\n",
    "    avg_ari = total_ari / len(unique_words)\n",
    "    ap_results.append({'params': params, 'ari': avg_ari, 'time': total_time})\n",
    "    print(\n",
    "        f\"  --------------------\\n  Avg ARI: {avg_ari:.4f} | Total Time: {total_time:.4f}s\")\n",
    "\n",
    "# Финальные результаты\n",
    "best_ari_run_ap = max(ap_results, key=lambda x: x['ari'])\n",
    "fastest_run_ap = min(ap_results, key=lambda x: x['time'])\n",
    "\n",
    "print(\"\\n--- Affinity Propagation Final Summary ---\")\n",
    "print(\n",
    "    f\"Best ARI Score: {best_ari_run_ap['ari']:.4f} with params: {best_ari_run_ap['params']}\")\n",
    "print(\n",
    "    f\"Fastest Run: {fastest_run_ap['time']:.4f}s with params: {fastest_run_ap['params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b446895",
   "metadata": {},
   "source": [
    "### Эксперимент 4: Agglomerative Clustering\n",
    "\n",
    "Здесь, как и в случае с K-means, мы можем задать количество кластеров, а также тип связи. Кроме того, вместо количества кластеров мы можем задать разброс расстояния, и тогда алгоритм попытается определить количество кластеров самостоятельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63b1f5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Agglomerative Clustering Experiments ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48595e80d77c497a8964ed6b11cf7ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Agglomerative Hyperparameters:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing params: {'n_clusters': 2, 'linkage': 'ward'} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 69.0\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 55.0\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 67.5\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 28.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.8958 | Total Time: 0.0154s\n",
      "\n",
      "--- Testing params: {'n_clusters': 2, 'linkage': 'complete'} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 69.0\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 55.0\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 67.5\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 28.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.4614 | Total Time: 0.0247s\n",
      "\n",
      "--- Testing params: {'n_clusters': 2, 'linkage': 'average'} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 69.0\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 55.0\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 67.5\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 2\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 28.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.5330 | Total Time: 0.0213s\n",
      "\n",
      "--- Testing params: {'n_clusters': 3, 'linkage': 'ward'} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 3\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 46.0\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 3\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 36.7\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 3\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 45.0\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 3\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 18.7\n",
      "  --------------------\n",
      "  Avg ARI: 0.6355 | Total Time: 0.0109s\n",
      "\n",
      "--- Testing params: {'n_clusters': 4, 'linkage': 'ward'} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 4\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 34.5\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 4\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 27.5\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 4\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 33.8\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 4\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 14.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.4852 | Total Time: 0.0132s\n",
      "\n",
      "--- Testing params: {'n_clusters': None, 'distance_threshold': 0.1, 'linkage': 'complete', 'metric': 'cosine'} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 3\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 46.0\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 4\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 27.5\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 8\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 16.9\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 3\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 18.7\n",
      "  --------------------\n",
      "  Avg ARI: 0.6509 | Total Time: 0.0176s\n",
      "\n",
      "--- Testing params: {'n_clusters': None, 'distance_threshold': 0.2, 'linkage': 'complete', 'metric': 'cosine'} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 138.0\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 110.0\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 135.0\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 56.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.0000 | Total Time: 0.0190s\n",
      "\n",
      "--- Testing params: {'n_clusters': None, 'distance_threshold': 0.3, 'linkage': 'complete', 'metric': 'cosine'} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 138.0\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 110.0\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 135.0\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 56.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.0000 | Total Time: 0.0164s\n",
      "\n",
      "--- Testing params: {'n_clusters': None, 'distance_threshold': 0.5, 'linkage': 'average', 'metric': 'cosine'} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 138.0\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 110.0\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 135.0\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 1\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 56.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.0000 | Total Time: 0.0199s\n",
      "\n",
      "--- Agglomerative Clustering Final Summary ---\n",
      "Best ARI Score: 0.8958 with params: {'n_clusters': 2, 'linkage': 'ward'}\n",
      "Fastest Run: 0.0109s with params: {'n_clusters': 3, 'linkage': 'ward'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "print(\"\\n--- Running Agglomerative Clustering Experiments ---\")\n",
    "\n",
    "# Гиперпараметры\n",
    "agg_params_to_test = [\n",
    "    {'n_clusters': 2, 'linkage': 'ward'},\n",
    "    {'n_clusters': 2, 'linkage': 'complete'},\n",
    "    {'n_clusters': 2, 'linkage': 'average'},\n",
    "    {'n_clusters': 3, 'linkage': 'ward'},\n",
    "    {'n_clusters': 4, 'linkage': 'ward'},\n",
    "    {'n_clusters': None, 'distance_threshold': 0.1, 'linkage': 'complete', 'metric': 'cosine'},\n",
    "    {'n_clusters': None, 'distance_threshold': 0.2, 'linkage': 'complete', 'metric': 'cosine'},\n",
    "    {'n_clusters': None, 'distance_threshold': 0.3, 'linkage': 'complete', 'metric': 'cosine'},\n",
    "    {'n_clusters': None, 'distance_threshold': 0.5, 'linkage': 'average', 'metric': 'cosine'},\n",
    "]\n",
    "\n",
    "agg_results = []\n",
    "\n",
    "for params in tqdm(agg_params_to_test, desc=\"Agglomerative Hyperparameters\"):\n",
    "    print(f\"\\n--- Testing params: {params} ---\")\n",
    "    total_ari = 0\n",
    "    total_time = 0\n",
    "\n",
    "    for word in unique_words:\n",
    "        word_df = df_embedded[df_embedded['word'] == word]\n",
    "        embeddings = np.vstack(word_df['embedding'].values)\n",
    "        gold_labels = word_df['gold_sense_id'].values\n",
    "\n",
    "        current_params = params.copy()\n",
    "        if current_params.get('linkage') != 'ward':\n",
    "            current_params['metric'] = 'cosine'\n",
    "\n",
    "        clusterer = AgglomerativeClustering(**current_params)\n",
    "\n",
    "        start_time = time.time()\n",
    "        predicted_labels = clusterer.fit_predict(embeddings)\n",
    "        end_time = time.time()\n",
    "\n",
    "        gold_counts = collections.Counter(gold_labels)\n",
    "        pred_counts = collections.Counter(predicted_labels)\n",
    "        print(f\"  Word: '{word}'\")\n",
    "        print(\n",
    "            f\"    - Clusters (Gold/Pred): {len(gold_counts)} / {len(pred_counts)}\")\n",
    "        print(\n",
    "            f\"    - Avg. Contexts/Cluster (Gold/Pred): {np.mean(list(gold_counts.values())):.1f} / {np.mean(list(pred_counts.values())):.1f}\")\n",
    "\n",
    "        total_time += (end_time - start_time)\n",
    "        total_ari += adjusted_rand_score(gold_labels, predicted_labels)\n",
    "\n",
    "    avg_ari = total_ari / len(unique_words)\n",
    "    agg_results.append({'params': params, 'ari': avg_ari, 'time': total_time})\n",
    "    print(\n",
    "        f\"  --------------------\\n  Avg ARI: {avg_ari:.4f} | Total Time: {total_time:.4f}s\")\n",
    "\n",
    "# Финальные результаты\n",
    "best_ari_run_agg = max(agg_results, key=lambda x: x['ari'])\n",
    "fastest_run_agg = min(agg_results, key=lambda x: x['time'])\n",
    "\n",
    "print(\"\\n--- Agglomerative Clustering Final Summary ---\")\n",
    "print(\n",
    "    f\"Best ARI Score: {best_ari_run_agg['ari']:.4f} with params: {best_ari_run_agg['params']}\")\n",
    "print(\n",
    "    f\"Fastest Run: {fastest_run_agg['time']:.4f}s with params: {fastest_run_agg['params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c9a685",
   "metadata": {},
   "source": [
    "### Эксперимент 5: HDBSCAN\n",
    "\n",
    "HDBSCAN (Иерархический DBSCAN) — это усовершенствованная версия алгоритма DBSCAN. Он также ищет кластеры как области с высокой плотностью точек, но делает это более гибко. Вместо того чтобы искать кластеры на одном уровне плотности, который нужно задать вручную (параметр eps в DBSCAN), HDBSCAN рассматривает все возможные плотности одновременно. Он строит иерархию вложенных друг в друга кластеров, а затем выбирает из неё самые стабильные и устойчивые группы.\n",
    "Основное отличие от DBSCAN заключается в том, что HDBSCAN не требует подбора параметра eps, что делает его способным находить кластеры разной плотности в одном и том же наборе данных. Главное преимущество этого алгоритма — он более надёжен и прост в настройке, так как требует подбора всего одного интуитивно понятного параметра (min_cluster_size). Но мы будем подбирать ещё и `min_samples`, а также попробуем две разные метрики.\n",
    "\n",
    "Мы не будем использовать реализацию из Sklearn, а воспользуемся той, что есть в отдельной библиотеке `hdbscan`: она более правильная, простая в настройке и в целом «продовая»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1cc7e6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running HDBSCAN Experiments ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbca302a9ea847f1ab72076044223970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HDBSCAN Hyperparameters:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing params: {'min_cluster_size': 5, 'min_samples': 5, 'metric': 'euclidean'} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 4 (+ 33 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 26.2\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 2 (+ 9 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 50.5\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 5 (+ 28 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 21.4\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 2 (+ 10 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 23.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.5602 | Total Time: 0.1262s\n",
      "\n",
      "--- Testing params: {'min_cluster_size': 8, 'min_samples': 8, 'metric': 'euclidean'} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 2 (+ 52 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 43.0\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 2 (+ 22 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 44.0\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 4 (+ 29 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 26.5\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 2 (+ 16 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 20.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.4476 | Total Time: 0.1079s\n",
      "\n",
      "--- Testing params: {'min_cluster_size': 10, 'min_samples': 5, 'metric': 'euclidean'} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 2 (+ 35 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 51.5\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 2 (+ 9 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 50.5\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 3 (+ 14 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 40.3\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 2 (+ 10 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 23.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.6267 | Total Time: 0.1007s\n",
      "\n",
      "--- Testing params: {'min_cluster_size': 12, 'metric': 'euclidean'} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 2 (+ 75 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 31.5\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 2 (+ 44 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 33.0\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 3 (+ 16 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 39.7\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 0 (+ 56 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 0.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.2527 | Total Time: 0.1025s\n",
      "\n",
      "--- Testing params: {'min_cluster_size': 12, 'min_samples': 5, 'metric': 'euclidean'} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 2 (+ 35 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 51.5\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 2 (+ 9 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 50.5\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 3 (+ 14 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 40.3\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 2 (+ 10 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 23.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.6267 | Total Time: 0.1071s\n",
      "\n",
      "--- Testing params: {'min_cluster_size': 10, 'metric': 'manhattan'} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 2 (+ 51 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 43.5\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 2 (+ 30 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 40.0\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 3 (+ 16 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 39.7\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 2 (+ 17 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 19.5\n",
      "  --------------------\n",
      "  Avg ARI: 0.4478 | Total Time: 0.1074s\n",
      "\n",
      "--- Testing params: {'min_cluster_size': 15, 'metric': 'manhattan'} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 0 (+ 138 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 0.0\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 2 (+ 52 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 29.0\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 2 (+ 22 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 56.5\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 0 (+ 56 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 0.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.2206 | Total Time: 0.1016s\n",
      "\n",
      "--- Testing params: {'min_cluster_size': 5, 'min_samples': 3, 'metric': 'manhattan'} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 4 (+ 22 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 29.0\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 5 (+ 35 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 15.0\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 8 (+ 32 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 12.9\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 2 (+ 9 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 23.5\n",
      "  --------------------\n",
      "  Avg ARI: 0.4273 | Total Time: 0.1034s\n",
      "\n",
      "--- Testing params: {'min_cluster_size': 10, 'min_samples': 5, 'metric': 'manhattan'} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 2 (+ 23 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 57.5\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 2 (+ 7 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 51.5\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 3 (+ 14 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 40.3\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 2 (+ 12 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 22.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.6523 | Total Time: 0.1025s\n",
      "\n",
      "--- Testing params: {'min_cluster_size': 12, 'min_samples': 5, 'metric': 'manhattan'} ---\n",
      "  Word: 'замок'\n",
      "    - Clusters (Gold/Pred): 2 / 2 (+ 23 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 69.0 / 57.5\n",
      "  Word: 'лук'\n",
      "    - Clusters (Gold/Pred): 2 / 2 (+ 7 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 55.0 / 51.5\n",
      "  Word: 'суда'\n",
      "    - Clusters (Gold/Pred): 2 / 3 (+ 14 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 67.5 / 40.3\n",
      "  Word: 'бор'\n",
      "    - Clusters (Gold/Pred): 2 / 2 (+ 12 noise points)\n",
      "    - Avg. Contexts/Cluster (Gold/Pred): 28.0 / 22.0\n",
      "  --------------------\n",
      "  Avg ARI: 0.6523 | Total Time: 0.1114s\n",
      "\n",
      "--- HDBSCAN Final Summary ---\n",
      "Best ARI Score: 0.6523 with params: {'min_cluster_size': 10, 'min_samples': 5, 'metric': 'manhattan'}\n",
      "Fastest Run: 0.1007s with params: {'min_cluster_size': 10, 'min_samples': 5, 'metric': 'euclidean'}\n"
     ]
    }
   ],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "\n",
    "print(\"\\n--- Running HDBSCAN Experiments ---\")\n",
    "\n",
    "# Гиперпараметры\n",
    "hdbscan_params_to_test = [\n",
    "    {'min_cluster_size': 5, 'min_samples': 5, 'metric': 'euclidean'},\n",
    "    {'min_cluster_size': 8, 'min_samples': 8, 'metric': 'euclidean'},\n",
    "    {'min_cluster_size': 10, 'min_samples': 5, 'metric': 'euclidean'},\n",
    "    {'min_cluster_size': 12, 'metric': 'euclidean'},\n",
    "    {'min_cluster_size': 12, 'min_samples': 5, 'metric': 'euclidean'},\n",
    "    {'min_cluster_size': 10, 'metric': 'manhattan'},\n",
    "    {'min_cluster_size': 15, 'metric': 'manhattan'},\n",
    "    {'min_cluster_size': 5, 'min_samples': 3, 'metric': 'manhattan'},\n",
    "    {'min_cluster_size': 10, 'min_samples': 5, 'metric': 'manhattan'},\n",
    "    {'min_cluster_size': 12, 'min_samples': 5, 'metric': 'manhattan'},\n",
    "]\n",
    "\n",
    "hdbscan_results = []\n",
    "\n",
    "for params in tqdm(hdbscan_params_to_test, desc=\"HDBSCAN Hyperparameters\"):\n",
    "    print(f\"\\n--- Testing params: {params} ---\")\n",
    "    total_ari = 0\n",
    "    total_time = 0\n",
    "\n",
    "    for word in unique_words:\n",
    "        word_df = df_embedded[df_embedded['word'] == word]\n",
    "        embeddings = np.vstack(word_df['embedding'].values)\n",
    "        gold_labels = word_df['gold_sense_id'].values\n",
    "\n",
    "        clusterer = HDBSCAN(**params)\n",
    "\n",
    "        start_time = time.time()\n",
    "        predicted_labels = clusterer.fit_predict(embeddings)\n",
    "        end_time = time.time()\n",
    "\n",
    "        gold_counts = collections.Counter(gold_labels)\n",
    "        pred_counts = collections.Counter(predicted_labels)\n",
    "        num_pred_clusters = len(pred_counts) - (1 if -1 in pred_counts else 0)\n",
    "\n",
    "        print(f\"  Word: '{word}'\")\n",
    "        print(\n",
    "            f\"    - Clusters (Gold/Pred): {len(gold_counts)} / {num_pred_clusters} (+ {pred_counts.get(-1, 0)} noise points)\")\n",
    "        print(\n",
    "            f\"    - Avg. Contexts/Cluster (Gold/Pred): {np.mean(list(gold_counts.values())):.1f} / {np.mean([c for k, c in pred_counts.items() if k != -1]) if num_pred_clusters > 0 else 0:.1f}\")\n",
    "\n",
    "        total_time += (end_time - start_time)\n",
    "        if len(set(predicted_labels)) > 1:\n",
    "            total_ari += adjusted_rand_score(gold_labels, predicted_labels)\n",
    "        else:\n",
    "            total_ari += 0\n",
    "\n",
    "    avg_ari = total_ari / len(unique_words)\n",
    "    hdbscan_results.append(\n",
    "        {'params': params, 'ari': avg_ari, 'time': total_time})\n",
    "    print(\n",
    "        f\"  --------------------\\n  Avg ARI: {avg_ari:.4f} | Total Time: {total_time:.4f}s\")\n",
    "\n",
    "# Финальные результаты\n",
    "best_ari_run_hdb = max(hdbscan_results, key=lambda x: x['ari'])\n",
    "fastest_run_hdb = min(hdbscan_results, key=lambda x: x['time'])\n",
    "\n",
    "print(\"\\n--- HDBSCAN Final Summary ---\")\n",
    "print(\n",
    "    f\"Best ARI Score: {best_ari_run_hdb['ari']:.4f} with params: {best_ari_run_hdb['params']}\")\n",
    "print(\n",
    "    f\"Fastest Run: {fastest_run_hdb['time']:.4f}s with params: {fastest_run_hdb['params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c674e4",
   "metadata": {},
   "source": [
    "Выходит, что на нашей задаче лучше всего работают алгоритмы, в которых можно эксплицитно задать количество кластеров, что, наверное, ожидаемо. У наших данных нет разнообразия, у всех представленных слов по два значения, и для всех есть примерно равное количество контекстов. При этом Agglomerative Clustering работает лучше, чем K_means (~89.5% против ~83% при лучших подобранных гиперпараметрах), но K-means работает незначительно быстрее.\n",
    "\n",
    "Проблема с этими алгоритмами в том, что для их реального применения для WSI мы должны точно знать, сколько предполагается значений у многозначного слова (скажем, инферировать эти значения из словарей или ещё как-то), в некотором смысле на наших данных задание количества кластеров сработало как \"трейн на тесте\", потому что мы знали, сколько значений есть у каждого слова, т.е. сколько должно быть кластеров, и по сути решали только задачу распределения контекстов по уже известным кластерам, что по сути уже практически классификация. Как подобрать количество кластеров для неизвестной выборки — не вполне понятно. Хотя, конечно, при намеренном задании кластеров 3-4 для этих алгоритмов значение метрики падало не так катострофически, так что можно попробовать. В целом же, видимо, надо опираться на какую-то статистику: например, взять много многозначных слов с известными контекстами и посмотреть, сколько в среднем у слова значений.\n",
    "\n",
    "Что касается других алгоритмов, где мы не можем эксплицитно задавать количество кластеров, относительно неплохо справился только HDBSCAN. Там почти при всех параметрах получалось небольшое число кластеров, по которым распределялись контексты. Но проблема этого алгоритма в том, что значительное количество данных уходит в «шумовой» кластер, поэтому даже при верном определении количества кластеров туда просто не попадают многие контексты. Остальные же алгоритмы с задачей не справились практически совсем: DBSCAN показывает нулевые результаты, а AP — очень низкие, да и то — при очень тщательной настройке параметра `preference`, который позволяет уменьшить количество кластеров. Алгоритм Agglomerative Clustering можно настроить так, чтобы он сам пытался определять количество кластеров, основываясь на расстоянии, но это не сработало.\n",
    "\n",
    "Про время работы: так как у нас очень мало данных, все алгоритмы работают быстро, и на пользовательском уровне разница не чувствуется. Однако при просмотре статистики видно, что HDBSCAN работает в ~10 раз медленнее, чем все остальные алгоритмы, тогда как остальные алгоритмы кластеризации находятся примерно в одном диапазоне. Самые быстрые — AP и KMeans.\n",
    "\n",
    "Что касается интуитивности параметров, то, как кажется, количество кластеров у алгоритмов, где этот параметр есть, как раз и есть самый понятный и настраиваемый параметр (но у него есть проблемы, см. выше). Назначение других параметров плохо понятно без документации, для использования скорее удобнее запомнить, как они влияют на количество и плотность кластеров (как, например, `preference` в AP). Хотя параметры `min_cluster_size` и `min_samples` в HDBSCAN тоже выглядят довольно интуитивно, сразу +- понятно, за что они отвечают.\n",
    "\n",
    "В целом можно сказать, что выбранная нами модель производит достаточно неплохие для различения контекстов эмбеддинги, но без точного количества этих значений кластеризация затруднена. Возможно также, что нам следовало использовать не эмбеддинг всего предложения, а только эмбеддинг, составленный из токенов таргетного слова — может быть, тогда бы результат был бы лучше. Кроме того, мы не экспериментировали с Umap и понижением размерности, в то время как для некоторых алгоритмов типа HDBSCAN это могло бы быть полезно. Ну и, наконец, никто не мешает дообучить Sentence-Transformers-модельку на задачу WSI. Та же задача sentence similarity, но тут мы просто сближаем эмбеддинги похожих контекстов. Опционально можно и отдалять эмбеддинги непохожих (контрастивное дообучение)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
