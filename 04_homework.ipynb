{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48abae58",
   "metadata": {},
   "source": [
    "# Задание 1\n",
    "\n",
    "\n",
    "Выберите 5 языков в википедии (не тех, что использовались в семинаре). Скачайте по 10 случайных статей для каждого языка. Предобработайте тексты, удаляя лишние теги/отступы/разделители (если они есть). Разделите тексты на предложения и создайте датасет, в котором каждому предложению соответствует язык. Кластеризуйте тексты, используя эбмединг модель из прошлого семинара и любой алгоритм кластеризации. Проверьте качество кластеризации с помощь метрики ARI. Отдельно проанализируйте 3 ошибочно кластеризованных текста (если такие есть)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1769f07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 10:32:40.414589: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759908760.430316   36566 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1759908760.435001   36566 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1759908760.447533   36566 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759908760.447556   36566 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759908760.447558   36566 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759908760.447559   36566 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-10-08 10:32:40.451978: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package punkt to /home/futyn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/futyn/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wikipedia\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaad99c",
   "metadata": {},
   "source": [
    "## 1. Загружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b7beeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_with_disambiguation(page):\n",
    "    # Функция для загрузки страницы с обработкой неоднозначностей\n",
    "    try:\n",
    "        p = wikipedia.page(page, auto_suggest=False)\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        random_option = np.random.choice(e.options)\n",
    "        p = load_with_disambiguation(random_option)\n",
    "    return p\n",
    "\n",
    "\n",
    "def get_texts_for_lang(lang, n=10):\n",
    "    # Функция для скачивания n случайных статей для заданного языка\n",
    "    wikipedia.set_lang(lang)\n",
    "    wiki_content = []\n",
    "    pages = wikipedia.random(n)\n",
    "\n",
    "    for page_name in tqdm(pages, desc=f'Loading pages for {lang}'):\n",
    "        try:\n",
    "            page = load_with_disambiguation(page_name)\n",
    "            # Убираем заголовки секций и лишние переносы строк\n",
    "            content = re.sub(r'==.*?==', '', page.content)\n",
    "            content = re.sub(r'\\n+', '\\n', content).strip()\n",
    "            if content:  # Добавляем только если есть контент\n",
    "                wiki_content.append(content)\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f'Skipping page {page_name} in language {lang} due to error: {e}')\n",
    "            continue\n",
    "    return wiki_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f06797a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a4ee113a13545db9b3a69bd46a59781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pages for fr:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/futyn/miniconda3/lib/python3.12/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/futyn/miniconda3/lib/python3.12/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 10 articles for language: fr\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ed450004ac4e51985fb1f794695e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pages for de:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 10 articles for language: de\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "134b6bedd1c24e99acfc526341c900e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pages for es:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 10 articles for language: es\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d7f4d5e85242258c1f9c300042cf42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pages for sv:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 10 articles for language: sv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78315a8edda54ea1b47ee46c265821fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pages for da:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping page Nørreby in language da due to error: Page id \"Nørreby (Femø Sogn)\" does not match any pages. Try another id!\n",
      "Downloaded 9 articles for language: da\n",
      "CPU times: user 1.31 s, sys: 146 ms, total: 1.46 s\n",
      "Wall time: 4min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Французский, немецкий, испанский, шведский, датский\n",
    "langs = ['fr', 'de', 'es', 'sv', 'da']\n",
    "wiki_texts = {}\n",
    "\n",
    "for lang in langs:\n",
    "    wiki_texts[lang] = get_texts_for_lang(lang, n=10)\n",
    "    print(f'Downloaded {len(wiki_texts[lang])} articles for language: {lang}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a320dab",
   "metadata": {},
   "source": [
    "## 2. Готовим датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e393925a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 1122\n"
     ]
    }
   ],
   "source": [
    "lang_map_nltk = {\n",
    "    'fr': 'french',\n",
    "    'de': 'german',\n",
    "    'es': 'spanish',\n",
    "    'sv': 'swedish',\n",
    "    'da': 'danish'\n",
    "}\n",
    "\n",
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "for lang, texts in wiki_texts.items():\n",
    "    for text in texts:\n",
    "        sents = nltk.sent_tokenize(text, language=lang_map_nltk[lang])\n",
    "        sentences.extend(sents)\n",
    "        labels.extend([lang] * len(sents))\n",
    "\n",
    "print(f'Total sentences: {len(sentences)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31508069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Ice Warriors (Les Guerriers de Glace) est le trente-neuvième épisode de la première série de la série télévisée britannique de science-fiction Doctor Who, diffusé pour la première fois en six parties hebdomadaires du 11 novembre au 16 décembre 1967, il montre pour la première fois les \"Guerriers des glaces\" des ennemis récurrents du Docteur.',\n",
       " 'Sur les six parties de cet épisode, deux sont manquantes.',\n",
       " \"Le TARDIS arrive sur la Terre du futur en proie à une nouvelle ère glaciaire, et les voyageurs trouvent le chemin vers une base scientifique où une créature à l'apparence d'un humain géant a été trouvé prisonnier des glaces.\",\n",
       " 'Ces étranges créatures se trouvent être des guerriers venus de Mars et prêts à tout pour retrouver leur planète.',\n",
       " \"Patrick Troughton — Le Docteur\\nFrazer Hines — Jamie McCrimmon\\nDeborah Watling  — Victoria Waterfield\\nPeter Barkworth — Clent\\nPeter Sallis — Penley\\nWendy Gifford — Miss Garrett\\nAngus Lennie — Storr\\nPeter Diamond — Davis\\nGeorge Waring — Arden\\nMalcolm Taylor — Walters\\nRoy Skelton — Voix de l'ordinateur\\nBernard Bresslaw — Varga\\nRoger Jones — Zondal\\nSonny Caldinez — Turoc\\nMichael Attwell — Isbur\\nTony Harwood — Rintan\\nL'épisode commence dans un futur lointain où une nouvelle ère glaciaire a eu lieu à la suite des pollutions terrestres.\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad993d2a",
   "metadata": {},
   "source": [
    "## 3. Эмбеддинги\n",
    "\n",
    "Будем использовать уже полюбившуюся инструктивную E5-модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e51b9e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'intfloat/multilingual-e5-large-instruct'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07eba271",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_instruction = 'Identify the language of the text'\n",
    "\n",
    "formatted_sentences = [\n",
    "    f'Instruct: {task_instruction}\\nQuery: {s}' for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28c82e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285d3c45850d41ccb6f1fcfebe811efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = model.encode(\n",
    "    formatted_sentences,\n",
    "    normalize_embeddings=True,\n",
    "    show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21b726d",
   "metadata": {},
   "source": [
    "## 4. Кластеризация\n",
    "\n",
    "В прошлый раз лучше всего себя показал AgglomerativeClustering. Учитывая, что мы можем задать количество кластеров (по количеству языков), это представляется лучшим выбором."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d622bc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 260 ms, sys: 8.41 ms, total: 269 ms\n",
      "Wall time: 275 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clusterizer = AgglomerativeClustering(n_clusters=len(langs), linkage='ward')\n",
    "cluster_labels = clusterizer.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12d0b965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Index (ARI): 0.7086\n"
     ]
    }
   ],
   "source": [
    "ari_score = adjusted_rand_score(labels, cluster_labels)\n",
    "print(f'Adjusted Rand Index (ARI): {ari_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940ddf02",
   "metadata": {},
   "source": [
    "## 5. Анализ ошибок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96d95dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Датафрейм, чтобы было удобнее анализировать\n",
    "df = pd.DataFrame({'sentence': sentences,\n",
    "                   'true_label': labels,\n",
    "                   'cluster_id': cluster_labels})\n",
    "\n",
    "# Находим наиболее частый язык для каждого кластера\n",
    "cluster_to_lang_map = {}\n",
    "for i in range(len(langs)):\n",
    "    # Фильтруем по кластеру и находим самый частый язык\n",
    "    most_common_lang = df[df['cluster_id'] == i]['true_label'].mode()[0]\n",
    "    cluster_to_lang_map[i] = most_common_lang\n",
    "\n",
    "# Применяем маппинг для получения предсказанных языков\n",
    "df['predicted_label'] = df['cluster_id'].map(cluster_to_lang_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87f250fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 1122\n",
      "Correctly clustered: 1043\n",
      "Incorrectly clustered: 79\n"
     ]
    }
   ],
   "source": [
    "# Считаем количество верных и неверных предсказаний\n",
    "correct_predictions = (df['true_label'] == df['predicted_label']).sum()\n",
    "incorrect_predictions = len(df) - correct_predictions\n",
    "\n",
    "print(f\"Total sentences: {len(df)}\")\n",
    "print(f\"Correctly clustered: {correct_predictions}\")\n",
    "print(f\"Incorrectly clustered: {incorrect_predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf4b0181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of erroneously clustered texts:\n",
      "\n",
      "--- Language: FR ---\n",
      "Found 5 errors. Examples:\n",
      "  - Sentence: '=\n",
      "2003 : Evil Never Dies\n",
      "2005 : Hell on Earth\n",
      "2008 : An Overdose of Death...\n",
      "2011 : Conjure and Command\n",
      "2013 : Chemistry of Conciousness\n",
      "2019  : Primal Future: 2019\n",
      "=\n",
      "1999 : Radiation Sickness\n",
      "2002 : Critical Mass\n",
      "2004 : Promo 2004\n",
      "=\n",
      "2001 : Toxic Holocaust - Orichniki\n",
      "2002 : Implements of Mass Destruction Nuclear Apocalypse:666\n",
      "2004 : Outbreak of Evil\n",
      "2004 : Thrashbeast from Hell\n",
      "2005 : Blasphemy, Mayhem, War\n",
      "2005 : HRPS Vol.1'\n",
      "2006 : Don't Burn the Witch\n",
      "2008 : Speed n' Spikes Vol.'\n",
      "    - True: fr, Predicted as: da\n",
      "\n",
      "  - Sentence: 'J.-C.'\n",
      "    - True: fr, Predicted as: da\n",
      "\n",
      "  - Sentence: '].'\n",
      "    - True: fr, Predicted as: da\n",
      "\n",
      "--- Language: DE ---\n",
      "Found 34 errors. Examples:\n",
      "  - Sentence: '„I have spoken“.'\n",
      "    - True: de, Predicted as: da\n",
      "\n",
      "  - Sentence: 'XLI.'\n",
      "    - True: de, Predicted as: da\n",
      "\n",
      "  - Sentence: 'The Edwin Mellen Press, Lewiston/Queenston/Lampeter 1993.'\n",
      "    - True: de, Predicted as: da\n",
      "\n",
      "--- Language: ES ---\n",
      "Found 7 errors. Examples:\n",
      "  - Sentence: 'UU.'\n",
      "    - True: es, Predicted as: da\n",
      "\n",
      "  - Sentence: 'A. Cast.'\n",
      "    - True: es, Predicted as: da\n",
      "\n",
      "  - Sentence: 'Tephrocactus mandragora Backeb.'\n",
      "    - True: es, Predicted as: da\n",
      "\n",
      "--- Language: SV ---\n",
      "Found 33 errors. Examples:\n",
      "  - Sentence: 'Beauville är en kommun i departementet Haute-Garonne i regionen Occitanien i södra Frankrike.'\n",
      "    - True: sv, Predicted as: da\n",
      "\n",
      "  - Sentence: 'Kommunen ligger i kantonen Caraman som tillhör arrondissementet Toulouse.'\n",
      "    - True: sv, Predicted as: da\n",
      "\n",
      "  - Sentence: 'År 2022 hade Beauville 170 invånare.'\n",
      "    - True: sv, Predicted as: da\n",
      "\n",
      "--- Language: DA ---\n",
      "No clustering errors found for this language.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Находим все ошибочно кластеризованные тексты\n",
    "misclassified = df[df['true_label'] != df['predicted_label']]\n",
    "\n",
    "print(\"Analysis of erroneously clustered texts:\\n\")\n",
    "\n",
    "for lang in langs:\n",
    "    lang_errors = misclassified[misclassified['true_label'] == lang]\n",
    "    if not lang_errors.empty:\n",
    "        print(f\"--- Language: {lang.upper()} ---\")\n",
    "        print(f\"Found {len(lang_errors)} errors. Examples:\")\n",
    "        for _, row in lang_errors.head(3).iterrows():\n",
    "            print(f\"  - Sentence: '{row['sentence']}'\")\n",
    "            print(\n",
    "                f\"    - True: {row['true_label']}, Predicted as: {row['predicted_label']}\\n\")\n",
    "    else:\n",
    "        print(f\"--- Language: {lang.upper()} ---\")\n",
    "        print(\"No clustering errors found for this language.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b47cb6",
   "metadata": {},
   "source": [
    "В целом, эмбеддинги достаточно качественные и кластеризуются очень неплохо. Ошибки в большинстве случаев связаны с шумом и мусором, не очень понятно, как его вычистить: названия произведений, ссылки на дополнительную литературу на английском и так далее. В одном из немецких текстов, видимо, попались какие-то цитаты на английском.\n",
    "\n",
    "По-настоящему путаются только похожие языки: датский и шведский. Мы видим много валидных шведских предложений, которые классифицированы как датские. Это ожидаемо, так как языки родственные и сильно похожи и, видимо, фичи их эмбеддингов во многом совпадают. Забавно, что обратной ситуации не встречается, да и вообще кластер датского языка перетягивает на себя все ошибки, хотя все сами датские предложения кластеризовались верно. По всей видимости, сюда попадают все предложения, для которых не удалось выявить уверенных паттернов для отнесения к другим кластерам."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd75839",
   "metadata": {},
   "source": [
    "# Задание 2\n",
    "\n",
    "Загрузите корпус `annot.opcorpora.no_ambig_strict.xml.bz2` с OpenCorpora. Найдите в корпусе самые частотные морфологически омонимичные словоформы (те, которым соответствует разный грамматический разбор в разных предложениях). Также найдите словоформы с самых большим количеством вариантов грамматических разборов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a211751c",
   "metadata": {},
   "source": [
    "## 1. Загружаем и открываем корпус"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f5edc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-10-08 11:27:12--  http://opencorpora.org/files/export/annot/annot.opcorpora.no_ambig_strict.xml.bz2\n",
      "Resolving opencorpora.org (opencorpora.org)... 172.67.163.210, 104.21.15.199, 2606:4700:3031::ac43:a3d2, ...\n",
      "Connecting to opencorpora.org (opencorpora.org)|172.67.163.210|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://opencorpora.org/files/export/annot/annot.opcorpora.no_ambig_strict.xml.bz2 [following]\n",
      "--2025-10-08 11:27:12--  https://opencorpora.org/files/export/annot/annot.opcorpora.no_ambig_strict.xml.bz2\n",
      "Connecting to opencorpora.org (opencorpora.org)|172.67.163.210|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1435452 (1.4M) [application/x-bzip2]\n",
      "Saving to: ‘annot.opcorpora.no_ambig_strict.xml.bz2’\n",
      "\n",
      "annot.opcorpora.no_ 100%[===================>]   1.37M  3.95MB/s    in 0.3s    \n",
      "\n",
      "2025-10-08 11:27:13 (3.95 MB/s) - ‘annot.opcorpora.no_ambig_strict.xml.bz2’ saved [1435452/1435452]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://opencorpora.org/files/export/annot/annot.opcorpora.no_ambig_strict.xml.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a27fff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "\n",
    "with bz2.open('annot.opcorpora.no_ambig_strict.xml.bz2', 'rb') as f_in, open('annot.opcorpora.no_ambig_strict.xml', 'wb') as f_out:\n",
    "    f_out.write(f_in.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1889d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "open_corpora = etree.fromstring(\n",
    "    open('annot.opcorpora.no_ambig_strict.xml', 'rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50454381",
   "metadata": {},
   "source": [
    "## 2. Собираем и структурируем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "136bba8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713b170052d344f8ac5b58972862e0a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analyses_by_word = defaultdict(list)\n",
    "\n",
    "# Итерируемся по всем предложениям в корпусе\n",
    "for sentence in tqdm(open_corpora.xpath('//tokens')):\n",
    "    for token in sentence.xpath('token'):\n",
    "        # Получаем словоформу\n",
    "        word_form = token.xpath('@text')\n",
    "        # Получаем грамматическую информацию\n",
    "        gram_info = token.xpath('tfr/v/l/g/@v')\n",
    "\n",
    "        # Проверяем, что оба значения существуют\n",
    "        if word_form and gram_info:\n",
    "            # Сохраняем информацию в виде кортежа, чтобы ее можно было добавить\n",
    "            # в set\n",
    "            analyses_by_word[word_form[0].lower()].append(\n",
    "                tuple(sorted(gram_info)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc3e7a7",
   "metadata": {},
   "source": [
    "## 3. Самые частотные омонимичные формы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13c00b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most frequent morphologically homonymous word forms:\n",
      "- 'в': 2059 occurrences\n",
      "    - Analysis: Abbr, Fixd, NOUN, gent, inan, masc, sing\n",
      "    - Analysis: PREP\n",
      "- 'на': 786 occurrences\n",
      "    - Analysis: PRCL\n",
      "    - Analysis: PREP\n",
      "- 'с': 613 occurrences\n",
      "    - Analysis: PRCL\n",
      "    - Analysis: PREP\n",
      "- 'и': 574 occurrences\n",
      "    - Analysis: CONJ\n",
      "    - Analysis: PRCL\n",
      "- 'о': 213 occurrences\n",
      "    - Analysis: INTJ\n",
      "    - Analysis: PREP\n",
      "- 'году': 115 occurrences\n",
      "    - Analysis: NOUN, inan, loc2, masc, sing\n",
      "    - Analysis: NOUN, datv, inan, masc, sing\n",
      "- 'а': 113 occurrences\n",
      "    - Analysis: CONJ\n",
      "    - Analysis: INTJ\n",
      "- 'этом': 104 occurrences\n",
      "    - Analysis: ADJF, Anph, Apro, Subx, loct, masc, sing\n",
      "    - Analysis: ADJF, Anph, Apro, Subx, loct, neut, sing\n",
      "    - Analysis: NPRO, loct, neut, sing\n",
      "- 'россии': 91 occurrences\n",
      "    - Analysis: Geox, NOUN, Sgtm, datv, femn, inan, sing\n",
      "    - Analysis: Geox, NOUN, Sgtm, femn, gent, inan, sing\n",
      "    - Analysis: Geox, NOUN, Sgtm, femn, inan, loct, sing\n",
      "- 'было': 89 occurrences\n",
      "    - Analysis: PRCL\n",
      "    - Analysis: VERB, impf, indc, intr, neut, past, sing\n",
      "- 'может': 81 occurrences\n",
      "    - Analysis: CONJ, Prnt\n",
      "    - Analysis: 3per, VERB, impf, indc, intr, pres, sing\n",
      "- 'после': 69 occurrences\n",
      "    - Analysis: ADVB\n",
      "    - Analysis: PREP\n",
      "- 'нет': 69 occurrences\n",
      "    - Analysis: INTJ\n",
      "    - Analysis: PRED, pres\n",
      "    - Analysis: PRCL\n",
      "- 'меня': 54 occurrences\n",
      "    - Analysis: 1per, NPRO, accs, sing\n",
      "    - Analysis: 1per, NPRO, gent, sing\n",
      "- 'которые': 53 occurrences\n",
      "    - Analysis: ADJF, Anph, Apro, Subx, accs, inan, plur\n",
      "    - Analysis: ADJF, Anph, Apro, Subx, nomn, plur\n",
      "- 'мне': 51 occurrences\n",
      "    - Analysis: 1per, NPRO, datv, sing\n",
      "    - Analysis: 1per, NPRO, loct, sing\n",
      "- 'под': 51 occurrences\n",
      "    - Analysis: Abbr, Fixd, NOUN, Sgtm, inan, masc, nomn, sing\n",
      "    - Analysis: PREP\n",
      "- 'чем': 50 occurrences\n",
      "    - Analysis: CONJ\n",
      "    - Analysis: NPRO, ablt, neut, sing\n",
      "    - Analysis: NPRO, loct, neut, sing\n",
      "- 'этого': 49 occurrences\n",
      "    - Analysis: ADJF, Anph, Apro, Subx, gent, neut, sing\n",
      "    - Analysis: ADJF, Anph, Apro, Subx, gent, masc, sing\n",
      "    - Analysis: NPRO, gent, neut, sing\n",
      "- 'всех': 48 occurrences\n",
      "    - Analysis: ADJF, Apro, Subx, gent, plur\n",
      "    - Analysis: ADJF, Apro, Subx, accs, anim, plur\n",
      "    - Analysis: ADJF, Apro, Subx, loct, plur\n"
     ]
    }
   ],
   "source": [
    "homonym_frequencies = {}\n",
    "\n",
    "for word, analyses in analyses_by_word.items():\n",
    "    if len(set(analyses)) > 1:\n",
    "        homonym_frequencies[word] = len(analyses)\n",
    "\n",
    "# Сортируем слова по убыванию их частоты\n",
    "sorted_homonyms_by_freq = sorted(\n",
    "    homonym_frequencies.items(),\n",
    "    key=lambda item: item[1],\n",
    "    reverse=True)\n",
    "\n",
    "print(\"Top 20 most frequent morphologically homonymous word forms:\")\n",
    "for word, freq in sorted_homonyms_by_freq[:20]:\n",
    "    print(f\"- '{word}': {freq} occurrences\")\n",
    "    for analysis in set(analyses_by_word[word]):\n",
    "        print(f\"    - Analysis: {', '.join(analysis)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67144bd9",
   "metadata": {},
   "source": [
    "В основном это служебные слова, которые могут относиться к разным частям речи: союзы, предлоги, частицы и местоимения. Есть только два частотных падежных омонима: **году** и **России**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6b5dc6",
   "metadata": {},
   "source": [
    "## 4. Самые грамматически неоднозначные слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ac166f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 word forms with the most grammatical analyses:\n",
      "- 'сша': 6 unique analyses\n",
      "    - Analysis: Abbr, Fixd, GNdr, Geox, NOUN, Pltm, datv, inan, plur\n",
      "    - Analysis: Abbr, Fixd, GNdr, Geox, NOUN, Pltm, gent, inan, plur\n",
      "    - Analysis: Abbr, Fixd, GNdr, Geox, NOUN, Pltm, accs, inan, plur\n",
      "    - Analysis: Abbr, Fixd, GNdr, Geox, NOUN, Pltm, inan, loct, plur\n",
      "    - Analysis: Abbr, Fixd, GNdr, Geox, NOUN, Pltm, ablt, inan, plur\n",
      "    - Analysis: Abbr, Fixd, GNdr, Geox, NOUN, Pltm, inan, nomn, plur\n",
      "- 'кино': 5 unique analyses\n",
      "    - Analysis: Fixd, NOUN, gent, inan, neut, sing\n",
      "    - Analysis: Fixd, NOUN, accs, inan, neut, sing\n",
      "    - Analysis: Fixd, NOUN, datv, inan, neut, sing\n",
      "    - Analysis: Fixd, NOUN, inan, neut, nomn, sing\n",
      "    - Analysis: Fixd, NOUN, inan, loct, neut, sing\n",
      "- 'евро': 5 unique analyses\n",
      "    - Analysis: Fixd, NOUN, gent, inan, neut, sing\n",
      "    - Analysis: Fixd, NOUN, datv, inan, neut, sing\n",
      "    - Analysis: Fixd, NOUN, accs, inan, neut, sing\n",
      "    - Analysis: Fixd, NOUN, inan, neut, nomn, sing\n",
      "    - Analysis: Fixd, NOUN, gent, inan, neut, plur\n",
      "- 'компании': 5 unique analyses\n",
      "    - Analysis: NOUN, datv, femn, inan, sing\n",
      "    - Analysis: NOUN, femn, inan, loct, sing\n",
      "    - Analysis: NOUN, femn, gent, inan, sing\n",
      "    - Analysis: NOUN, femn, inan, nomn, plur\n",
      "    - Analysis: NOUN, accs, femn, inan, plur\n",
      "- 'пути': 5 unique analyses\n",
      "    - Analysis: NOUN, gent, inan, masc, sing\n",
      "    - Analysis: NOUN, inan, loct, masc, sing\n",
      "    - Analysis: NOUN, accs, inan, masc, plur\n",
      "    - Analysis: NOUN, datv, inan, masc, sing\n",
      "    - Analysis: NOUN, inan, masc, nomn, plur\n",
      "- 'какой': 5 unique analyses\n",
      "    - Analysis: ADJF, Apro, masc, nomn, sing\n",
      "    - Analysis: ADJF, Apro, femn, gent, sing\n",
      "    - Analysis: ADJF, Apro, datv, femn, sing\n",
      "    - Analysis: ADJF, Apro, ablt, femn, sing\n",
      "    - Analysis: ADJF, Apro, femn, loct, sing\n",
      "- 'это': 4 unique analyses\n",
      "    - Analysis: NPRO, neut, nomn, sing\n",
      "    - Analysis: NPRO, accs, neut, sing\n",
      "    - Analysis: ADJF, Anph, Apro, Subx, accs, neut, sing\n",
      "    - Analysis: PRCL\n",
      "- 'одной': 4 unique analyses\n",
      "    - Analysis: ADJF, Anum, Apro, datv, femn, sing\n",
      "    - Analysis: ADJF, Anum, Apro, femn, loct, sing\n",
      "    - Analysis: ADJF, Anum, Apro, ablt, femn, sing\n",
      "    - Analysis: ADJF, Anum, Apro, femn, gent, sing\n",
      "- 'своей': 4 unique analyses\n",
      "    - Analysis: ADJF, Anph, Apro, datv, femn, sing\n",
      "    - Analysis: ADJF, Anph, Apro, ablt, femn, sing\n",
      "    - Analysis: ADJF, Anph, Apro, femn, gent, sing\n",
      "    - Analysis: ADJF, Anph, Apro, femn, loct, sing\n",
      "- 'лица': 4 unique analyses\n",
      "    - Analysis: NOUN, gent, inan, neut, sing\n",
      "    - Analysis: NOUN, anim, gent, neut, sing\n",
      "    - Analysis: NOUN, accs, inan, neut, plur\n",
      "    - Analysis: NOUN, anim, neut, nomn, plur\n",
      "- 'этой': 4 unique analyses\n",
      "    - Analysis: ADJF, Anph, Apro, Subx, ablt, femn, sing\n",
      "    - Analysis: ADJF, Anph, Apro, Subx, femn, gent, sing\n",
      "    - Analysis: ADJF, Anph, Apro, Subx, femn, loct, sing\n",
      "    - Analysis: ADJF, Anph, Apro, Subx, datv, femn, sing\n",
      "- 'версии': 4 unique analyses\n",
      "    - Analysis: NOUN, femn, inan, loct, sing\n",
      "    - Analysis: NOUN, femn, inan, nomn, plur\n",
      "    - Analysis: NOUN, femn, gent, inan, sing\n",
      "    - Analysis: NOUN, datv, femn, inan, sing\n",
      "- 'нато': 4 unique analyses\n",
      "    - Analysis: Abbr, Fixd, NOUN, Orgn, Sgtm, femn, gent, inan, sing\n",
      "    - Analysis: Abbr, Fixd, NOUN, Orgn, Sgtm, femn, inan, nomn, sing\n",
      "    - Analysis: Abbr, Fixd, NOUN, Orgn, Sgtm, accs, femn, inan, sing\n",
      "    - Analysis: Abbr, Fixd, NOUN, Orgn, Sgtm, femn, inan, loct, sing\n",
      "- 'истории': 4 unique analyses\n",
      "    - Analysis: NOUN, femn, inan, loct, sing\n",
      "    - Analysis: NOUN, femn, inan, nomn, plur\n",
      "    - Analysis: NOUN, femn, gent, inan, sing\n",
      "    - Analysis: NOUN, datv, femn, inan, sing\n",
      "- 'этим': 4 unique analyses\n",
      "    - Analysis: ADJF, Anph, Apro, Subx, datv, plur\n",
      "    - Analysis: NPRO, ablt, neut, sing\n",
      "    - Analysis: ADJF, Anph, Apro, Subx, ablt, masc, sing\n",
      "    - Analysis: ADJF, Anph, Apro, Subx, ablt, neut, sing\n",
      "- 'позиции': 4 unique analyses\n",
      "    - Analysis: NOUN, femn, inan, loct, sing\n",
      "    - Analysis: NOUN, femn, inan, nomn, plur\n",
      "    - Analysis: NOUN, femn, gent, inan, sing\n",
      "    - Analysis: NOUN, accs, femn, inan, plur\n",
      "- 'большой': 4 unique analyses\n",
      "    - Analysis: ADJF, Qual, ablt, femn, sing\n",
      "    - Analysis: ADJF, Qual, femn, loct, sing\n",
      "    - Analysis: ADJF, Qual, accs, inan, masc, sing\n",
      "    - Analysis: ADJF, Qual, femn, gent, sing\n",
      "- 'что': 4 unique analyses\n",
      "    - Analysis: CONJ\n",
      "    - Analysis: NPRO, neut, nomn, sing\n",
      "    - Analysis: NPRO, accs, neut, sing\n",
      "    - Analysis: PRCL\n",
      "- 'власти': 4 unique analyses\n",
      "    - Analysis: NOUN, femn, inan, nomn, plur\n",
      "    - Analysis: NOUN, femn, gent, inan, sing\n",
      "    - Analysis: NOUN, datv, femn, inan, sing\n",
      "    - Analysis: NOUN, accs, femn, inan, plur\n",
      "- 'всем': 4 unique analyses\n",
      "    - Analysis: ADJF, Apro, Subx, loct, neut, sing\n",
      "    - Analysis: ADJF, Apro, Subx, datv, plur\n",
      "    - Analysis: ADJF, Apro, Subx, loct, masc, sing\n",
      "    - Analysis: ADJF, Apro, Subx, ablt, masc, sing\n"
     ]
    }
   ],
   "source": [
    "homonym_variety = {}\n",
    "\n",
    "for word, analyses in analyses_by_word.items():\n",
    "    num_unique_analyses = len(set(analyses))\n",
    "    # Нас интересуют только слова с >1 вариантом разбора\n",
    "    if num_unique_analyses > 1:\n",
    "        homonym_variety[word] = num_unique_analyses\n",
    "\n",
    "# Сортируем слова по убыванию количества уникальных разборов\n",
    "sorted_homonyms_by_variety = sorted(\n",
    "    homonym_variety.items(),\n",
    "    key=lambda item: item[1],\n",
    "    reverse=True)\n",
    "\n",
    "print(\"\\nTop 20 word forms with the most grammatical analyses:\")\n",
    "for word, num_analyses in sorted_homonyms_by_variety[:20]:\n",
    "    print(f\"- '{word}': {num_analyses} unique analyses\")\n",
    "    for analysis in set(analyses_by_word[word]):\n",
    "        print(f\"    - Analysis: {', '.join(analysis)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ffa030",
   "metadata": {},
   "source": [
    "Здесь встречаем омонимы трёх типов. Во-первых, неизменяемые существительные (в т.ч. сокращения): *США*, *кино*, *НАТО*, *евро*. Во-вторых - падежные омографы и омонимы, где много форм с совпадающими окончаниями: *путь*, *лицо*, *компания*, *версия*. Ну и, наконец, служебные слова, которые допускают разный анализ (частично пересекается с первым списком): *какой*, *что*, *это*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b29c9c",
   "metadata": {},
   "source": [
    "# Задание 3\n",
    "Загрузите один и з файлов корпуса Syntagrus - https://github.com/UniversalDependencies/UD_Russian-SynTagRus/tree/master (можно взять тестовый)\n",
    "\n",
    "Преобразуйте все разборы предложений в графовые структуры через DependencyGraph, выберите 3 любых отношения и для каждого найдите топ-5 самых встречаемых пар слов, связанных этим отношением. \n",
    "\n",
    "Для самой частотной пары слов в каждом из отношений вытащите все подзависимые слова для каждого из них во всех предложениях (используя `flatten(get_subtree(d.nodes, index_of_a_word)` и сортируя результат по порядку слов в предложениях, аналогично тому как я делал с summaries только у вас будет два слова) \n",
    "В итоге у вас должен получится что-то такое:\n",
    "\n",
    "```\n",
    "### отношение\n",
    "relation_name\n",
    "\n",
    "### топ 5 пар слов связанных этим отношением\n",
    "(word1, word2), (word3, word4), (word5, word6), (word7, word8), (word9, word10)\n",
    "\n",
    "### подзависимые для самого частотного\n",
    "(subword word1 subword, word2 subword subword)\n",
    "\n",
    "... (и так три раза)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66c2a3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.parse import DependencyGraph\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa58c976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Рекурсивная функция для получения всех узлов поддерева\n",
    "def get_subtree(nodes, node):\n",
    "    # Исключаем пунктуацию\n",
    "    if not nodes[node]['deps'] or all(\n",
    "            rel == 'punct' for rel in nodes[node]['deps']):\n",
    "        return [node]\n",
    "    else:\n",
    "        return [node] + [get_subtree(nodes, dep) for rel in nodes[node]['deps']\n",
    "                         if rel != 'punct'\n",
    "                         for dep in nodes[node]['deps'][rel]]\n",
    "\n",
    "\n",
    "# Рекурсивная функция для преобразования вложенных списков\n",
    "def flatten(l):\n",
    "    flat = []\n",
    "    for el in l:\n",
    "        if not isinstance(el, list):\n",
    "            flat.append(el)\n",
    "        else:\n",
    "            flat += flatten(el)\n",
    "    return flat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b2a344",
   "metadata": {},
   "source": [
    "## 1. Загружаем и парсим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa418f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-10-08 13:55:39--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-test.conllu\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15253211 (15M) [text/plain]\n",
      "Saving to: ‘ru_syntagrus-ud-test.conllu’\n",
      "\n",
      "ru_syntagrus-ud-tes 100%[===================>]  14.55M  12.9MB/s    in 1.1s    \n",
      "\n",
      "2025-10-08 13:55:42 (12.9 MB/s) - ‘ru_syntagrus-ud-test.conllu’ saved [15253211/15253211]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-test.conllu -O ru_syntagrus-ud-test.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f820ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58e5a68f441427084795923e80ab24a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping sentence due to parsing error: \n",
      "Skipping sentence due to parsing error: \n",
      "Skipping sentence due to parsing error: \n",
      "Skipping sentence due to parsing error: \n",
      "Successfully parsed 8796 sentences.\n"
     ]
    }
   ],
   "source": [
    "graphs = []\n",
    "with open('ru_syntagrus-ud-test.conllu', 'r', encoding='utf-8') as f:\n",
    "    sents = f.read().split('\\n\\n')\n",
    "\n",
    "for sent in tqdm(sents):\n",
    "    # Отфильтровываем строки с метаинформацией и пустые строки\n",
    "    tree_lines = [line for line in sent.split('\\n') if line and line[0] != '#']\n",
    "    if not tree_lines:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        graph = DependencyGraph(\n",
    "            '\\n'.join(tree_lines),\n",
    "            top_relation_label='root')\n",
    "        graphs.append(graph)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping sentence due to parsing error: {e}\")\n",
    "        pass\n",
    "\n",
    "print(f\"Successfully parsed {len(graphs)} sentences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c86b897",
   "metadata": {},
   "source": [
    "## 2. Собираем статистику по словам для отношений\n",
    "\n",
    "Мы будем рассматривать самое стандартное: субъект, объект и модификатор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8a47f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6233d386d12b4f5686fd46acf0a4e8f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "relations_data = defaultdict(Counter)\n",
    "relations_to_find = ['nsubj', 'obj', 'amod']\n",
    "\n",
    "for graph in tqdm(graphs):\n",
    "    for node_index, node in graph.nodes.items():\n",
    "        # Пропускаем корневой узел\n",
    "        if node['head'] is None:\n",
    "            continue\n",
    "\n",
    "        relation = node['rel']\n",
    "        if relation in relations_to_find:\n",
    "            head_node = graph.nodes[node['head']]\n",
    "\n",
    "            # Убеждаемся, что у обоих слов есть лемма\n",
    "            if head_node.get('lemma') and node.get('lemma'):\n",
    "                head_lemma = head_node['lemma']\n",
    "                dep_lemma = node['lemma']\n",
    "                relations_data[relation][(head_lemma, dep_lemma)] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d8b3f4",
   "metadata": {},
   "source": [
    "## 3. Извлекаем поддеревья"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6640b962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### отношение\n",
      "nsubj\n",
      "\n",
      "### топ 5 пар слов связанных этим отношением\n",
      "(сказать, я), (мочь, я), (видеть, я), (идти, речь), (касаться, что)\n",
      "\n",
      "### подзависимые для самого частотного\n",
      "(Я сказал эти сметы пока отложите да да, Я)\n",
      "(И я бы сказал что Евангелие укрепляет нашу человечность, я)\n",
      "(Я просто скажу что по-моему украинское общество сейчас на распутье, Я)\n",
      "(сказал я, я)\n",
      "(сказал я, я)\n",
      "(Я бы сказал что определенным триггером стала отставка Черномырдина с поста премьер-министра, Я)\n",
      "(Я это сказала вслух и еще несколько человек признались что то же самое, Я)\n",
      "(Я сказала что живу на Садовой улице совсем недалеко от их бывшего дома, Я)\n",
      "(сказал я поглядывая на Смерчева снизу и сбоку, я)\n",
      "(сказал я, я)\n",
      "(сказал я жене, я)\n",
      "(сказал я жене, я)\n",
      "(сказал я пытаясь сдвинуть его со своего пути, я)\n",
      "(сказал я себе самому, я)\n",
      "(Уклоняясь от прямого ответа я сказал что я латыш, я)\n",
      "(сказал я, я)\n",
      "(сказал я, я)\n",
      "(сказал я к ней повернувшись, я)\n",
      "(сказал я не выдержав и перешел на другую сторону, я)\n",
      "(сказал я себе самому, я)\n",
      "(сказал я и попытался ухватить ее за что-нибудь, я)\n",
      "(сказал я и приблизился к регистратуре где сидела пожилая женщина в очках с оправой вырезанной из картона, я)\n",
      "(сказал я, я)\n",
      "(сказал я и вытащил уже проколотый с двух сторон обрывок Зюддойче цайтунг, я)\n",
      "(Изобразив на своем лице удивление я сказал что по-моему и дураку ясно что здесь начикано исключительно по-китайски, я)\n",
      "(сказал я, я)\n",
      "(сказал я, я)\n",
      "(сказал я и плюнул ему в морду кровью, я)\n",
      "(сказал я и вырвав у нее руку прямо локтем заехал дежурному в рыло отчего у него из носа кровь брызнула в разные стороны, я)\n",
      "(сказал я, я)\n",
      "(Трогая пальцем разбитый нос я сказал что хотел всего лишь пройтись посмотреть своими глазами как живут простые комуняне, я)\n",
      "(сказал я, я)\n",
      "(сказал я довольно холодно, я)\n",
      "(сказал я, я)\n",
      "(сказал я неуверенно, я)\n",
      "(сказал я волнуясь, я)\n",
      "(сказал я робея, я)\n",
      "(сказал я, я)\n",
      "(сказал я, я)\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "### отношение\n",
      "obj\n",
      "\n",
      "### топ 5 пар слов связанных этим отношением\n",
      "(делать, что), (делать, это), (играть, роль), (говорить, что), (вести, себя)\n",
      "\n",
      "### подзависимые для самого частотного\n",
      "(что они делали, что)\n",
      "(Что делает наша власть, Что)\n",
      "(что делаю, что)\n",
      "(что они делают, что)\n",
      "(что и делает весь медицинский мир на протяжении последних 100 лет, что)\n",
      "(что он до сих пор делал, что)\n",
      "(что он уже сделал для страны, что)\n",
      "(что сделать в это свободное время для страны, что)\n",
      "(что делают депутаты которые работают в России и тащат деньги за границу, что)\n",
      "(и что делать, что)\n",
      "(Что мне теперь с ними делать, Что)\n",
      "(что вы делаете, что)\n",
      "(Например что ему делать с его зарплатой, что)\n",
      "(что плохого сделать, что плохого)\n",
      "(И что ты будешь делать если посреди пути одно из таких колес придет в негодность, что)\n",
      "(Что делает сапожник, Что)\n",
      "(Что делает поэт, Что)\n",
      "(что делать замечательному писателю Олеше, что)\n",
      "(что они делают на своих позициях, что)\n",
      "(Ну что будем делать, что)\n",
      "(что делал Путин в экономической политике до сегодняшнего дня, что)\n",
      "(что я делал с огромным воодушевлением, что)\n",
      "(Что же я буду делать в этом городе здесь так скучно думала я, Что же)\n",
      "(А чего ж мне делать пытаясь вырваться заорал он плаксиво, чего ж)\n",
      "(Да что же это вы делаете, что же это)\n",
      "(А что вы делали во Дворце Любви, что)\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "### отношение\n",
      "amod\n",
      "\n",
      "### топ 5 пар слов связанных этим отношением\n",
      "(год, 2010), (год, 1998), (глаз, правый), (год, 2003), (год, 1919)\n",
      "\n",
      "### подзависимые для самого частотного\n",
      "(к 2010 году, 2010)\n",
      "(До 2010 года когда я смогу получить деньги, 2010)\n",
      "(по 2010 год, 2010)\n",
      "(на 2010 2020 годы, 2010 2020)\n",
      "(2010 г., 2010)\n",
      "(вплоть до 2010 года, 2010)\n",
      "(С 2010 года, 2010)\n",
      "(на 2010 год, 2010)\n",
      "(В 2010 году, 2010)\n",
      "(2010 г., 2010)\n",
      "(В 2010 году, 2010)\n",
      "(с 2010 года, 2010)\n",
      "(в 2010 году, 2010)\n",
      "(2010 года, 2010)\n",
      "(на 2010 год, 2010)\n",
      "(на 2010 год, 2010)\n",
      "(2010 года, 2010)\n",
      "(С 2010 года, 2010)\n",
      "(2010 г., 2010)\n",
      "(В 2010 году, 2010)\n",
      "(К 2010 году, 2010)\n",
      "(до 2010 г, 2010)\n",
      "\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for relation in relations_to_find:\n",
    "    print(f\"### отношение\\n{relation}\\n\")\n",
    "\n",
    "    top_5_pairs = relations_data[relation].most_common(5)\n",
    "    print(\"### топ 5 пар слов связанных этим отношением\")\n",
    "    print(', '.join([f\"({p[0][0]}, {p[0][1]})\" for p in top_5_pairs]))\n",
    "\n",
    "    # Находим подзависимые для самой частой пары\n",
    "    print(\"\\n### подзависимые для самого частотного\")\n",
    "\n",
    "    if not top_5_pairs:\n",
    "        print(\"(нет данных для анализа)\\n\")\n",
    "        print(\"-\" * 40)\n",
    "        continue\n",
    "\n",
    "    top_head_lemma, top_dep_lemma = top_5_pairs[0][0]\n",
    "\n",
    "    # Итерируемся по корпусу снова, чтобы найти примеры\n",
    "    found_examples = 0\n",
    "    for graph in graphs:\n",
    "        for node_index, node in graph.nodes.items():\n",
    "            if node['rel'] == relation and node['head'] is not None:\n",
    "                head_node = graph.nodes[node['head']]\n",
    "\n",
    "                # Ищем точное совпадение по леммам\n",
    "                if head_node.get('lemma') == top_head_lemma and node.get(\n",
    "                        'lemma') == top_dep_lemma:\n",
    "\n",
    "                    # Получаем поддеревья для обоих слов\n",
    "                    head_subtree_indices = sorted(\n",
    "                        flatten(\n",
    "                            get_subtree(\n",
    "                                graph.nodes,\n",
    "                                head_node['address'])))\n",
    "                    dep_subtree_indices = sorted(\n",
    "                        flatten(get_subtree(graph.nodes, node['address'])))\n",
    "\n",
    "                    # Преобразуем индексы обратно в слова\n",
    "                    head_subtree_words = ' '.join(\n",
    "                        [graph.nodes[i]['word'] for i in head_subtree_indices if graph.nodes[i]['word'] is not None])\n",
    "                    dep_subtree_words = ' '.join(\n",
    "                        [graph.nodes[i]['word'] for i in dep_subtree_indices if graph.nodes[i]['word'] is not None])\n",
    "\n",
    "                    print(f\"({head_subtree_words}, {dep_subtree_words})\")\n",
    "                    found_examples += 1\n",
    "\n",
    "    if found_examples == 0:\n",
    "        print(\"(не найдено предложений с данной парой)\")\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda50533",
   "metadata": {},
   "source": [
    "В целом результат достаточно правдоподобный, если помнить, что основу для корпуса составляют новостные тексты, в которых часто упоминаются даты, а также используются соответствующие штампы."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
