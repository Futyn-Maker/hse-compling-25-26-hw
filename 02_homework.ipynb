{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43fc8290",
   "metadata": {},
   "source": [
    "## Задание 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06a5ace",
   "metadata": {},
   "source": [
    "Посчитайте частоты для 5-грамм в корпусе lenta.txt. двумя способами:  \n",
    "1) lenta.txt -> sent_tokenize (russian) -> word_tokenize -> ngrammer  \n",
    "2) lenta.txt -> word_tokene(preserve_line=True) - ngrammer  \n",
    "    \n",
    "Проанализируйте топ-20 самых частотных нграмм и проверьте есть ли различия? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "957f5656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56bb277c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-09-26 23:31:08--  https://github.com/mannefedov/compling_nlp_hse_course/raw/master/data/lenta.txt.zip\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/lenta.txt.zip [following]\n",
      "--2025-09-26 23:31:09--  https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/lenta.txt.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5723675 (5.5M) [application/zip]\n",
      "Saving to: ‘lenta.txt.zip’\n",
      "\n",
      "lenta.txt.zip       100%[===================>]   5.46M  4.50MB/s    in 1.2s    \n",
      "\n",
      "2025-09-26 23:31:11 (4.50 MB/s) - ‘lenta.txt.zip’ saved [5723675/5723675]\n",
      "\n",
      "Archive:  lenta.txt.zip\n",
      "  inflating: lenta.txt               \n",
      "  inflating: __MACOSX/._lenta.txt    \n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/mannefedov/compling_nlp_hse_course/raw/master/data/lenta.txt.zip\n",
    "!unzip -o lenta.txt.zip\n",
    "!rm lenta.txt.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0702bd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('lenta.txt', 'r', encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5be218fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrammer(tokens, n=5):\n",
    "    \"\"\"\n",
    "    Функция для генерации n-грамм из списка токенов. В основном скопирована из семинара.\n",
    "    \"\"\"\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngrams.append(' '.join(tokens[i:i + n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc6d44f",
   "metadata": {},
   "source": [
    "### Метод 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85c9a800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.09 s, sys: 383 ms, total: 6.47 s\n",
      "Wall time: 6.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Разбиваем корпус на предложения\n",
    "sentences = sent_tokenize(corpus, language='russian')\n",
    "\n",
    "# Токенизируем каждое предложение\n",
    "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "all_tokens_method1 = [token.lower() for sent in tokenized_sentences for token in sent]\n",
    "\n",
    "five_grams_method1 = Counter(ngrammer(all_tokens_method1, n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e791dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"сообщает риа `` новости ''\", 1318), (\"риа `` новости '' .\", 1220), (', сообщает риа `` новости', 832), ('. по его словам ,', 705), (\"риа `` новости '' ,\", 612), (\"передает риа `` новости ''\", 558), (', передает риа `` новости', 463), (\"риа `` новости '' со\", 424), (\"`` новости '' со ссылкой\", 404), (\"новости '' со ссылкой на\", 400), (\"риа `` новости '' в\", 382), (\"сообщили риа `` новости ''\", 326), ('как сообщает риа `` новости', 323), ('. как сообщает риа ``', 312), ('. в то же время', 302), (\", сообщает `` интерфакс ''\", 256), ('. тем не менее ,', 224), ('как сообщили риа `` новости', 212), (\"сообщает `` интерфакс '' .\", 211), (\"`` эхо москвы '' .\", 208)]\n"
     ]
    }
   ],
   "source": [
    "print(five_grams_method1.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b790d10",
   "metadata": {},
   "source": [
    "### Метод 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d9afe44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.19 s, sys: 404 ms, total: 4.59 s\n",
      "Wall time: 4.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Токенизируем корпус сразу на слова\n",
    "all_tokens_method2 = [token.lower() for token in word_tokenize(corpus, language='russian', preserve_line=True)]\n",
    "\n",
    "# Создаём и считаем 5-граммы\n",
    "five_grams_method2 = Counter(ngrammer(all_tokens_method2, n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff87d09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"сообщает риа `` новости ''\", 1318), (\"риа `` новости '' .\", 1220), (', сообщает риа `` новости', 829), (\"риа `` новости '' ,\", 612), (\"передает риа `` новости ''\", 558), (', передает риа `` новости', 461), (\"риа `` новости '' со\", 424), (\"`` новости '' со ссылкой\", 404), (\"новости '' со ссылкой на\", 400), (\"риа `` новости '' в\", 382), (\"сообщили риа `` новости ''\", 326), ('как сообщает риа `` новости', 323), (\", сообщает `` интерфакс ''\", 256), ('как сообщили риа `` новости', 212), (\"сообщает `` интерфакс '' .\", 211), (\"`` эхо москвы '' .\", 208), (\"этом риа `` новости ''\", 188), ('об этом риа `` новости', 184), (\"`` интерфакс '' со ссылкой\", 179), (\"интерфакс '' со ссылкой на\", 179)]\n"
     ]
    }
   ],
   "source": [
    "print(five_grams_method2.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41323b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5_gramm_1</th>\n",
       "      <th>count_1</th>\n",
       "      <th>5_gramm_2</th>\n",
       "      <th>count_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>сообщает риа `` новости ''</td>\n",
       "      <td>1318</td>\n",
       "      <td>сообщает риа `` новости ''</td>\n",
       "      <td>1318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>риа `` новости '' .</td>\n",
       "      <td>1220</td>\n",
       "      <td>риа `` новости '' .</td>\n",
       "      <td>1220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>, сообщает риа `` новости</td>\n",
       "      <td>832</td>\n",
       "      <td>, сообщает риа `` новости</td>\n",
       "      <td>829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>. по его словам ,</td>\n",
       "      <td>705</td>\n",
       "      <td>риа `` новости '' ,</td>\n",
       "      <td>612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>риа `` новости '' ,</td>\n",
       "      <td>612</td>\n",
       "      <td>передает риа `` новости ''</td>\n",
       "      <td>558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>передает риа `` новости ''</td>\n",
       "      <td>558</td>\n",
       "      <td>, передает риа `` новости</td>\n",
       "      <td>461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>, передает риа `` новости</td>\n",
       "      <td>463</td>\n",
       "      <td>риа `` новости '' со</td>\n",
       "      <td>424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>риа `` новости '' со</td>\n",
       "      <td>424</td>\n",
       "      <td>`` новости '' со ссылкой</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>`` новости '' со ссылкой</td>\n",
       "      <td>404</td>\n",
       "      <td>новости '' со ссылкой на</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>новости '' со ссылкой на</td>\n",
       "      <td>400</td>\n",
       "      <td>риа `` новости '' в</td>\n",
       "      <td>382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>риа `` новости '' в</td>\n",
       "      <td>382</td>\n",
       "      <td>сообщили риа `` новости ''</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>сообщили риа `` новости ''</td>\n",
       "      <td>326</td>\n",
       "      <td>как сообщает риа `` новости</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>как сообщает риа `` новости</td>\n",
       "      <td>323</td>\n",
       "      <td>, сообщает `` интерфакс ''</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>. как сообщает риа ``</td>\n",
       "      <td>312</td>\n",
       "      <td>как сообщили риа `` новости</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>. в то же время</td>\n",
       "      <td>302</td>\n",
       "      <td>сообщает `` интерфакс '' .</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>, сообщает `` интерфакс ''</td>\n",
       "      <td>256</td>\n",
       "      <td>`` эхо москвы '' .</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>. тем не менее ,</td>\n",
       "      <td>224</td>\n",
       "      <td>этом риа `` новости ''</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>как сообщили риа `` новости</td>\n",
       "      <td>212</td>\n",
       "      <td>об этом риа `` новости</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>сообщает `` интерфакс '' .</td>\n",
       "      <td>211</td>\n",
       "      <td>`` интерфакс '' со ссылкой</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>`` эхо москвы '' .</td>\n",
       "      <td>208</td>\n",
       "      <td>интерфакс '' со ссылкой на</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      5_gramm_1  count_1                    5_gramm_2  count_2\n",
       "0    сообщает риа `` новости ''     1318   сообщает риа `` новости ''     1318\n",
       "1           риа `` новости '' .     1220          риа `` новости '' .     1220\n",
       "2     , сообщает риа `` новости      832    , сообщает риа `` новости      829\n",
       "3             . по его словам ,      705          риа `` новости '' ,      612\n",
       "4           риа `` новости '' ,      612   передает риа `` новости ''      558\n",
       "5    передает риа `` новости ''      558    , передает риа `` новости      461\n",
       "6     , передает риа `` новости      463         риа `` новости '' со      424\n",
       "7          риа `` новости '' со      424     `` новости '' со ссылкой      404\n",
       "8      `` новости '' со ссылкой      404     новости '' со ссылкой на      400\n",
       "9      новости '' со ссылкой на      400          риа `` новости '' в      382\n",
       "10          риа `` новости '' в      382   сообщили риа `` новости ''      326\n",
       "11   сообщили риа `` новости ''      326  как сообщает риа `` новости      323\n",
       "12  как сообщает риа `` новости      323   , сообщает `` интерфакс ''      256\n",
       "13        . как сообщает риа ``      312  как сообщили риа `` новости      212\n",
       "14              . в то же время      302   сообщает `` интерфакс '' .      211\n",
       "15   , сообщает `` интерфакс ''      256           `` эхо москвы '' .      208\n",
       "16             . тем не менее ,      224       этом риа `` новости ''      188\n",
       "17  как сообщили риа `` новости      212       об этом риа `` новости      184\n",
       "18   сообщает `` интерфакс '' .      211   `` интерфакс '' со ссылкой      179\n",
       "19           `` эхо москвы '' .      208   интерфакс '' со ссылкой на      179"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Сделаем табличку, чтобы удобнее сравнивать результат\n",
    "df1 = pd.DataFrame(\n",
    "    five_grams_method1.most_common(20),\n",
    "    columns=[\n",
    "        '5_gramm_1',\n",
    "        'count_1'])\n",
    "df2 = pd.DataFrame(\n",
    "    five_grams_method2.most_common(20),\n",
    "    columns=[\n",
    "        '5_gramm_2',\n",
    "        'count_2'])\n",
    "df = pd.concat([df1, df2], axis=1)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a361342",
   "metadata": {},
   "source": [
    "Основные различия состоят в том, что в случае с первым методом есть две 5-граммы, которые начинаются с точки (то есть переходят через границу предложения), во втором таковых нет. Кроме того, отличается частотность 5-граммы *, сообщает риа `` новости*: для первого метода она немного выше.\n",
    "\n",
    "Это говорит о том, что в первом методе токенизация происходит более актуально (в том числе и потому, что при делении на предложения учитывается язык): все пунктуационные токены отделяются от слов исправно. В случае со вторым методом есть некоторые проблемы, и, видимо, в некоторых случаях пунктуация остаётся приклеянной к алфавитной последовательности. Но если нормально препроцессить пунктуацию, убирать и её, и стоп-слова, эти различия практически сотрутся."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5781f34",
   "metadata": {},
   "source": [
    "## Задание 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4292716e",
   "metadata": {},
   "source": [
    "Найдите какую-то инетересную (по вашему мнению) закономерность на https://books.google.com/ngrams/ для русского языка (с 1990 по 2022)\n",
    "\n",
    "Вставьте сюда скриншот"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3bf76d",
   "metadata": {},
   "source": [
    "![*без дураков*](./ngram_hw2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c383da",
   "metadata": {},
   "source": [
    "Это график употребления фразеологизма *без дураков* (в значении 'серьёзно, без шуток'), которое я исследовал ещё для курсовой на 1 курсе бакалавриата. Для меня было совершенно неожиданным, что в книгах оно начало встречаться только начиная с 1925 года и ранее совершенно не использовалось, причём не только в качестве фразеологизма, но и даже в прямом значении. Ещё интересно, что происходит спад употребления с 1940 до 1960-х, и только после этого начинается резкий рост. Последнее, по всей видимости, связано с тем, что в переносном, фразеологическом смысле выражение стало употребляться именно тогда, а ранние вхождения - скорее прямое."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0a89ec",
   "metadata": {},
   "source": [
    "## Задание 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40c35e9",
   "metadata": {},
   "source": [
    "Когда мы разбирали PMI мы использовали такую функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "221f1bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer_simple(word_count_a, word_count_b, bigram_count, *args):\n",
    "    try:\n",
    "        score = bigram_count / ((word_count_a + word_count_b))\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fd2def",
   "metadata": {},
   "source": [
    "Но если вы посмотрите на определение в википедии, то увидите, что формула немного другая ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/094243d23c19d2d032f6bb26c4dc4f47d98d32f8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1905862",
   "metadata": {},
   "source": [
    "Перепишите функцию, чтобы она точно соответствовала этому определению. Расчитайте PMI для всех биграммов так же как мы делали в семинаре с помощью функции score_bigrams используя изначальный scorer и обновленный. Посмотрите есть ли разница в топ-10 биграммов. Подумайте почему результаты совпадают/отличаются?\n",
    "\n",
    "*Подсказка: для вероятностей можно поделить на количество слов в корпусе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4eb57ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "russian_stopwords = set(stopwords.words(\"russian\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40d92193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего токенов в корпусе: 1449115\n",
      "Уникальных биграмм: 761082\n"
     ]
    }
   ],
   "source": [
    "# Функция для генерации биграмм\n",
    "def ngrammer_bigrams(tokens, n=2):\n",
    "    ngrams = []\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords]\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngrams.append(' '.join(tokens[i:i + n]))\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "# Считаем статистику\n",
    "unigrams = Counter()\n",
    "bigrams = Counter()\n",
    "\n",
    "for sentence in tokenized_sentences:\n",
    "    unigrams.update(\n",
    "        token for token in sentence if token not in russian_stopwords)\n",
    "    bigrams.update(ngrammer_bigrams(sentence))\n",
    "\n",
    "# Считаем общую длину корпуса\n",
    "corpus_length = sum(unigrams.values())\n",
    "\n",
    "print(f\"Всего токенов в корпусе: {corpus_length}\")\n",
    "print(f\"Уникальных биграмм: {len(bigrams)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1431f618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer_pmi(word_count_a, word_count_b, bigram_count, corpus_length):\n",
    "    if word_count_a == 0 or word_count_b == 0:\n",
    "        return -float('inf')\n",
    "\n",
    "    # Считаем вероятности\n",
    "    p_a = word_count_a / corpus_length\n",
    "    p_b = word_count_b / corpus_length\n",
    "    p_a_b = bigram_count / corpus_length\n",
    "\n",
    "    try:\n",
    "        pmi = np.log2(p_a_b / (p_a * p_b))\n",
    "    except (ValueError, ZeroDivisionError):\n",
    "        return -float('inf')\n",
    "\n",
    "    return pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5fedcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_bigrams(unigrams, bigrams, scorer, corpus_length=None):\n",
    "    \"\"\"Вычисляет скор для каждой биграммы, используя предоставленную функцию подсчета. Немного модифицирована из семинара, чтобы принимать длину корпуса\"\"\"\n",
    "    bigram2score = Counter()\n",
    "\n",
    "    for bigram, bigram_count in bigrams.items():\n",
    "        word_a, word_b = bigram.split()\n",
    "        word_count_a = unigrams[word_a]\n",
    "        word_count_b = unigrams[word_b]\n",
    "\n",
    "        if scorer.__name__ == 'scorer_pmi' or scorer.__name__ == 'scorer_pmi_log':\n",
    "            score = scorer(\n",
    "                word_count_a,\n",
    "                word_count_b,\n",
    "                bigram_count,\n",
    "                corpus_length)\n",
    "        else:\n",
    "            score = scorer(word_count_a, word_count_b, bigram_count)\n",
    "\n",
    "        bigram2score[bigram] = score\n",
    "\n",
    "    return bigram2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cca81200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Топ 10, scorer_simple ---\n",
      "Score: 0.50, Bigram: \"Сопоцкина Друскеник\"\n",
      "Score: 0.50, Bigram: \"австрийский аэроплан\"\n",
      "Score: 0.50, Bigram: \"показывался аэроплан-птица\"\n",
      "Score: 0.50, Bigram: \"Das ist\"\n",
      "Score: 0.50, Bigram: \"ist Nesteroff\"\n",
      "Score: 0.50, Bigram: \"Песнь Нестерове\"\n",
      "Score: 0.50, Bigram: \"зловеще гремели.И\"\n",
      "Score: 0.50, Bigram: \"гремели.И пламенно\"\n",
      "Score: 0.50, Bigram: \"пламенно богу\"\n",
      "Score: 0.50, Bigram: \"жаждали битвы…Величие\"\n",
      "\n",
      "--- Топ 10, scorer_pmi ---\n",
      "Score: 20.47, Bigram: \"Сопоцкина Друскеник\"\n",
      "Score: 20.47, Bigram: \"австрийский аэроплан\"\n",
      "Score: 20.47, Bigram: \"показывался аэроплан-птица\"\n",
      "Score: 20.47, Bigram: \"Das ist\"\n",
      "Score: 20.47, Bigram: \"ist Nesteroff\"\n",
      "Score: 20.47, Bigram: \"Песнь Нестерове\"\n",
      "Score: 20.47, Bigram: \"зловеще гремели.И\"\n",
      "Score: 20.47, Bigram: \"гремели.И пламенно\"\n",
      "Score: 20.47, Bigram: \"пламенно богу\"\n",
      "Score: 20.47, Bigram: \"жаждали битвы…Величие\"\n"
     ]
    }
   ],
   "source": [
    "scores_simple = score_bigrams(unigrams, bigrams, scorer_simple)\n",
    "\n",
    "scores_pmi = score_bigrams(\n",
    "    unigrams,\n",
    "    bigrams,\n",
    "    scorer_pmi,\n",
    "    corpus_length=corpus_length)\n",
    "\n",
    "print(\"--- Топ 10, scorer_simple ---\")\n",
    "for bigram, score in scores_simple.most_common(10):\n",
    "    print(f'Score: {score:.2f}, Bigram: \"{bigram}\"')\n",
    "\n",
    "print(\"\\n--- Топ 10, scorer_pmi ---\")\n",
    "for bigram, score in scores_pmi.most_common(10):\n",
    "    print(f'Score: {score:.2f}, Bigram: \"{bigram}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c62eeb",
   "metadata": {},
   "source": [
    "Биграммы в обоих списках полностью совпадают, различается только скор. Это происходит, потому что мы никак не фильтровали пороги частотности. В итоге мы получаем биграммы, в которых оба слова в паре являются hapax legomena: встречаются только один раз во всём корпусе, и при этом и составляют эту биграмму. Поэтому, в соответствии с обеими формулами, им присваивается максимально возможный скор. Если фильтровать такие слова, можно получить более осмысленный (и различающийся) результат."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6e1c99",
   "metadata": {},
   "source": [
    "## Задание 4*\n",
    "\n",
    "Обновите функцию получившуюся в предыдущем задании так, чтобы вместо произведения/деления вероятностей использовались сложение и вычитание логирифмов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3f55a362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer_pmi_log(word_count_a, word_count_b, bigram_count, corpus_length):\n",
    "    \"\"\"\n",
    "    Рассчитывает PMI, используя сложение/вычитание логарифмов.\n",
    "    \"\"\"\n",
    "    # Если какой-либо из подсчетов равен нулю, его логарифм будет -inf.\n",
    "    if word_count_a == 0 or word_count_b == 0 or bigram_count == 0:\n",
    "        return -float('inf')\n",
    "\n",
    "    # Рассчитываем логарифм каждой вероятности напрямую\n",
    "    log_p_a = np.log2(word_count_a / corpus_length)\n",
    "    log_p_b = np.log2(word_count_b / corpus_length)\n",
    "    log_p_a_b = np.log2(bigram_count / corpus_length)\n",
    "\n",
    "    # Применяем правило log(a/b) = log(a) - log(b)\n",
    "    pmi = log_p_a_b - (log_p_a + log_p_b)\n",
    "\n",
    "    return pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d44fab6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Топ-10, scorer_pmi (оригинальная формула) ---\n",
      "Score: 20.47, Bigram: \"Сопоцкина Друскеник\"\n",
      "Score: 20.47, Bigram: \"австрийский аэроплан\"\n",
      "Score: 20.47, Bigram: \"показывался аэроплан-птица\"\n",
      "Score: 20.47, Bigram: \"Das ist\"\n",
      "Score: 20.47, Bigram: \"ist Nesteroff\"\n",
      "Score: 20.47, Bigram: \"Песнь Нестерове\"\n",
      "Score: 20.47, Bigram: \"зловеще гремели.И\"\n",
      "Score: 20.47, Bigram: \"гремели.И пламенно\"\n",
      "Score: 20.47, Bigram: \"пламенно богу\"\n",
      "Score: 20.47, Bigram: \"жаждали битвы…Величие\"\n",
      "\n",
      "--- Топ-10, scorer_pmi_log ---\n",
      "Score: 20.47, Bigram: \"Сопоцкина Друскеник\"\n",
      "Score: 20.47, Bigram: \"австрийский аэроплан\"\n",
      "Score: 20.47, Bigram: \"показывался аэроплан-птица\"\n",
      "Score: 20.47, Bigram: \"Das ist\"\n",
      "Score: 20.47, Bigram: \"ist Nesteroff\"\n",
      "Score: 20.47, Bigram: \"Песнь Нестерове\"\n",
      "Score: 20.47, Bigram: \"зловеще гремели.И\"\n",
      "Score: 20.47, Bigram: \"гремели.И пламенно\"\n",
      "Score: 20.47, Bigram: \"пламенно богу\"\n",
      "Score: 20.47, Bigram: \"жаждали битвы…Величие\"\n"
     ]
    }
   ],
   "source": [
    "scores_pmi_log = score_bigrams(\n",
    "    unigrams,\n",
    "    bigrams,\n",
    "    scorer_pmi_log,\n",
    "    corpus_length=corpus_length)\n",
    "\n",
    "print(\"--- Топ-10, scorer_pmi (оригинальная формула) ---\")\n",
    "for bigram, score in scores_pmi.most_common(10):\n",
    "    print(f'Score: {score:.2f}, Bigram: \"{bigram}\"')\n",
    "\n",
    "print(\"\\n--- Топ-10, scorer_pmi_log ---\")\n",
    "for bigram, score in scores_pmi_log.most_common(10):\n",
    "    print(f'Score: {score:.2f}, Bigram: \"{bigram}\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
