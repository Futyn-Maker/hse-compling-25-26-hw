{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fad453",
   "metadata": {},
   "source": [
    "# Домашнее задание № 4. Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d056af4",
   "metadata": {},
   "source": [
    "## Задание 1 (8 баллов)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f532a8",
   "metadata": {},
   "source": [
    "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de743d1d",
   "metadata": {},
   "source": [
    "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста.\n",
    "Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели. \n",
    "Можно использовать данные из семинара или любые другие (можно брать только часть текста, если считается слишком долго). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
    "\n",
    "\n",
    "Подсказки:  \n",
    "    - нужно будет добавить еще один тэг \\<start>  \n",
    "    - можете использовать тот же подход с матрицей вероятностей, но по строкам хронить биграмы, а по колонкам униграммы \n",
    "    - тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так)\n",
    "    - у вас будут словари с индексами биграммов и униграммов, не перепутайте их при переводе индекса в слово - словарь биграммов будет больше словаря униграммов и все индексы из униграммного словаря будут формально подходить для словаря биграммов (не будет ошибки при id2bigram[unigram_id]), но маппинг при этом будет совершенно неправильным "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2409e4",
   "metadata": {},
   "source": [
    "## 1. Загружаем данные и смотрим на них\n",
    "\n",
    "Мы будем использовать датасет статей с Лурка, чтобы веселее было. То, что лежит на HF, кажется, не самое свежее (даже с учётом того, что Лурк де факто умер после апреля 2021 года), но материала для модели, которая будет шпарить на луркоязе, там вроде бы достаточно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d078056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_parquet(\n",
    "    \"hf://datasets/averoo/lurk/data/train-00000-of-00001-75f3e3227ae9f5fc.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "490c9d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5671 entries, 0 to 5670\n",
      "Data columns (total 3 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   header             5671 non-null   object\n",
      " 1   text               5671 non-null   object\n",
      " 2   __index_level_0__  5671 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 133.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06ddaa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% населения — идиоты\n",
      "\n",
      "«95% населения — идиоты». Как утверждает сам Гоблин, это научный факт, по-видимому, доказанный\n",
      "<del>британскими учёными</del> профессором Савельевым, из которого следует, что читающий эту статью, с вероятностью 0,95 идиот, как и её авторы.\n",
      "Хотя, на самом деле, теория вероятностей не имеет дела с единичными клиническими случаями, которыми занимается психиатрия. А вот количество таких случаев — это уже, да, статистика.\n",
      "\n",
      " \n",
      "\n",
      "Однако же статистикоёбы напоминают, что 95% соответствуют интервалу из двух среднеквадратичных отклонений нормально распределённой случайной величины. То есть, если взять несколько дискретных случайных величин, характеризующих идиотизм среди населения, высчитать их математическое ожидание, затем дисперсию и среднеквадратичное отклонение, потом взять интервал в два среднеквадратичных отклонения вокруг мат. ожидания, то в этот интервал попадёт 95% результатов исследования. То есть 95% населения — идиоты.\n",
      "Сие предполагает двусторонний доверительный интервал, то есть переводит архиидиотов из 2,5% населения в обособленную категорию. Алсо если применять правило 3σ, то просто идиотами окажутся 99,73% населения, а вместе с архиидиотами — все 99,865% (это меньше 9,5 млн умных людей на весь глобус). Вполне возможен и другой вариант, когда 5% архиидиотов приплюсовывается к 90% обычных, в сумме давая пресловутые 95%. Хотя это выглядит менее мизантропично. Однако, вероятность встречи инакомыслящего идиота \"с точки зрения отдельного их представителя\" всегда стремится к 100%, и лишь квантовомеханическая неопределённость наблюдаемых уменьшает \"ожидание\" до 95%.\n",
      "\n",
      "Дебильность — легкая степень малоумия (IQ от 50 до 75). Её трудно отличить от психики на нижней границе нормы. Поведение дебилов достаточно вменяемо и самостоятельно, речь развита. Поэтому дебильность замечается не сразу, а обычно в процессе начального обучения. В подростковом возрасте, когда дебильность особенно проявляется, обнаруживаются дефекты в абстрактном мышлении. Дебилами все понимается буквально, переносный смысл пословиц, метафор не улавливается. Лица, страдающие дебильностью, овладевают преимущественно конкретными знаниями, усвоение теоретических им не дается.\n",
      "Дебилы нередко легко приспосабливаются к жизни (вне систем образования и науки). Как правило, они становятся счастливыми, всем довольными людьми, без комплексов, «завихрений» и тому подобной гадости. Послушны, трудолюбивы, исполнительны, обычно безынициативны и апатичны, уважают начальство и любую власть, что делает их хорошими исполнителями. По ироническому замечанию одного из психологов, возможно, именно дебилов придется со временем признать воплощением нормы, так как, повзрослев, они становятся тем, что ближе всего к понятию «почтенного обывателя» («простой советский человек», «простой американский гражданин», «почтенный европейский буржуа» etc). К 45 годам дебила уже невозможно отличить от обычного среднестатистического обывателя, чье времяпровождение сводится к употреблению пива под присмотром телевизора.\n",
      "Удельный вес дебильности нигде официально не превышает 3,5%. Но многие сомневаются в этой оценке, так как ни одна страна не заинтересована в её точности. По многим регионам нет вообще никаких данных, а специализированные исследования (в школах, армии, службах занятости) дают данные на порядок выше. Поэтому в ряде стран решено, вслед за США, где дебильность является национальной <del>трагедией</del> гордостью, признать его легкие формы нормой и максимально сократить в общедоступных школах долю обязательного учебного материала, требующего способностей к абстрактному мышлению.\n",
      "Nuff said.\n",
      "Имбецильность (от лат. \"imbecillus \"  — \"слабый, немощный\") — средняя степень олигофрении, малоумия, интеллектуального недоразвития, обусловленная задержкой развития мозга плода или ребёнка в первые годы жизни.\n",
      "При имбецильности дети отстают в физическом развитии, отклонения заметны внешне. Имбецилы понимают речь окружающих, сами могут произносить короткие фразы. Речь бедна и неправильна, но более или менее связна. Мышление конкретно и примитивно, но последовательно, отвлечения недоступны, запас сведений крайне узок, резкое недоразвитие внимания, памяти, воли. Страдающим имбецильностью удаётся привить элементарные трудовые навыки, обучить чтению, письму, счёту, etc. Некоторые имбецилы способны производить элементарные счетные операции, усваивать простейшие трудовые навыки и навыки самообслуживания. Эмоции имбецилов более дифференцированы, чем у идиотов, они привязаны к родным, адекватно реагируют на похвалу или порицание. Имбецилы лишены инициативы, инертны, внушаемы, легко теряются при изменении обстановки, нуждаются в постоянном надзоре и уходе, при неблагоприятном окружении поведение может быть асоциальным.\n",
      "Идиотия (от лат. \"idiōtēs \"  — \"невежда, неуч\", букв. \"отдельный, частный человек\") — самая глубокая степень олигофрении («малоумия»), характеризующаяся почти полным отсутствием речи и мышления. Больные, страдающие идиотией, не могут ходить, у них нарушено строение внутренних органов. Идиотам недоступна осмысленная деятельность. Речь не развивается. Идиоты произносят лишь отдельные нечленораздельные звуки и слова, часто не понимают речи окружающих, не отличают родственников от посторонних.\n",
      "Они не способны к самостоятельной жизни: не владеют элементарными навыками самообслуживания, не могут самостоятельно есть, иногда даже не пережевывают пищу, неопрятны, нуждаются в постоянном уходе и надзоре. Мышление не развивается, реакция на окружающее резко снижена. Эмоциональная жизнь исчерпывается примитивными реакциями удовольствия и неудовольствия. У одних преобладают злобно-гневливые вспышки, у других — вялость и безразличие ко всему окружающему.\n",
      "\n",
      "Практика показала, что Лири ошибся всего на два процента, впрочем, анонимус намекает, что на момент написания книги ситуация была ничуть не лучше. На это неиллюзорно намекает тот факт, что сажая в тюрьму расового американского психолога и программиста, омереканские власти тестировали его им же и написанным тестом. Благодаря чему он и сбежал.\n",
      "Всемиrная некоммеrческая оrганизация Менса, являющая собой что-то вроде клуба знакомств интеллектуальных меньшинств, принимает в свои ряды исключительно и только по принципу интеллектуального превосходства. Так, сдать тест IQ для вхождения в ряды может каждый, но только 2% лучших соискателей получат масонские погоны, что оставляет за анальной оградой 98% не прошедших проверку на вшивость. Такие дела.\n",
      "Но стоит также заметить, что членами этой организации являются, среди прочих, боксёры, парапсихологи, комики и другие звёзды шоубизнеса, википедия гарантирует это.\n",
      "Способность к предпринимательству имеется у 3-5 процентов населения.\n",
      "\n",
      " \n",
      "По утверждениям педивикии, альбом расового афроамериканского исполнителя Lord Jamar назван «» в честь как раз тех оставшихся 5%, что не идиоты. В свою очередь нигра позаимствовал эту идею у американской же расовой исламской культурно-политической группировки The Nation of Gods and Earths (также известной как Five-Percent Nation, the Five-Percent Nation of Islam, the Five Percenters, ну ты понел). Также эта идея была позаимствована Кровостоком («оставшиеся 5-6 процентов» (ц)), алсо есть быдло-песенка сибирских репперков 95 из 100\n",
      "Джеф Раскин\n",
      "Генри Форд «Моя жизнь, мои достижения»\n",
      "— Почему нет программы под «Линукс»?\n",
      "— Потому что сначала мы хотим предоставить возможность работать с клавиатурой 95 % людей.\n",
      "]\n",
      "По утверждениям некоторых британских учёных, падение метеорита и усиление вулканической деятельности в Сибири привело в своё время к вымиранию 95% всех существовавших тогда видов, что символизирует.\n",
      "Первого августа 2009-го были предприняты попытки сделать «95%» форсед-мемом хабра. К сожалению, fail. Примерная хронология: раз, два, три, четыре.\n",
      "На совещании с премьером В. В. Путиным министр связи и массовых коммуникаций И. Щёголев недвусмысленно обрисовал приоритеты:\n",
      "\n",
      "«День Триффидов» Джон Уиндем\n",
      "«Гадкие лебеди» Стругацкие\n",
      "Альзо, в годном НФ-романе Ефремова «Час быка» жители планеты Торманс делятся на две части: кжи (счастливо живущие до 30 необременнные интеллектом люди) и джи (им позволяют жить дольше, чтобы оправдалось затраченное на высшее образование время). Первых, как вы догадались, 95%.\n",
      "В одной из новелл по Харухи Судзумии, а конкретно в «Endless Eight», Богиня выдает примерно следующую сакраментальную фразу: «Микуру в таком юката просто не может не понравиться 95% мужского населения. А 95, потому что 5% мужиков — геи!»\n",
      "\n",
      "5 декабря 2008 года Новодворская заявила: «Россия — это мы. Русские не сдаются, в отличие от поднимающих лапки совков. В России 5 процентов русских, варягов, викингов, европейцев, носителей скандинавской традиции. Остальные — пресмыкающиеся, амебы и инфузории-туфельки. Динозавры из КПСС или АКМ и НБП. Птеродактили из чекистов».\n",
      "Как всегда с Абрамом Гоблиным — \"гасло файне, але мета кепська \" (из разговора двух западэнцев, наблюдающих за демонстрацией на Красной площади с лозунгом «Бей жыдов — спасай Россiю»).\n",
      "Некоторые особенно фимозные гоблинские модераторы утверждают, что число в 95% занижено и называют как более точное 98%. Мысль о том, что Гуру также может принадлежать этому множеству, камрадам даже не приходит в голову. Впрочем, сам гуру и не открещивается: «В известных обстоятельствах туда входим все мы. В том числе и я, Ваня» (пруф)\n",
      "«95%» является универсальным ответом, позволяющим объяснить любое действие администрации Тупичка, обычно пресловутое контр-революционное чутье модераторов в отношении к некоторым посетителям и даже камрадам.\n",
      "Взяв чужое вполне разумное или вообще общественно-абстрактное утверждение (имеется ввиду Закона Старджона сформулированный ещё аж в 1958-ом году: «90% чего угодно — это барахло»), он придает ему твердую ассоциативную связь с Тупичком. Если его продолжать использовать, то возникает обоснованное подозрение в гоблинофагии, если отрицать — то в собственном идиотизме, если делать вид, что его вообще в природе не существует — то теряется огромный пласт Бобра и Света.\n",
      "Как сам Гоблин вкратце объясняет свое утверждение:\n",
      "Это трудно объяснить коротко.\n",
      "Собери коллектив — и в нём немедленно образуется иерархия.\n",
      "Одни станут главнее, другие станут ниже.\n",
      "Одни получат доступ к важной информации, другие — нет.\n",
      "Одни будут ей уверенно пользоваться, а другие не смогут.\n",
      "Одним это интересно, другим нет.\n",
      "В результате всегда и везде большинство — бараны, пусть они хоть нобелевские лауреаты.\n",
      "Это не говоря про случаи типа «батальон Наполеонов», где все мега-стратеги и титаны военного дела, а сегодня будут рыть канаву от КПП до ужина.\n",
      "Гоблин, конечно, неоднократно встречался с нобелевскими лауреатами (или хотя бы приличными, аккуратно отобранными на олимпиадах и экзаменах, матанными студентами) повзводно и поротно, откуда и вынес такую убежденность. Но в остальных 95% он, тем не менее, прав.\n",
      "Алсо, Гоблин утверждает, что 95% заключенных в тюрьмах уверены, что сидят ни за что. Конечно, Гоблин лично ходил по тюрьмам и проводил опросы. Ладно, это вам гарантирует Анонимус, проведший на зоне полтора года — таки да, 95% «шли к успеху», «взяли покататься», «она сама виновата» и «просто шутили». Ну, и тот факт, что Гоблин долгое время рулил тюремной оперчастью, неиллюзорно намекает.\n"
     ]
    }
   ],
   "source": [
    "# Пример текста\n",
    "# Дисклеймер: возможен баттхёрт - не читайте\n",
    "target_header = '95% населения — идиоты'\n",
    "example_text = df[df['header'] == target_header]['text'].iloc[0]\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ced758a",
   "metadata": {},
   "source": [
    "## 2. Чистим данные\n",
    "\n",
    "Чтобы получить более чистые данные, лучше удалить заголовок статьи в начале текста, а также возможные заголовки разделов (строки, которые не оканчиваются пунктуационным знаком)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49a5991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def remove_title(text):\n",
    "    # Находим конец первой строки\n",
    "    first_line_end = text.find('\\n')\n",
    "\n",
    "    # Если нет переноса строки или первая строка пустая, ничего не делаем\n",
    "    if first_line_end == -1:\n",
    "        return text\n",
    "\n",
    "    first_line = text[:first_line_end].strip()\n",
    "\n",
    "    if not first_line:\n",
    "        return text\n",
    "\n",
    "    # Проверяем, заканчивается ли строка на знак препинания\n",
    "    if first_line.endswith(('.', '!', '?', ':', ';')):\n",
    "        # Это не заголовок, возвращаем исходный текст\n",
    "        return text\n",
    "    else:\n",
    "        # Это заголовок, удаляем его и перенос строки\n",
    "        return text[first_line_end + 1:].lstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0112b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_text'] = df['text'].apply(remove_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27e91c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "«95% населения — идиоты». Как утверждает сам Гоблин, это научный факт, по-видимому, доказанный\n",
      "<del>британскими учёными</del> профессором Савельевым, из которого следует, что читающий эту статью, с ве\n"
     ]
    }
   ],
   "source": [
    "cleaned_example_text = df[df['header'] ==\n",
    "                          target_header]['cleaned_text'].iloc[0]\n",
    "print(cleaned_example_text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396fe8da",
   "metadata": {},
   "source": [
    "## 3. Делим на предложения и делаем сплиты: для сбора статистики и для расчёта перплексии\n",
    "\n",
    "Тут мудрить не будем и сделаем всё с помощью razdel. Пунктуацию, увы, тоже придётся померфить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e808cdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего предложений: 418145\n"
     ]
    }
   ],
   "source": [
    "from razdel import sentenize\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "all_sentences = []\n",
    "for text in df['cleaned_text']:\n",
    "    sentences = [s.text for s in sentenize(text) if s.text.strip()]\n",
    "    all_sentences.extend(sentences)\n",
    "\n",
    "print(f\"Всего предложений: {len(all_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4c67b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предложений в train: 397238\n",
      "Предложений в test: 20907\n"
     ]
    }
   ],
   "source": [
    "# Перемешиваем и делим\n",
    "random.seed(42)\n",
    "random.shuffle(all_sentences)\n",
    "\n",
    "# Берем 5% на тест для расчёта перплексии\n",
    "split_idx = int(len(all_sentences) * 0.05)\n",
    "\n",
    "test_sentences = all_sentences[:split_idx]\n",
    "train_sentences = all_sentences[split_idx:]\n",
    "\n",
    "print(f\"Предложений в train: {len(train_sentences)}\")\n",
    "print(f\"Предложений в test: {len(test_sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9060cebf",
   "metadata": {},
   "source": [
    "## 4. Делаем триграммную модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6a9d530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from scipy.sparse import lil_matrix, csc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5f65ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нормализуем, как в семинаре\n",
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word\n",
    "                       in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower()\n",
    "                       for word in normalized_text if word and len(word) < 20]\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b02bf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngrammer, тоже с семинара\n",
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0, len(tokens) - n + 1):\n",
    "        ngrams.append(' '.join(tokens[i:i + n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23fb3887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Счётчики\n",
    "unigrams = Counter()\n",
    "bigrams = Counter()\n",
    "trigrams = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f4f6d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f591ef39734478a5198a405e4581e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/397238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Собираем статистики по обучающей выборке\n",
    "for sent in tqdm(train_sentences):\n",
    "    # Добавляем два <start> токена для триграммной модели\n",
    "    normalized_sent = ['<start>', '<start>'] + normalize(sent) + ['<end>']\n",
    "\n",
    "    unigrams.update(normalized_sent)\n",
    "    bigrams.update(ngrammer(normalized_sent, n=2))\n",
    "    trigrams.update(ngrammer(normalized_sent, n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb320628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем маппинги для слов\n",
    "id2word = list(unigrams)\n",
    "word2id = {word: i for i, word in enumerate(id2word)}\n",
    "\n",
    "# Создаем маппинги для биграмм\n",
    "id2bigram = list(bigrams)\n",
    "bigram2id = {bigram: i for i, bigram in enumerate(id2bigram)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8332c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c4b48c7a28d4cb49a4e49104339f10a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6100940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Матрица: строки - биграммы (контекст), столбцы - униграммы\n",
    "\n",
    "matrix_lurk = lil_matrix((len(bigrams), len(unigrams)))\n",
    "\n",
    "for trigram_str, trigram_count in tqdm(trigrams.items()):\n",
    "    w1, w2, w3 = trigram_str.split()\n",
    "\n",
    "    context_bigram = f\"{w1} {w2}\"\n",
    "    target_unigram = w3\n",
    "\n",
    "    probability = trigram_count / bigrams[context_bigram]\n",
    "\n",
    "    matrix_lurk[bigram2id[context_bigram],\n",
    "                word2id[target_unigram]] = probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d55133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_lurk = csc_matrix(matrix_lurk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d456dcea",
   "metadata": {},
   "source": [
    "## 5. Генерируем текст и смотрим, что получается"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55d921bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trigram(\n",
    "        matrix,\n",
    "        id2word,\n",
    "        word2id,\n",
    "        id2bigram,\n",
    "        bigram2id,\n",
    "        n=100,\n",
    "        start_context='<start> <start>'):\n",
    "    text = []\n",
    "    current_context = start_context\n",
    "\n",
    "    for i in range(n):\n",
    "        # Находим ID текущей биграммы-контекста\n",
    "        if current_context not in bigram2id:\n",
    "            # Если мы сгенерировали биграмму, которой нет в трейне, начинаем\n",
    "            # заново\n",
    "            current_context = start_context\n",
    "\n",
    "        current_context_id = bigram2id[current_context]\n",
    "\n",
    "        # Получаем распределение вероятностей для следующего слова\n",
    "        probas = matrix[current_context_id].toarray()[0]\n",
    "\n",
    "        # Проверяем, есть ли вообще варианты\n",
    "        if not np.any(probas):\n",
    "            current_context = start_context\n",
    "            continue\n",
    "\n",
    "        # Нормализуем вероятности, чтобы их сумма была ровно 1.0\n",
    "        probas /= probas.sum()\n",
    "\n",
    "        # Выбираем следующее слово\n",
    "        chosen_word_id = np.random.choice(matrix.shape[1], p=probas)\n",
    "        chosen_word = id2word[chosen_word_id]\n",
    "\n",
    "        text.append(chosen_word)\n",
    "\n",
    "        if chosen_word == '<end>':\n",
    "            # Если конец предложения, сбрасываем контекст\n",
    "            current_context = start_context\n",
    "        else:\n",
    "            # Обновляем контекст\n",
    "            w1, w2 = current_context.split()\n",
    "            current_context = f\"{w2} {chosen_word}\"\n",
    "\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7c7a01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "курица — « even if i know that you would face severe consequences in order to spread the truth of the night и стали выбивать эфедрин из разных ракурсов а не пошел а аргументированно отстаивал свою позицию одержимые псинобесием зоозащитники имеют о сабже \n",
      " будучи аж пятой частью населения страны колесить по миру миллионами копий \n",
      " в конце 80-х стали вводиться компостеры \n",
      " в том что речи обращены в буддизм \n",
      " суть всегда сводилась к превращению простейшего казалось бы кого ещё часто называют « смерть от естественных причин имеет склонность к киданию игроков на пк ниппонские гг-фаги написали патч позволяющий\n",
      "--------------------\n",
      "\n",
      "в час х сами они слишком заняты поглощением еды и копипасты с engadget com membrana ru а позже получили разрыв сердца \n",
      " поэтому пластинка как оригинал а лишь одна маленькая деталь стоит на чёрном фоне \n",
      " было открыто и смело выходят на тернистый путь сомнений и объяснений приписывает этот флаг в кустах раком » \n",
      " при этом большинство приходящих поступать в нераспечатанном состоянии потому что достичь большей привлекательности действиями направленными на сознательную реакцию почти не испытывают так как мы гадим » « неуподоблюсь » — ответил миша \n",
      " самыми первыми концептами « thief » \n",
      " ведущий аналитик не поверил бы\n",
      "--------------------\n",
      "\n",
      "на новый сервер \n",
      " но вскоре выяснилось что в конце концов не рассчитав силы огребает пиздюлей от дружков жениха невесты причем в его арсенале образовалась целая куча багов и вездесущий пиар готеля \n",
      " с каких то результатов к лету этого же года и рост утилизационного сбора в начале 90-х съебался из отеля и не дают но как-то раз висело несколько его уровней которые после получения диплома тоже пойдут в народ на висте знают о степени схожести западного мультика и аниме стали популярны компьютеры nec pc-8801 и nec благополучно были привиты в детстве не строил » так как при малярии \n",
      "\n",
      "--------------------\n",
      "\n",
      "он хотел погонять несколько пионеров подвернувшихся ему под предлогом « малозначимости » \n",
      " а есть конкретное число отогнутых пальцев — расценивает песню в названии обозначает не что иное как фимоз нашей исторической науки в результате производство встает все равно полностью переделывает сценарий на свой и так далее однако рока в этом случае узбекистан — единственная программа из комплекта выпадает \n",
      " например добрый улыбающийся номад — молоко говядина « молочные » улуны пуэры и т д \n",
      " « база выпускающая летающие тарелки для картины были куплены у ccp шведами из teliasonera в девичестве coderipper проводивший в 95-97 годах демопати enlight \n",
      "\n",
      "--------------------\n",
      "\n",
      "существует ли бог или нет уверенности в том что в среде молодёжных субкультур последних трёх десятилетий — от души пострелявший в илитарном лагере для del ниггеров и прочих говноманий на форуме росбалта \n",
      " так как отличить подобного индивида \n",
      " в 2010 году \n",
      " много их \n",
      " в 2005 году дарья попробовала себя на странице тома хэнкса была фильмография на этот счет \n",
      " отдельным пунктом можно отметить подзабытый сериал « ну погоди » целиком и полностью сгоревшие тела \n",
      " с массовым потреблением алкоголя \n",
      " противники евромайдана утверждают что они должны затормозиться \n",
      " понятное дело еще продолжается \n",
      " в 1845 году нищеброда\n",
      "--------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Генерируем 5 примеров\n",
    "for _ in range(5):\n",
    "    generated_text = generate_trigram(\n",
    "        matrix_lurk, id2word, word2id, id2bigram, bigram2id)\n",
    "    print(generated_text.replace('<end>', '\\n') + '\\n' + '-' * 20 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5485e2e9",
   "metadata": {},
   "source": [
    "Не, ну в целом-то смешно и связно, хоть и обрывается на середине и выглядит местами как просто куски из разных статей! Хотя есть несколько эпичных фразочек в стиле луркояза, пожалуй! :D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba6e0f2",
   "metadata": {},
   "source": [
    "## 6. Считаем перплексию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae222a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция перплексии из семинара\n",
    "def perplexity(logp, N):\n",
    "    return np.exp((-1 / N) * logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4be43789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trigram_proba(text, unigrams, bigrams, trigrams):\n",
    "    prob = 0\n",
    "    tokens = normalize(text)\n",
    "    N = len(tokens)\n",
    "\n",
    "    if N == 0:\n",
    "        return 0, 0\n",
    "\n",
    "    # Оборачиваем в <start> и <end>\n",
    "    prob_tokens = ['<start>', '<start>'] + tokens + ['<end>']\n",
    "\n",
    "    for trigram_str in ngrammer(prob_tokens, n=3):\n",
    "        w1, w2, w3 = trigram_str.split()\n",
    "        context_bigram = f\"{w1} {w2}\"\n",
    "\n",
    "        if context_bigram in bigrams and trigram_str in trigrams:\n",
    "            # Используем логарифм частного\n",
    "            prob += np.log(trigrams[trigram_str] / bigrams[context_bigram])\n",
    "        else:\n",
    "            # Сглаживание для неизвестных триграм\n",
    "            prob += np.log(2e-5)\n",
    "\n",
    "    return prob, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19f44ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средняя перплексия триграммной модели: 30427535.93038977\n"
     ]
    }
   ],
   "source": [
    "# Считаем перплексию на тестовой выборке\n",
    "all_ps = []\n",
    "for sent in test_sentences:\n",
    "    logp, N = compute_trigram_proba(sent, unigrams, bigrams, trigrams)\n",
    "\n",
    "    if N > 0:\n",
    "        all_ps.append(perplexity(logp, N))\n",
    "\n",
    "mean_perplexity = np.mean(all_ps)\n",
    "print(f\"Средняя перплексия триграммной модели: {mean_perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e54245f",
   "metadata": {},
   "source": [
    "Жесть!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d49c2a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средняя перплексия триграммной модели на 50 предложениях: 37161.092324562225\n"
     ]
    }
   ],
   "source": [
    "# А если сократить тестовую выборку до 50 предложений?\n",
    "\n",
    "small_test_sentences = test_sentences[:50]\n",
    "all_ps_small = []\n",
    "for sent in small_test_sentences:\n",
    "    logp, N = compute_trigram_proba(sent, unigrams, bigrams, trigrams)\n",
    "\n",
    "    if N > 0:\n",
    "        all_ps_small.append(perplexity(logp, N))\n",
    "mean_perplexity_small = np.mean(all_ps_small)\n",
    "print(\n",
    "    f\"Средняя перплексия триграммной модели на 50 предложениях: {mean_perplexity_small}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11724e9f",
   "metadata": {},
   "source": [
    "Ну, прямо скажем — сильно меньше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a28cb984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средняя перплексия триграммной модели на 10 предложениях: 19486.93981412321\n"
     ]
    }
   ],
   "source": [
    "# А на 10?\n",
    "\n",
    "tiny_test_sentences = test_sentences[:10]\n",
    "all_ps_tiny = []\n",
    "for sent in tiny_test_sentences:\n",
    "    logp, N = compute_trigram_proba(sent, unigrams, bigrams, trigrams)\n",
    "\n",
    "    if N > 0:\n",
    "        all_ps_tiny.append(perplexity(logp, N))\n",
    "mean_perplexity_tiny = np.mean(all_ps_tiny)\n",
    "print(\n",
    "    f\"Средняя перплексия триграммной модели на 10 предложениях: {mean_perplexity_tiny}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788b284b",
   "metadata": {},
   "source": [
    "Получается, что чем больше предложений в нашей выборке, на которой мы считаем перплексию, тем больше у нас получается триграмм, которые модель не может предсказать, потому что в трейне их нет, и количество шума, пропусков растёт катастрофически, из-за чего и имеем такую огромную цифру. В целом получается, что наша модель хуже предсказывает корпус, чем марковская, потому что количество ненайденных биграмм для контекста гораздо больше, чем одиночных слов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a8dd5",
   "metadata": {},
   "source": [
    "## Задание № 2* (2 балла). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f733858c",
   "metadata": {},
   "source": [
    "Измените функцию generate_with_beam_search так, чтобы она работала с моделью, которая учитывает два предыдущих слова. \n",
    "Сравните получаемый результат с первым заданием. \n",
    "Также попробуйте начинать генерацию не с нуля (подавая \\<start> \\<start>), а с какого-то промпта. Но помните, что учитываться будут только два последних слова, так что не делайте длинные промпты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c426746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Beam:\n",
    "    def __init__(self, sequence: list, score: float):\n",
    "        self.sequence: list = sequence\n",
    "        self.score: float = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2bd9e99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trigram_beam_search(\n",
    "        matrix,\n",
    "        id2word,\n",
    "        word2id,\n",
    "        id2bigram,\n",
    "        bigram2id,\n",
    "        n=50,\n",
    "        max_beams=5,\n",
    "        prompt='<start> <start>'):\n",
    "\n",
    "    if prompt == '<start> <start>':\n",
    "        # Если промпт по умолчанию, просто начинаем с двух стартов\n",
    "        normalized_prompt = ['<start>', '<start>']\n",
    "    else:\n",
    "        # Нормализуем промпт\n",
    "        normalized_prompt = normalize(prompt)\n",
    "\n",
    "    # Если промпт короче 2 слов, дополняем его токенами <start>\n",
    "    start_tokens = (['<start>'] * (2 - len(normalized_prompt))\n",
    "                    ) + normalized_prompt\n",
    "    start_tokens = start_tokens[-2:]  # Оставляем ровно два токена\n",
    "\n",
    "    initial_node = Beam(sequence=start_tokens, score=0.0)\n",
    "    beams = [initial_node]\n",
    "\n",
    "    for i in range(n):\n",
    "        # Делаем n шагов генерации\n",
    "        new_beams = []\n",
    "\n",
    "        # На каждом шаге продолжаем каждый из имеющихся лучей\n",
    "        for beam in beams:\n",
    "            # Лучи, которые уже закончены, не продолжаем (но и не удаляем)\n",
    "            if beam.sequence[-1] == '<end>':\n",
    "                new_beams.append(beam)\n",
    "                continue\n",
    "\n",
    "            context_bigram = f\"{beam.sequence[-2]} {beam.sequence[-1]}\"\n",
    "\n",
    "            # Если такой биграммы-контекста нет в трейне, луч отбрасывается\n",
    "            if context_bigram not in bigram2id:\n",
    "                continue\n",
    "\n",
    "            context_id = bigram2id[context_bigram]\n",
    "\n",
    "            probas = matrix[context_id].toarray()[0]\n",
    "\n",
    "            # Если нет продолжений, луч отбрасывается\n",
    "            if not np.any(probas):\n",
    "                continue\n",
    "\n",
    "            # Возьмем топ-k самых вероятных продолжений\n",
    "            top_idxs = probas.argsort()[:-(max_beams + 1):-1]\n",
    "\n",
    "            for top_id in top_idxs:\n",
    "                if not probas[top_id]:\n",
    "                    break\n",
    "\n",
    "                new_word = id2word[top_id]\n",
    "                new_sequence = beam.sequence + [new_word]\n",
    "\n",
    "                new_score = beam.score + np.log(probas[top_id] + 1e-10)\n",
    "\n",
    "                new_beam = Beam(sequence=new_sequence, score=new_score)\n",
    "                new_beams.append(new_beam)\n",
    "\n",
    "        # Сортируем все новые лучи по *нормализованному* скору\n",
    "        # Делим на длину, чтобы не штрафовать длинные предложения\n",
    "        beams = sorted(new_beams, key=lambda x: x.score /\n",
    "                       len(x.sequence), reverse=True)[:max_beams]\n",
    "\n",
    "    # В конце возвращаем отсортированные лучшие гипотезы\n",
    "    sorted_beams = sorted(beams, key=lambda x: x.score /\n",
    "                          len(x.sequence), reverse=True)\n",
    "\n",
    "    final_sequences = []\n",
    "    for beam in sorted_beams:\n",
    "        final_sequence = \" \".join(beam.sequence[2:])\n",
    "        final_sequences.append(final_sequence)\n",
    "\n",
    "    return final_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25617d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "на самом деле \n",
      "\n",
      "----------\n",
      "на самом деле это не так уж и много чего ещё \n",
      "\n",
      "----------\n",
      "на самом деле это не так уж и плохо \n",
      "\n",
      "----------\n",
      "на самом деле это не так уж и много других \n",
      "\n",
      "----------\n",
      "на самом деле это не так уж и много чего ещё не было \n",
      "\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Генерируем с настройками по умолчанию, начиная с <start> <start>\n",
    "default_generation = generate_trigram_beam_search(\n",
    "    matrix_lurk, id2word, word2id, id2bigram, bigram2id\n",
    ")\n",
    "for text in default_generation:\n",
    "    print(text.replace('<end>', '\\n'))\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0209d6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "визуальная новелла запиленная силами отечественных сценаристов и режиссеров \n",
      "\n",
      "----------\n",
      "визуальная новелла запиленная силами отечественных сценаристов и продюсеров \n",
      "\n",
      "----------\n",
      "визуальная новелла обычно требует \n",
      "\n",
      "----------\n",
      "визуальная новелла запиленная силами отечественных сценаристов и продюсеров рубящих еврейский расовый жид \n",
      "\n",
      "----------\n",
      "визуальная новелла запиленная силами отечественных сценаристов и продюсеров рубящих еврейский расовый жид по имени « ан стасия lifestyle d s … \n",
      "\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Генерируем с промптом\n",
    "prompted_gen = generate_trigram_beam_search(\n",
    "    matrix_lurk, id2word, word2id, id2bigram, bigram2id,\n",
    "    prompt=\"визуальная новелла\"\n",
    ")\n",
    "for text in prompted_gen:\n",
    "    print(f\"визуальная новелла {text.replace('<end>', '\\n')}\")\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562a33cf",
   "metadata": {},
   "source": [
    "Я бы сказал, что результат на самых длинных последовательностях связнее случайной генерации, но варианты по сути представляют собой просто генерации разной длины, причём не попорядку. Кроме того, похоже, они детерминированы и при перезапуске с теми же параметрами не будет ничего нового. Случайная генерация как будто выдавала более богатый текст, поэтому в смысле разговоров о конкретных улучшениях я бы остановился на ней и попытался использовать какие-то более продвинутые техники сэмплирование и игру с гиперпараметрами типа температуры."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
