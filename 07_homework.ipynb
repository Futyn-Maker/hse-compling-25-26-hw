{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b20f786e",
   "metadata": {},
   "source": [
    "# Домашнее задание № 2. Мешок слов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf72d19",
   "metadata": {},
   "source": [
    "## Задание 1 (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a045e99",
   "metadata": {},
   "source": [
    "У векторайзеров в sklearn есть встроенная токенизация на регулярных выражениях. Найдите способо заменить её на кастомную токенизацию"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b4d453",
   "metadata": {},
   "source": [
    "Обучите векторайзер с дефолтной токенизацией, с токенизацией razdel.tokenize и с токенизацией+лемматизацией из mystem. Обучите классификатор (любой) с каждым из векторизаторов. Сравните метрики и выберите победителя. \n",
    "\n",
    "(в вашей тетрадке должен быть код обучения и все метрики; если вы сдаете в .py файлах то сохраните полученные метрики в отдельном файле или в комментариях)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "129c4d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4314de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('labeled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e76d2b9",
   "metadata": {},
   "source": [
    "Будем использовать обычный мешок слов. В токенизации с razdel и Mystem попробуем два подхода: с удалением стоп-слов и без него."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbffbbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import razdel\n",
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a260c7d9",
   "metadata": {},
   "source": [
    "### 1. Готовим обучающую и тестовую выборки (с сохранением баланса классов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d2477b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Баланс классов:\n",
      "toxic\n",
      "0.0    0.66514\n",
      "1.0    0.33486\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Баланс классов:\")\n",
    "print(data['toxic'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73072097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Размер обучающей выборки: 11529\n",
      "Размер тестовой выборки: 2883\n"
     ]
    }
   ],
   "source": [
    "X = data['comment']\n",
    "y = data['toxic']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)\n",
    "\n",
    "print(f\"\\nРазмер обучающей выборки: {len(X_train)}\")\n",
    "print(f\"Размер тестовой выборки: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8b67b0",
   "metadata": {},
   "source": [
    "### 2. Токенизаторы\n",
    "\n",
    "Будем оставлять только алфавитные токены (слова). Встроенная регулярка в CountVectorizer (`token_pattern) оставляет ещё и цифры, поэтому у нас будет ещё и такое преимущество. Конечно, `token_pattern` можно и изменить, но зачем?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea111507",
   "metadata": {},
   "outputs": [],
   "source": [
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "mystem = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16563366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Токенизация с помощью razdel\n",
    "def tokenize_razdel(text):\n",
    "    return [token.text for token in razdel.tokenize(\n",
    "        text) if token.text.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32112f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Токенизация и лемматизация с Mystem\n",
    "def tokenize_mystem(text):\n",
    "    tokens = mystem.lemmatize(text)\n",
    "    # Оставляем только слова\n",
    "    tokens = [token for token in tokens if token.isalpha() and token.strip()]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f680685d",
   "metadata": {},
   "source": [
    "### 3. Пайплайны: логистическая регрессия с по-разному настроенными векторизаторами\n",
    "\n",
    "Тут удобнее всего использовать Sklearn pipeline, в который сразу передавать векторизатор и классификатор с нужными параметрами. По-хорошему надо бы задать нужные параметры через grid_search и выполнить всё одним скопом, но лень."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76af5983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Метод 1: Default Tokenizer ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.94      0.89      1918\n",
      "         1.0       0.86      0.67      0.75       965\n",
      "\n",
      "    accuracy                           0.85      2883\n",
      "   macro avg       0.85      0.81      0.82      2883\n",
      "weighted avg       0.85      0.85      0.85      2883\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Метод 1: Default Tokenizer ---\")\n",
    "\n",
    "pipeline_default = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', LogisticRegression(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "# Обучаем\n",
    "pipeline_default.fit(X_train, y_train)\n",
    "\n",
    "# Предсказываем и выводим метрики\n",
    "preds_default = pipeline_default.predict(X_test)\n",
    "print(classification_report(y_test, preds_default))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0a8a538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Метод 2: razdel.tokenize + Stopwords ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/futyn/miniconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.94      0.88      1918\n",
      "         1.0       0.85      0.61      0.71       965\n",
      "\n",
      "    accuracy                           0.83      2883\n",
      "   macro avg       0.84      0.78      0.79      2883\n",
      "weighted avg       0.83      0.83      0.82      2883\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Метод 2: razdel.tokenize + Stopwords ---\")\n",
    "\n",
    "pipeline_razdel = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize_razdel, stop_words=russian_stopwords)),\n",
    "    ('clf', LogisticRegression(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "# Обучаем\n",
    "pipeline_razdel.fit(X_train, y_train)\n",
    "\n",
    "# Предсказываем и выводим метрики\n",
    "preds_razdel = pipeline_razdel.predict(X_test)\n",
    "print(classification_report(y_test, preds_razdel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cb2c5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Метод 3: Mystem lemmatize + Stopwords ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/futyn/miniconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/futyn/miniconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['весь', 'свой', 'это'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.94      0.90      1918\n",
      "         1.0       0.85      0.69      0.76       965\n",
      "\n",
      "    accuracy                           0.86      2883\n",
      "   macro avg       0.85      0.81      0.83      2883\n",
      "weighted avg       0.85      0.86      0.85      2883\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Метод 3: Mystem lemmatize + Stopwords ---\")\n",
    "\n",
    "pipeline_mystem = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize_mystem, stop_words=russian_stopwords)),\n",
    "    ('clf', LogisticRegression(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "# Обучаем\n",
    "pipeline_mystem.fit(X_train, y_train)\n",
    "\n",
    "# Предсказываем и выводим метрики\n",
    "preds_mystem = pipeline_mystem.predict(X_test)\n",
    "print(classification_report(y_test, preds_mystem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fe8198",
   "metadata": {},
   "source": [
    "Хм, а если не удалять стоп-слова?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62b2cfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Метод 4: razdel.tokenize, сохраняем стопслова ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/futyn/miniconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.94      0.89      1918\n",
      "         1.0       0.85      0.68      0.75       965\n",
      "\n",
      "    accuracy                           0.85      2883\n",
      "   macro avg       0.85      0.81      0.82      2883\n",
      "weighted avg       0.85      0.85      0.85      2883\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Метод 4: razdel.tokenize, сохраняем стопслова ---\")\n",
    "\n",
    "pipeline_razdel = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize_razdel)),\n",
    "    ('clf', LogisticRegression(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "# Обучаем\n",
    "pipeline_razdel.fit(X_train, y_train)\n",
    "\n",
    "# Предсказываем и выводим метрики\n",
    "preds_razdel = pipeline_razdel.predict(X_test)\n",
    "print(classification_report(y_test, preds_razdel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ca5f022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Метод 5: Mystem lemmatize, с сохранением стопслов ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/futyn/miniconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.94      0.91      1918\n",
      "         1.0       0.87      0.74      0.80       965\n",
      "\n",
      "    accuracy                           0.88      2883\n",
      "   macro avg       0.87      0.84      0.85      2883\n",
      "weighted avg       0.88      0.88      0.87      2883\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Метод 5: Mystem lemmatize, с сохранением стопслов ---\")\n",
    "\n",
    "pipeline_mystem = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize_mystem)),\n",
    "    ('clf', LogisticRegression(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "# Обучаем\n",
    "pipeline_mystem.fit(X_train, y_train)\n",
    "\n",
    "# Предсказываем и выводим метрики\n",
    "preds_mystem = pipeline_mystem.predict(X_test)\n",
    "print(classification_report(y_test, preds_mystem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593a1593",
   "metadata": {},
   "source": [
    "Что ж, однозначный победитель: токенизатор+лемматизатор Mystem с сохранением стоп-слов. Можно сделать два вывода:\n",
    "\n",
    "1. Для русского языка лемматизация существенно повышает качество. Во-первых, потому что это флективный язык, и сведение множества форм слова к одной очень полезно. Во-вторых, потому что это уменьшает размер словаря и снижает размерность итоговых векторов, что упрощает задачу по обучению.\n",
    "2. Для задачи определения сентимента стоп-слова важны, удалять их не надо. В принципе, логично, потому что там есть и отрицание, и всякие местоимения, которые часто используются в токсичных текстах.\n",
    "\n",
    "А ещё дисбаланс классов всё-таки чувствуется, потому что самая низкая метрика во всех случаях — recall для положительного класса (токсичные тексты). Выходит, что модели всё же biased в сторону нетоксичных текстов и чаще предсказывают 0, потому что видели таких текстов больше."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b9076e",
   "metadata": {},
   "source": [
    "## Задание 2 (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e25357",
   "metadata": {},
   "source": [
    "Обучите 2 любых разных классификатора (используя алгоритмы из семинара). Используйте eng сабсет из `mteb/multi-hatecheck` в качестве датасета. \n",
    "\n",
    "Предскажите токсичность (`is_hateful`) для текстов из тестовой выборки (используйте одну и ту же выборку для обоих классификаторов) и найдите 10 самых токсичных для каждого из классификаторов (по вероятности). Сравните получаемые тексты - какие тексты совпадают, какие отличаются, правда ли тексты токсичные?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de962ad",
   "metadata": {},
   "source": [
    "Требования к моделям:   \n",
    "а) один классификатор должен использовать CountVectorizer, другой TfidfVectorizer  \n",
    "б) у векторазера должны быть вручную заданы как минимум 5 параметров (можно ставить разные параметры tfidfvectorizer и countvectorizer)  \n",
    "в) у классификатора должно быть задано вручную как минимум 2 параметра (по возможности)  \n",
    "г)  f1 мера каждого из классификаторов должна быть минимум 0.60 \n",
    "\n",
    "*random_seed, encoding, decode_error не считаются за параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac0a5471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e952289",
   "metadata": {},
   "source": [
    "### 1. Загружаем данные и смотрим на них, а также готовим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66f85d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['functionality', 'text', 'is_hateful', 'lang'],\n",
      "        num_rows: 40310\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "hatecheck = load_dataset('mteb/multi-hatecheck')\n",
    "print(hatecheck)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb491846",
   "metadata": {},
   "source": [
    "У нас есть только тестовый датасет, так что придётся вручную бить его на сплиты и делать его часть тренировочным. В академических работах, кстати, это жёстко порицается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "958e927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фильтруем по английскому языку\n",
    "eng_hatecheck = hatecheck.filter(lambda e: e[\"lang\"] == 'eng')\n",
    "\n",
    "# Конвертируем в pandas DataFrame для удобства\n",
    "df = pd.DataFrame(eng_hatecheck['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0603d9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       functionality                      text is_hateful lang\n",
      "0  derog_neg_emote_h            I hate women.     hateful  eng\n",
      "1  derog_neg_emote_h     I hate trans people.     hateful  eng\n",
      "2  derog_neg_emote_h       I hate gay people.     hateful  eng\n",
      "3  derog_neg_emote_h     I hate black people.     hateful  eng\n",
      "4  derog_neg_emote_h  I hate disabled people.     hateful  eng\n",
      "\n",
      "Информация о DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3728 entries, 0 to 3727\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   functionality  3728 non-null   object\n",
      " 1   text           3728 non-null   object\n",
      " 2   is_hateful     3728 non-null   object\n",
      " 3   lang           3728 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 116.6+ KB\n",
      "\n",
      "Баланс классов:\n",
      "is_hateful\n",
      "hateful        0.6875\n",
      "non-hateful    0.3125\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "print(\"\\nИнформация о DataFrame:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nБаланс классов:\")\n",
    "print(df['is_hateful'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b304af8",
   "metadata": {},
   "source": [
    "Класс, дисбаланс в другую сторону!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "745bd58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразуем текстовые метки в бинарные (1 для 'hateful', 0 для 'not_hateful')\n",
    "df['label'] = df['is_hateful'].apply(lambda x: 1 if x == 'hateful' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f6ee43",
   "metadata": {},
   "source": [
    "### 2. Делаем тренировочную и тестовую выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90311fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Размер обучающей выборки: 2982\n",
      "Размер тестовой выборки: 746\n"
     ]
    }
   ],
   "source": [
    "X = df['text']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)\n",
    "\n",
    "print(f\"\\nРазмер обучающей выборки: {len(X_train)}\")\n",
    "print(f\"Размер тестовой выборки: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec549d4b",
   "metadata": {},
   "source": [
    "### 3. Модельки\n",
    "\n",
    "Возьмём наивный Байесовский классификатор с мешком слов и мощный лес с TF-IDF. Будем использовать биграммы и триграммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32c9bdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_nb = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "        max_df=0.90,  # игнорируем слова, которые встречаются в > 90% документов\n",
    "        min_df=3,  # игнорируем слова, которые встречаются < 3 раз\n",
    "        ngram_range=(1, 2),  # включаем биграммы\n",
    "        max_features=10000,  # ограничиваем словарь 10000 слов\n",
    "        lowercase=True\n",
    "    )),\n",
    "    ('clf', MultinomialNB(\n",
    "        alpha=0.1,\n",
    "        fit_prior=False\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29c40361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Обучение: Модель 1 (Naive Bayes + CountVectorizer) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86       233\n",
      "           1       0.92      0.96      0.94       513\n",
      "\n",
      "    accuracy                           0.92       746\n",
      "   macro avg       0.91      0.89      0.90       746\n",
      "weighted avg       0.91      0.92      0.91       746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Обучение: Модель 1 (Naive Bayes + CountVectorizer) ---\")\n",
    "pipeline_nb.fit(X_train, y_train)\n",
    "\n",
    "preds_nb = pipeline_nb.predict(X_test)\n",
    "print(classification_report(y_test, preds_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28bc285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_rf = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "        max_df=0.85,\n",
    "        min_df=5,\n",
    "        ngram_range=(1, 3),\n",
    "        max_features=8000,\n",
    "        sublinear_tf=True  # применяем 1 + log(tf) для сглаживания\n",
    "    )),\n",
    "    ('clf', RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=30,\n",
    "        class_weight='balanced',  # пытаемся бороться с дисбалансом\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ee6b26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Обучение: Модель 2 (Random Forest + TfidfVectorizer) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.81      0.88       233\n",
      "           1       0.92      0.99      0.95       513\n",
      "\n",
      "    accuracy                           0.93       746\n",
      "   macro avg       0.94      0.90      0.92       746\n",
      "weighted avg       0.93      0.93      0.93       746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Обучение: Модель 2 (Random Forest + TfidfVectorizer) ---\")\n",
    "pipeline_rf.fit(X_train, y_train)\n",
    "\n",
    "preds_rf = pipeline_rf.predict(X_test)\n",
    "print(classification_report(y_test, preds_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f5dc60",
   "metadata": {},
   "source": [
    "### Анализируем топ-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4958c352",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_nb = pipeline_nb.predict_proba(X_test)[:, 1]\n",
    "probs_rf = pipeline_rf.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f7b6279",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'text': X_test,\n",
    "    'true_label': y_test,\n",
    "    'prob_nb': probs_nb,\n",
    "    'prob_rf': probs_rf\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "222748ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 10 самых токсичных текстов (Naive Bayes + CountVectorizer) ---\n",
      "                                                                                                  text  \\\n",
      "1405                          I can't believe these women, thinking I won't murder every one of them.    \n",
      "473                           The life of any insect is more valuable than that of a disabled person.    \n",
      "1530                     Some immigrants get into government. They are still just common trash to me.    \n",
      "1502            An immigrant must have committed that crime. Only they can be that violent and cruel.    \n",
      "1523          Immigrants can be kind to you at times. Don't forget they're still just worthless scum.    \n",
      "1598  We're talking about disabled people here. They can't help themselves but be complete dickheads.    \n",
      "1590                           They are all complete assholes. Needless to say they are black people.    \n",
      "1434                        Trans people are never going to change so we should just get rid of them.    \n",
      "1600       We're talking about immigrants here. They can't help themselves but be complete dickheads.    \n",
      "824                                               Someone should snap the necks of all those Muslims.    \n",
      "\n",
      "      prob_nb  true_label  \n",
      "1405      1.0           1  \n",
      "473       1.0           1  \n",
      "1530      1.0           1  \n",
      "1502      1.0           1  \n",
      "1523      1.0           1  \n",
      "1598      1.0           1  \n",
      "1590      1.0           1  \n",
      "1434      1.0           1  \n",
      "1600      1.0           1  \n",
      "824       1.0           1  \n"
     ]
    }
   ],
   "source": [
    "print(\"--- 10 самых токсичных текстов (Naive Bayes + CountVectorizer) ---\")\n",
    "top_10_nb = results_df.sort_values(by='prob_nb', ascending=False).head(10)\n",
    "print(top_10_nb[['text', 'prob_nb', 'true_label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29327364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 10 самых токсичных текстов (Random Forest + TfidfVectorizer) ---\n",
      "                                      text   prob_rf  true_label\n",
      "906           You are nothing but a slut.   0.972599           1\n",
      "3660         You are nothing but a b1tch.   0.972350           1\n",
      "917           You are nothing but a mong.   0.972350           1\n",
      "3672          You are nothing but a m0ng.   0.972350           1\n",
      "914           You are nothing but a coon.   0.972350           1\n",
      "913         You are nothing but a nigger.   0.972350           1\n",
      "909        You are nothing but a shemale.   0.972350           1\n",
      "3675   You are nothing but a camelfuck3r.   0.972350           1\n",
      "920   You are nothing but a camel fucker.   0.972350           1\n",
      "3668        You are nothing but a nigg3r.   0.972350           1\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 10 самых токсичных текстов (Random Forest + TfidfVectorizer) ---\")\n",
    "top_10_rf = results_df.sort_values(by='prob_rf', ascending=False).head(10)\n",
    "print(top_10_rf[['text', 'prob_rf', 'true_label']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0d9e01",
   "metadata": {},
   "source": [
    "Прежде всего я хочу сказать, что датасет выглядит крайне искусственно. Те комментарии, которые мы увидели, слишком гротескно-провокационные, едва ли в живом Интернете их в таком виде можно встретить даже у законченных фашистов. Отсюда и очень высокие метрики у обоих классификаторов.\n",
    "\n",
    "Мы видим, что наивный байесовский классификатор присваивает вероятность 1 всем нашим топ-10, подозреваю, что и дальше таких тоже очень много — поэтому нельзя сказать, что это \"самые токсичные по мнению классификатора\" тексты — таких очень много. Но в топе мы наблюдаем просто преступные и расистские хейт-высказывания.\n",
    "\n",
    "Рандомный лес выдаёт более разнообразные вероятности, и в топе оказались, по сути, индивидуальные оскорбления «you are nothing but ...», которые, конечно, являются хейтом, но всё же сомневаюсь, что этот показатель у них должен быть выше, чем у высказываний с ненавистью по отношению ко всей группе. Учитывая более низкий Recall для нулевого класса, видимо, имеет место переобучение и концентрация на таких словах, как *you* и *nothing*, ну и, наверное, на названиях самих групп.\n",
    "\n",
    "Прямых миссклассификаций ни одна модель в топе не допустила.\n",
    "\n",
    "Так что скорее выходит, что с таким простым и понятным, но искусственным и малополезным в реальности датасеты немного лучше справляются более простые классификаторы, а при усложнениях уже возможен риск переобучения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f228c3e",
   "metadata": {},
   "source": [
    "## Задание 3 (2 балла - 0,5 балл за каждый классификатор)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566929b7",
   "metadata": {},
   "source": [
    "Для классификаторов Logistic Regression, Decision Trees, Naive Bayes, RandomForest найдите способ извлечь важность признаков для предсказания токсичного класса (любой из датасетов). Сопоставьте полученные числа со словами (или нграммами) в словаре и найдите топ - 5 \"токсичных\" слов для каждого из классификаторов. \n",
    "\n",
    "Важное требование: в топе не должно быть стоп-слов. Для этого вам нужно будет правильным образом настроить векторизацию. \n",
    "Также как и в предыдущем задании у классификаторов должно быть задано вручную как минимум 2 параметра (по возможности, f1 мера каждого из классификаторов должна быть минимум 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ad5cf3",
   "metadata": {},
   "source": [
    "Возьмём датасет из первого задания, и попробуем улучшить наш результат с мешком слов+Mystem. Будем использовать TF-IDF и триграммы, ну и лемматизацию, конечно же. Мы попробуем варианты с удалением стоп-слов и без него, так как мы увидели, что логистическая регрессия со стоп-словами перформит хуже, чем без них — проверим это и на других классификаторах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81f86878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2b0b8c",
   "metadata": {},
   "source": [
    "### 1. Функция для извлечения и отображения топ-N фичей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15af4e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_features(pipeline, top_n=10):\n",
    "    # Извлекаем обученный векторизатор и классификатор\n",
    "    vectorizer = pipeline.named_steps['vect']\n",
    "    classifier = pipeline.named_steps['clf']\n",
    "\n",
    "    # Получаем имена фичей\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    coef_data = None\n",
    "    top_indices = None\n",
    "\n",
    "    # Логика для логистической регрессии\n",
    "    if hasattr(classifier, 'coef_'):\n",
    "        print(f\"Top {top_n} токсичных фичей:\")\n",
    "\n",
    "        if classifier.coef_.shape[0] == 1:\n",
    "            coef_data = classifier.coef_[0]\n",
    "        else:\n",
    "            coef_data = classifier.coef_[1]\n",
    "\n",
    "        # Сортируем по убыванию (самые токсичные слова)\n",
    "        top_indices = np.argsort(coef_data)[-top_n:][::-1]\n",
    "\n",
    "    # Логика для деревьев и леса\n",
    "    elif hasattr(classifier, 'feature_importances_'):\n",
    "        print(\n",
    "            f\"Top {top_n} важных фичей:\")\n",
    "        coef_data = classifier.feature_importances_\n",
    "\n",
    "        # Сортируем по убыванию (самые важные слова)\n",
    "        top_indices = np.argsort(coef_data)[-top_n:][::-1]\n",
    "\n",
    "    # Логика для Naive Bayes\n",
    "    elif hasattr(classifier, 'feature_log_prob_'):\n",
    "        print(f\"Top {top_n} токсичных фичей:\")\n",
    "\n",
    "        # Рассчитываем важность как log(P(word|toxic)) - log(P(word|not_toxic))\n",
    "        coef_data = classifier.feature_log_prob_[\n",
    "            1] - classifier.feature_log_prob_[0]\n",
    "\n",
    "        # Сортируем по убыванию (самые токсичные слова)\n",
    "        top_indices = np.argsort(coef_data)[-top_n:][::-1]\n",
    "\n",
    "    else:\n",
    "        print(\n",
    "            f\"Этот классификатор ({type(classifier).__name__}) не имеет 'coef_', 'feature_importances_' или 'feature_log_prob_'.\")\n",
    "        return\n",
    "\n",
    "    # Выводим результат\n",
    "    if top_indices is not None:\n",
    "        for i in top_indices:\n",
    "            print(f\"  {feature_names[i]}: {coef_data[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356b432d",
   "metadata": {},
   "source": [
    "### 2. Параметры векторизатора (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcb2444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_params = {\n",
    "    'tokenizer': tokenize_mystem,\n",
    "    'stop_words': None,\n",
    "    'ngram_range': (1, 3),\n",
    "    'min_df': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65077480",
   "metadata": {},
   "source": [
    "### 3. Логрег"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f00aace4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Logistic Regression ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Logistic Regression ---\")\n",
    "\n",
    "pipeline_lr = Pipeline([\n",
    "    ('vect', TfidfVectorizer(**tfidf_params)),\n",
    "    ('clf', LogisticRegression(\n",
    "        random_state=42,\n",
    "        solver='liblinear',\n",
    "        class_weight='balanced',\n",
    "        C=5.0\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51506999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/futyn/miniconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.92      0.91      1918\n",
      "         1.0       0.84      0.80      0.82       965\n",
      "\n",
      "    accuracy                           0.88      2883\n",
      "   macro avg       0.87      0.86      0.87      2883\n",
      "weighted avg       0.88      0.88      0.88      2883\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_lr.fit(X_train, y_train)\n",
    "\n",
    "preds_lr = pipeline_lr.predict(X_test)\n",
    "print(classification_report(y_test, preds_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f793796c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 токсичных фичей:\n",
      "  ты: 9.4352\n",
      "  хохол: 9.2681\n",
      "  тупой: 7.8799\n",
      "  хохлов: 7.5105\n",
      "  шлюха: 7.0121\n",
      "  дебил: 6.9429\n",
      "  русский: 6.7398\n",
      "  быдло: 6.0374\n",
      "  баба: 5.4191\n",
      "  долбоеб: 5.2284\n"
     ]
    }
   ],
   "source": [
    "show_top_features(pipeline_lr, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ff435a",
   "metadata": {},
   "source": [
    "> Весь русский Твиттер в десяти словах — фото в цвете"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3f5c37",
   "metadata": {},
   "source": [
    "### 4. Решающее дерево"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "257452d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Decision Tree ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Decision Tree ---\")\n",
    "\n",
    "pipeline_dt = Pipeline([\n",
    "    ('vect', TfidfVectorizer(**tfidf_params)),\n",
    "    ('clf', DecisionTreeClassifier(\n",
    "        random_state=42,\n",
    "        max_depth=25,\n",
    "        min_samples_leaf=5,\n",
    "        class_weight='balanced'\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63018b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/futyn/miniconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.68      0.75      1918\n",
      "         1.0       0.54      0.72      0.62       965\n",
      "\n",
      "    accuracy                           0.70      2883\n",
      "   macro avg       0.68      0.70      0.68      2883\n",
      "weighted avg       0.73      0.70      0.71      2883\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_dt.fit(X_train, y_train)\n",
    "\n",
    "preds_dt = pipeline_dt.predict(X_test)\n",
    "print(classification_report(y_test, preds_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb28ed1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 важных фичей:\n",
      "  ты: 0.1665\n",
      "  быть: 0.0617\n",
      "  но: 0.0411\n",
      "  хохол: 0.0397\n",
      "  если: 0.0320\n",
      "  в: 0.0314\n",
      "  русский: 0.0298\n",
      "  не: 0.0287\n",
      "  он: 0.0246\n",
      "  год: 0.0217\n"
     ]
    }
   ],
   "source": [
    "show_top_features(pipeline_dt, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79031d11",
   "metadata": {},
   "source": [
    "### 5. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdc80ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Multinomial Naive Bayes ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Multinomial Naive Bayes ---\")\n",
    "\n",
    "pipeline_nb = Pipeline([\n",
    "    ('vect', TfidfVectorizer(**tfidf_params)),\n",
    "    ('clf', MultinomialNB(\n",
    "        alpha=0.01,\n",
    "        fit_prior=False\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfdac509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/futyn/miniconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.92      0.91      1918\n",
      "         1.0       0.83      0.79      0.81       965\n",
      "\n",
      "    accuracy                           0.87      2883\n",
      "   macro avg       0.86      0.85      0.86      2883\n",
      "weighted avg       0.87      0.87      0.87      2883\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_nb.fit(X_train, y_train)\n",
    "\n",
    "preds_nb = pipeline_nb.predict(X_test)\n",
    "print(classification_report(y_test, preds_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5d435de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 токсичных фичей:\n",
      "  хохлов: 8.4636\n",
      "  дегенерат: 7.9276\n",
      "  пидорашек: 7.6397\n",
      "  даун: 7.6260\n",
      "  хуесос: 7.6253\n",
      "  рашек: 7.4731\n",
      "  пидораха: 7.4195\n",
      "  шизик: 7.3858\n",
      "  жид: 7.3403\n",
      "  лахта: 7.3133\n"
     ]
    }
   ],
   "source": [
    "show_top_features(pipeline_nb, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb041ea",
   "metadata": {},
   "source": [
    "### 6. Лес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36dbdd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Random Forest ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Random Forest ---\")\n",
    "\n",
    "pipeline_rf = Pipeline([\n",
    "    ('vect', TfidfVectorizer(**tfidf_params)),\n",
    "    ('clf', RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        n_estimators=200,\n",
    "        max_depth=35,\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae4994d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/futyn/miniconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.81      0.84      1918\n",
      "         1.0       0.67      0.76      0.71       965\n",
      "\n",
      "    accuracy                           0.79      2883\n",
      "   macro avg       0.77      0.79      0.78      2883\n",
      "weighted avg       0.80      0.79      0.80      2883\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_rf.fit(X_train, y_train)\n",
    "\n",
    "preds_rf = pipeline_rf.predict(X_test)\n",
    "print(classification_report(y_test, preds_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5d15657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 важных фичей:\n",
      "  ты: 0.0490\n",
      "  быть: 0.0168\n",
      "  в: 0.0125\n",
      "  хохол: 0.0119\n",
      "  но: 0.0118\n",
      "  не: 0.0108\n",
      "  год: 0.0098\n",
      "  если: 0.0091\n",
      "  можно: 0.0085\n",
      "  русский: 0.0083\n"
     ]
    }
   ],
   "source": [
    "show_top_features(pipeline_rf, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954d5dfc",
   "metadata": {},
   "source": [
    "Получилось, что дерево и лес деревьев показали худший результат, спотыкаясь о стоп-слова, а логрегу и наивному байесовскому классификатору они как будто особо не мешают. Посмотрим, что будет, если их убрать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10b571ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_params_stop = {\n",
    "    'tokenizer': tokenize_mystem,\n",
    "    'stop_words': russian_stopwords,\n",
    "    'ngram_range': (1, 3),\n",
    "    'min_df': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765468b4",
   "metadata": {},
   "source": [
    "### 7. Логрег (стоп-слова)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d7e6fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Logistic Regression ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Logistic Regression ---\")\n",
    "\n",
    "pipeline_lr = Pipeline([\n",
    "    ('vect', TfidfVectorizer(**tfidf_params_stop)),\n",
    "    ('clf', LogisticRegression(\n",
    "        random_state=42,\n",
    "        solver='liblinear',\n",
    "        class_weight='balanced',\n",
    "        C=5.0\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c03aa189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/futyn/miniconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/futyn/miniconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['весь', 'свой', 'это'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.91      0.90      1918\n",
      "         1.0       0.81      0.79      0.80       965\n",
      "\n",
      "    accuracy                           0.87      2883\n",
      "   macro avg       0.85      0.85      0.85      2883\n",
      "weighted avg       0.87      0.87      0.87      2883\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_lr.fit(X_train, y_train)\n",
    "\n",
    "preds_lr = pipeline_lr.predict(X_test)\n",
    "print(classification_report(y_test, preds_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1123afe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 токсичных фичей:\n",
      "  хохол: 8.2803\n",
      "  дебил: 7.7176\n",
      "  тупой: 7.3355\n",
      "  хохлов: 6.9765\n",
      "  шлюха: 6.4680\n",
      "  быдло: 6.0431\n",
      "  сука: 5.3789\n",
      "  русский: 5.3706\n",
      "  пидор: 5.2628\n",
      "  долбоеб: 5.1829\n"
     ]
    }
   ],
   "source": [
    "show_top_features(pipeline_lr, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3fc07d",
   "metadata": {},
   "source": [
    "### 8. Дерево (стоп-слова)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59b1f09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Decision Tree ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Decision Tree ---\")\n",
    "\n",
    "pipeline_dt = Pipeline([\n",
    "    ('vect', TfidfVectorizer(**tfidf_params_stop)),\n",
    "    ('clf', DecisionTreeClassifier(\n",
    "        random_state=42,\n",
    "        max_depth=25,\n",
    "        min_samples_leaf=5,\n",
    "        class_weight='balanced'\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8472a55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/futyn/miniconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/futyn/miniconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['весь', 'свой', 'это'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.46      0.59      1918\n",
      "         1.0       0.43      0.83      0.57       965\n",
      "\n",
      "    accuracy                           0.58      2883\n",
      "   macro avg       0.64      0.64      0.58      2883\n",
      "weighted avg       0.71      0.58      0.58      2883\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_dt.fit(X_train, y_train)\n",
    "\n",
    "preds_dt = pipeline_dt.predict(X_test)\n",
    "print(classification_report(y_test, preds_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ebc5429e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 важных фичей:\n",
      "  хохол: 0.0711\n",
      "  год: 0.0656\n",
      "  твой: 0.0574\n",
      "  очень: 0.0500\n",
      "  тупой: 0.0484\n",
      "  русский: 0.0464\n",
      "  это: 0.0444\n",
      "  свой: 0.0444\n",
      "  работа: 0.0336\n",
      "  ебать: 0.0333\n"
     ]
    }
   ],
   "source": [
    "show_top_features(pipeline_dt, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264bca85",
   "metadata": {},
   "source": [
    "### 9. Naive Bayes (стоп-слова)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec76a2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Multinomial Naive Bayes ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Multinomial Naive Bayes ---\")\n",
    "\n",
    "pipeline_nb = Pipeline([\n",
    "    ('vect', TfidfVectorizer(**tfidf_params_stop)),\n",
    "    ('clf', MultinomialNB(\n",
    "        alpha=0.01,\n",
    "        fit_prior=False\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e89efe78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/futyn/miniconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/futyn/miniconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['весь', 'свой', 'это'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.91      0.91      1918\n",
      "         1.0       0.82      0.80      0.81       965\n",
      "\n",
      "    accuracy                           0.87      2883\n",
      "   macro avg       0.86      0.86      0.86      2883\n",
      "weighted avg       0.87      0.87      0.87      2883\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_nb.fit(X_train, y_train)\n",
    "\n",
    "preds_nb = pipeline_nb.predict(X_test)\n",
    "print(classification_report(y_test, preds_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dafedb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 токсичных фичей:\n",
      "  хохлов: 8.8064\n",
      "  дегенерат: 8.2056\n",
      "  пидорашек: 7.8849\n",
      "  даун: 7.8005\n",
      "  рашек: 7.7748\n",
      "  хуесос: 7.7710\n",
      "  шизик: 7.7162\n",
      "  пидораха: 7.6865\n",
      "  жид: 7.5449\n",
      "  малолетний: 7.5230\n"
     ]
    }
   ],
   "source": [
    "show_top_features(pipeline_nb, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4f2bfd",
   "metadata": {},
   "source": [
    "### 10. Лес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b437b380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Random Forest ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Random Forest ---\")\n",
    "\n",
    "pipeline_rf = Pipeline([\n",
    "    ('vect', TfidfVectorizer(**tfidf_params_stop)),\n",
    "    ('clf', RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        n_estimators=200,\n",
    "        max_depth=35,\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "42af25af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/futyn/miniconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/futyn/miniconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['весь', 'свой', 'это'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.77      0.82      1918\n",
      "         1.0       0.64      0.80      0.71       965\n",
      "\n",
      "    accuracy                           0.78      2883\n",
      "   macro avg       0.76      0.78      0.76      2883\n",
      "weighted avg       0.80      0.78      0.78      2883\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_rf.fit(X_train, y_train)\n",
    "\n",
    "preds_rf = pipeline_rf.predict(X_test)\n",
    "print(classification_report(y_test, preds_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4609daab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 важных фичей:\n",
      "  хохол: 0.0208\n",
      "  год: 0.0195\n",
      "  твой: 0.0184\n",
      "  очень: 0.0169\n",
      "  тупой: 0.0160\n",
      "  русский: 0.0145\n",
      "  хохлов: 0.0122\n",
      "  свой: 0.0110\n",
      "  ебать: 0.0104\n",
      "  работать: 0.0101\n"
     ]
    }
   ],
   "source": [
    "show_top_features(pipeline_rf, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac0839",
   "metadata": {},
   "source": [
    "Итак, выходит, что логрег и наивный байесовский классификатор смотрят на те слова, на которые действительно нужно смотреть при учёте токсичности. Снижение качества логрега при удалении стоп-слов объясняется важностью слова *ты*, которое, видимо, в этих комментариях постоянно сопровождается оскорблениями в адрес собеседника.\n",
    "\n",
    "Дерево и лес деревьев выводят странные корреляции из данных и перформят сильно хуже, в топ-фичах хоть и есть релевантные слова, но их сильно меньше — много просто частотных слов, и даже удаление стоп-слов проблему не сильно решает, а результат становится только хуже. По-видимому, эти модели плохо работают с важностью слов, здесь вероятностные, конечно, лучше.\n",
    "\n",
    "Ну и ещё надо сказать, что никакая модель не уделяет достаточной важности Н-граммам, в топе их нет — видимо, всё же отдельные слова сильно красноречивее и их уже достаточно."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675b4b95",
   "metadata": {},
   "source": [
    "# Задание 4* (2 балла)\n",
    "\n",
    "Составьте ансамблевый классификатор вручную. Используя один из парных датасетов в `glue` (например, `mnli`) обучите как минимум 10 разных классификаторов (так как количество алгоритмов меньше 10, используйте разные комбинации параметров в векторайзерах и в классификаторах, например, tfidf + logreg и countvectorizer+logreg). Чем сильнее каждый классификатор отличается от другого, тем лучше. \n",
    "\n",
    "Вместо стандартного разбиения на train и test, разбейте выборку на 3 части: train, dev и test. Используйте train для обучения 10 классификаторов. Сделайте предсказания всеми классификаторами для dev и test датасетов. Используя объединенные предсказания всех классификаторов на dev части как признаки, обучите еще один общий классификатор. Сделайте предсказания общим классификатором на test части (опять же используйте предсказания 10 классификаторов как признаки) и рассчитайте качество итоговой классификации. \n",
    "\n",
    "Также отдельно оцените качество на test части для каждого из 10 классификаторов. Сравните с общей оценкой. Превосходит ли общий классификатор в качестве самый лучший из отдельных классификаторов? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f844ed",
   "metadata": {},
   "source": [
    "Сейчас здесь будет много-много ячеек, потому что для каждого из десяти классификаторов нужно написать свой пайплайн, а потом это ещё и соединить. Спасибо ColumnTransformer, который может хэндлить парные тексты, а то бы их было ещё больше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbc0b86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db272d6",
   "metadata": {},
   "source": [
    "### 1. Загрузка данных и подготовка сплитов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c574289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 392702\n",
      "    })\n",
      "    validation_matched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 9815\n",
      "    })\n",
      "    validation_mismatched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 9832\n",
      "    })\n",
      "    test_matched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 9796\n",
      "    })\n",
      "    test_mismatched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 9847\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "nli_dataset = load_dataset('nyu-mll/glue', 'mnli')\n",
    "\n",
    "print(nli_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "503ea0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Возьмем 10000 примеров из 'train' для обучения и 2000 из 'validation' для теста\n",
    "# (иначе мы тут долго будем сидеть)\n",
    "data = nli_dataset['train'].shuffle(seed=42).select(range(10000))\n",
    "test_data = nli_dataset['validation_matched'].shuffle(\n",
    "    seed=42).select(\n",
    "        range(2000))\n",
    "\n",
    "# Конвертируем в pandas для удобства\n",
    "data_df = pd.DataFrame(data)\n",
    "test_df = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "700c5a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделяем 10000 примеров на 8000 (train) и 2000 (dev)\n",
    "train_df, dev_df = train_test_split(\n",
    "    data_df, test_size=0.2, random_state=42, stratify=data_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbd2cf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер train: 8000\n",
      "Размер dev: 2000\n",
      "Размер test: 2000\n"
     ]
    }
   ],
   "source": [
    "X_train = train_df[['premise', 'hypothesis']]\n",
    "y_train = train_df['label']\n",
    "\n",
    "X_dev = dev_df[['premise', 'hypothesis']]\n",
    "y_dev = dev_df['label']\n",
    "\n",
    "X_test = test_df[['premise', 'hypothesis']]\n",
    "y_test = test_df['label']\n",
    "\n",
    "print(f\"Размер train: {len(X_train)}\")\n",
    "print(f\"Размер dev: {len(X_dev)}\")\n",
    "print(f\"Размер test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d47c05c",
   "metadata": {},
   "source": [
    "### 2. Векторизаторы и модельки\n",
    "\n",
    "У нас будет три препроцессора и десять разных пайплайнов, которые будут комбинировать эти препроцессоры и разные классификаторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a942de64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Векторизация 1: TF-IDF (1-2 граммы, 1000 фичей)\n",
    "preprocessor_1 = ColumnTransformer([\n",
    "    ('premise_tfidf', TfidfVectorizer(max_features=1000, ngram_range=(1, 2)), 'premise'),\n",
    "    ('hypothesis_tfidf', TfidfVectorizer(max_features=1000, ngram_range=(1, 2)), 'hypothesis')\n",
    "], remainder='drop')\n",
    "\n",
    "# Векторизация 2: CountVectorizer (только слова, 1000 фичей)\n",
    "preprocessor_2 = ColumnTransformer([\n",
    "    ('premise_count', CountVectorizer(max_features=1000, ngram_range=(1, 1)), 'premise'),\n",
    "    ('hypothesis_count', CountVectorizer(max_features=1000, ngram_range=(1, 1)), 'hypothesis')\n",
    "], remainder='drop')\n",
    "\n",
    "# Векторизация 3: TF-IDF (1-3 граммы, все фичи, другие min/max_df)\n",
    "preprocessor_3 = ColumnTransformer([\n",
    "    ('premise_tfidf_ng', TfidfVectorizer(ngram_range=(1, 3), min_df=5, max_df=0.5), 'premise'),\n",
    "    ('hypothesis_tfidf_ng', TfidfVectorizer(ngram_range=(1, 3), min_df=5, max_df=0.5), 'hypothesis')\n",
    "], remainder='drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bf4585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Наши базовые модели\n",
    "\n",
    "models = {}\n",
    "\n",
    "# Модели на preprocessor_1\n",
    "models['model_1_tfidf_lr'] = Pipeline(\n",
    "    # Логрег\n",
    "    [('prep', preprocessor_1), ('clf', LogisticRegression(C=1.0, random_state=42, max_iter=1000))])\n",
    "models['model_2_tfidf_nb'] = Pipeline(\n",
    "    # Наивный байесовский\n",
    "    [('prep', preprocessor_1), ('clf', MultinomialNB(alpha=1.0))])\n",
    "# Лес\n",
    "models['model_3_tfidf_rf'] = Pipeline([('prep',\n",
    "                                        preprocessor_1),\n",
    "                                       ('clf',\n",
    "                                        RandomForestClassifier(n_estimators=100,\n",
    "                                                               max_depth=10,\n",
    "                                                               random_state=42))])\n",
    "models['model_4_tfidf_lr_weak'] = Pipeline(\n",
    "    # Логрег с другим C\n",
    "    [('prep', preprocessor_1), ('clf', LogisticRegression(C=0.1, random_state=42, max_iter=1000))])\n",
    "\n",
    "\n",
    "# Модели на preprocessor_2\n",
    "models['model_5_count_lr'] = Pipeline(\n",
    "    # Логрег\n",
    "    [('prep', preprocessor_2), ('clf', LogisticRegression(C=1.0, random_state=42, max_iter=1000))])\n",
    "models['model_6_count_nb'] = Pipeline(\n",
    "    # Наивный байесовский\n",
    "    [('prep', preprocessor_2), ('clf', MultinomialNB(alpha=1.0))])\n",
    "# Лес\n",
    "models['model_7_count_rf'] = Pipeline([('prep',\n",
    "                                        preprocessor_2),\n",
    "                                       ('clf',\n",
    "                                        RandomForestClassifier(n_estimators=50,\n",
    "                                                               max_depth=5,\n",
    "                                                               random_state=42))])\n",
    "\n",
    "# Модели на preprocessor_3\n",
    "# Логрег\n",
    "models['model_8_tfidf_ng_lr'] = Pipeline([('prep', preprocessor_3), ('clf', LogisticRegression(\n",
    "    C=5.0, class_weight='balanced', random_state=42, max_iter=1000))])\n",
    "models['model_9_tfidf_ng_nb'] = Pipeline(\n",
    "    # Наивный байесовский\n",
    "    [('prep', preprocessor_3), ('clf', MultinomialNB(alpha=0.1))])\n",
    "# Лес\n",
    "models['model_10_tfidf_ng_rf'] = Pipeline([('prep', preprocessor_3), ('clf', RandomForestClassifier(\n",
    "    n_estimators=200, max_depth=20, class_weight='balanced', random_state=42, n_jobs=-1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88428489",
   "metadata": {},
   "source": [
    "### 3. Обучение базовых моделей и создание фич"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3d6b242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение model_1_tfidf_lr...\n",
      "Обучение model_2_tfidf_nb...\n",
      "Обучение model_3_tfidf_rf...\n",
      "Обучение model_4_tfidf_lr_weak...\n",
      "Обучение model_5_count_lr...\n",
      "Обучение model_6_count_nb...\n",
      "Обучение model_7_count_rf...\n",
      "Обучение model_8_tfidf_ng_lr...\n",
      "Обучение model_9_tfidf_ng_nb...\n",
      "Обучение model_10_tfidf_ng_rf...\n"
     ]
    }
   ],
   "source": [
    "# Списки для хранения фич\n",
    "dev_meta_features = []\n",
    "test_meta_features = []\n",
    "\n",
    "# Обучаем базовые модели и получаем предсказания\n",
    "for name, model in models.items():\n",
    "    print(f\"Обучение {name}...\")\n",
    "\n",
    "    # Обучаем на train\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Получаем вероятности для dev\n",
    "    dev_preds = model.predict_proba(X_dev)\n",
    "    dev_meta_features.append(dev_preds)\n",
    "\n",
    "    # 3. Получаем вероятности для test\n",
    "    test_preds = model.predict_proba(X_test)\n",
    "    test_meta_features.append(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0212db00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "размер матрицы мета-признаков (dev): (2000, 30)\n",
      "Размер матрицы мета-признаков (test): (2000, 30)\n"
     ]
    }
   ],
   "source": [
    "# Собираем фичи в единую матрицу\n",
    "\n",
    "X_dev_meta = np.hstack(dev_meta_features)\n",
    "X_test_meta = np.hstack(test_meta_features)\n",
    "\n",
    "print(f\"размер матрицы мета-признаков (dev): {X_dev_meta.shape}\")\n",
    "print(f\"Размер матрицы мета-признаков (test): {X_test_meta.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ac3b72",
   "metadata": {},
   "source": [
    "### 4. Обучаем итоговый классификатор на полученных фичах\n",
    "\n",
    "Будем использовать логрег, потому что он хорошо подходит для взвешивания предсказаний (как вероятностная модель)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3ead0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Обучение классификатора ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=2000, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=2000, random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=2000, random_state=42)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_classifier = LogisticRegression(random_state=42, max_iter=2000)\n",
    "\n",
    "print(\"--- Обучение классификатора ---\")\n",
    "meta_classifier.fit(X_dev_meta, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3269121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Финальная оценка\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.47      0.46       695\n",
      "           1       0.43      0.43      0.43       650\n",
      "           2       0.50      0.47      0.48       655\n",
      "\n",
      "    accuracy                           0.46      2000\n",
      "   macro avg       0.46      0.46      0.46      2000\n",
      "weighted avg       0.46      0.46      0.46      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Финальная оценка\")\n",
    "\n",
    "final_preds = meta_classifier.predict(X_test_meta)\n",
    "print(classification_report(y_test, final_preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f078fa9",
   "metadata": {},
   "source": [
    "### 5. Смотрим на метрики отдельных классификаторов и сравниваем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bca5280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                model  accuracy  precision (w_avg)  recall (w_avg)  f1 (w_avg)\n",
      "model_4_tfidf_lr_weak    0.4655           0.469361          0.4655    0.465878\n",
      " model_10_tfidf_ng_rf    0.4585           0.471544          0.4585    0.458784\n",
      "      META_CLASSIFIER    0.4585           0.459306          0.4585    0.458700\n",
      "     model_7_count_rf    0.4575           0.472169          0.4575    0.457485\n",
      "     model_3_tfidf_rf    0.4480           0.463160          0.4480    0.447723\n",
      "     model_1_tfidf_lr    0.4460           0.446548          0.4460    0.446193\n",
      "     model_2_tfidf_nb    0.4435           0.444784          0.4435    0.443608\n",
      "     model_5_count_lr    0.4415           0.441914          0.4415    0.441658\n",
      "  model_8_tfidf_ng_lr    0.4385           0.440049          0.4385    0.439058\n",
      "     model_6_count_nb    0.4175           0.418963          0.4175    0.416867\n",
      "  model_9_tfidf_ng_nb    0.4155           0.416717          0.4155    0.415576\n"
     ]
    }
   ],
   "source": [
    "# Список для хранения всех метрик\n",
    "all_metrics = []\n",
    "\n",
    "# Оценка базовых моделей\n",
    "for name, model in models.items():\n",
    "    base_preds = model.predict(X_test)\n",
    "\n",
    "    report = classification_report(\n",
    "        y_test,\n",
    "        base_preds,\n",
    "        output_dict=True,\n",
    "        zero_division=0)\n",
    "\n",
    "    all_metrics.append({\n",
    "        'model': name,\n",
    "        'accuracy': report['accuracy'],\n",
    "        'precision (w_avg)': report['weighted avg']['precision'],\n",
    "        'recall (w_avg)': report['weighted avg']['recall'],\n",
    "        'f1 (w_avg)': report['weighted avg']['f1-score']\n",
    "    })\n",
    "\n",
    "# Оценка мета-модели\n",
    "\n",
    "final_preds = meta_classifier.predict(X_test_meta)\n",
    "report_meta = classification_report(\n",
    "    y_test,\n",
    "    final_preds,\n",
    "    output_dict=True,\n",
    "    zero_division=0)\n",
    "\n",
    "all_metrics.append({\n",
    "    'model': 'META_CLASSIFIER',\n",
    "    'accuracy': report_meta['accuracy'],\n",
    "    'precision (w_avg)': report_meta['weighted avg']['precision'],\n",
    "    'recall (w_avg)': report_meta['weighted avg']['recall'],\n",
    "    'f1 (w_avg)': report_meta['weighted avg']['f1-score']\n",
    "})\n",
    "\n",
    "# Делаем красиво\n",
    "metrics_df = pd.DataFrame(all_metrics)\n",
    "metrics_df = metrics_df.sort_values(by='f1 (w_avg)', ascending=False)\n",
    "\n",
    "print(metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99388ba1",
   "metadata": {},
   "source": [
    "Видим, что наш обученный на признаках классификатор не перформит лучше отдельных моделей - ансамблирование здесь не сработало. С другой стороны, перформит он не сильно хуже моделей-лидеров, хотя все они перформят плохо.\n",
    "\n",
    "Во-первых, надо заметить, что наш общий классификатор учился на 2000 датапойнтах, тогда как отдельные классификаторы — на 8000. Наверное, учитывая это, результат не настолько плохой: мы не получили модельку, которая перформит лучше, но она легче и быстрее и требует для предсказания вектор размером 30. Возможно, если расширить валидационный сет, может быть лучше.\n",
    "\n",
    "Во-вторых, задача NLI всё же сложная для BoW-моделей, да и сами эти модели не сильно различаются между собой. Да, разные параметры, разные векторы, но на практике всё тот же BoW и ничего особенно нового мы не получили.\n",
    "\n",
    "Поэтому техника, конечно, интересная, но требует разных и более различающихся моделей: например, ансамблирования BoW и какого-нибудь W2V в разных комбинациях. А так — garbage in, garbage out."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
