{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a98604cb",
   "metadata": {},
   "source": [
    "Домашку будет легче делать в колабе (убедитесь, что у вас runtype с gpu)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c422aa0",
   "metadata": {},
   "source": [
    "# Задание 1 (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a72790",
   "metadata": {},
   "source": [
    "Обучите word2vec модели с негативным семплированием (cbow и skip-gram) аналогично тому, как это было сделано в семинаре. Вам нужно изменить следующие пункты: \n",
    "1) добавьте лемматизацию в предобработку (любым способом)  \n",
    "2) измените размер окна в большую или меньшую сторону\n",
    "3) измените размерность итоговых векторов\n",
    "\n",
    "Выберете несколько не похожих по смыслу слов (не таких как в семинаре), и протестируйте полученные эмбединги (найдите ближайшие слова и оцените качество, как в семинаре). \n",
    "Постарайтесь обучать модели как можно дольше и на как можно большем количестве данных. (Но если у вас мало времени или ресурсов, то допустимо взять поменьше данных и поставить меньше эпох)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "645c194e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymystem3\n",
      "  Downloading pymystem3-0.2.0-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from pymystem3) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->pymystem3) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->pymystem3) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->pymystem3) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->pymystem3) (2025.11.12)\n",
      "Downloading pymystem3-0.2.0-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: pymystem3\n",
      "Successfully installed pymystem3-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cde5fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from collections import Counter\n",
    "from pymystem3 import Mystem\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f71d7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd19bc4d",
   "metadata": {},
   "source": [
    "## 1. Загружаем данные и готовим словарь с леммами\n",
    "\n",
    "В качестве лемматизатора мы будем использовать Mystem, потому что он работает достаточно хорошо и быстро. В него уже вшит хороший токенизатор, поэтому весь препроцессинг мы отдадим ему — регулярки нам больше не нужны."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82a02f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-27 15:05:37--  https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/notebooks/word_embeddings/wiki_data.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 68582461 (65M) [text/plain]\n",
      "Saving to: ‘wiki_data.txt’\n",
      "\n",
      "wiki_data.txt       100%[===================>]  65.41M   217MB/s    in 0.3s    \n",
      "\n",
      "2025-12-27 15:05:38 (217 MB/s) - ‘wiki_data.txt’ saved [68582461/68582461]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/notebooks/word_embeddings/wiki_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8f3a518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Препроцессинг полностью ложиться на Mystem\n",
    "m = Mystem()\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.replace('#', ' ')\n",
    "    lemmas = m.lemmatize(text)\n",
    "    tokens = [token.strip() for token in lemmas if token.strip().isalnum()]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5df3975",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = open('wiki_data.txt', encoding='utf8').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f92535e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используется текстов: 20003\n"
     ]
    }
   ],
   "source": [
    "# Параметр для управления размером данных\n",
    "NUM_TEXTS = len(wiki)\n",
    "wiki = wiki[:NUM_TEXTS]\n",
    "print(f\"Используется текстов: {len(wiki)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "107fe47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6376aa8809ce45b58ad063de37330877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего уникальных слов: 184474\n"
     ]
    }
   ],
   "source": [
    "vocab = Counter()\n",
    "\n",
    "for text in tqdm(wiki):\n",
    "    vocab.update(preprocess(text))\n",
    "\n",
    "print(f\"Всего уникальных слов: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42dd12e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Слов после фильтрации: 12618\n"
     ]
    }
   ],
   "source": [
    "# Фильтруем редкие слова\n",
    "\n",
    "freq = 30\n",
    "filtered_vocab = set()\n",
    "\n",
    "for word in vocab:\n",
    "    if vocab[word] > freq:\n",
    "        filtered_vocab.add(word)\n",
    "\n",
    "print(f\"Слов после фильтрации: {len(filtered_vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "567b6133",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {'PAD': 0}\n",
    "\n",
    "for word in filtered_vocab:\n",
    "    word2id[word] = len(word2id)\n",
    "\n",
    "id2word = {i: word for word, i in word2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "194e14a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33cb603ea16b4233a5cf3fdeed356864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего предложений: 19813\n"
     ]
    }
   ],
   "source": [
    "# Собираем предложения\n",
    "sentences = []\n",
    "\n",
    "for text in tqdm(wiki):\n",
    "    tokens = preprocess(text)\n",
    "    if not tokens:\n",
    "        continue\n",
    "    ids = [word2id[token] for token in tokens if token in word2id]\n",
    "    if ids:\n",
    "        sentences.append(ids)\n",
    "\n",
    "print(f\"Всего предложений: {len(sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aeff39a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для паддинга\n",
    "\n",
    "def pad_sequences(sequences, maxlen, padding='post', value=0):\n",
    "    res = np.full((len(sequences), maxlen), value, dtype='int64')\n",
    "    for i, seq in enumerate(sequences):\n",
    "        if not seq:\n",
    "            continue\n",
    "        if len(seq) >= maxlen:\n",
    "            if padding == 'post':\n",
    "                res[i] = np.array(seq[:maxlen])\n",
    "            else:\n",
    "                res[i] = np.array(seq[-maxlen:])\n",
    "        else:\n",
    "            if padding == 'post':\n",
    "                res[i, :len(seq)] = np.array(seq)\n",
    "            else:\n",
    "                res[i, -len(seq):] = np.array(seq)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d69e0a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря: 12619\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(id2word)\n",
    "print(f\"Размер словаря: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f030732",
   "metadata": {},
   "source": [
    "## 2. Модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6f28fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер окна: 8\n",
      "Размерность эмбеддингов: 300\n",
      "Количество эпох: 20\n"
     ]
    }
   ],
   "source": [
    "# Параметры моделей\n",
    "WINDOW_SIZE = 8\n",
    "EMB_DIM = 300\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "print(f\"Размер окна: {WINDOW_SIZE}\")\n",
    "print(f\"Размерность эмбеддингов: {EMB_DIM}\")\n",
    "print(f\"Количество эпох: {NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82c835af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация датасета для skipgram\n",
    "\n",
    "def gen_batches_skipgram(sentences, window=5, batch_size=1000):\n",
    "    while True:\n",
    "        X_target = []\n",
    "        X_context = []\n",
    "        y = []\n",
    "\n",
    "        for sent in sentences:\n",
    "            for i in range(len(sent)):\n",
    "                word = sent[i]\n",
    "                context = sent[max(0, i - window):i] + \\\n",
    "                    sent[i + 1:min(len(sent), i + window + 1)]\n",
    "\n",
    "                for context_word in context:\n",
    "                    X_target.append(word)\n",
    "                    X_context.append(context_word)\n",
    "                    y.append(1)\n",
    "\n",
    "                    X_target.append(word)\n",
    "                    X_context.append(np.random.randint(vocab_size))\n",
    "                    y.append(0)\n",
    "\n",
    "                    if len(X_target) >= batch_size:\n",
    "                        X_target_arr = np.array(X_target, dtype='int64')\n",
    "                        X_context_arr = np.array(X_context, dtype='int64')\n",
    "                        y_arr = np.array(y, dtype='float32')\n",
    "                        yield (X_target_arr, X_context_arr), y_arr\n",
    "                        X_target, X_context, y = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b88b2ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для CBOW\n",
    "\n",
    "def gen_batches_cbow(sentences, window=5, batch_size=1000):\n",
    "    while True:\n",
    "        X_target = []\n",
    "        X_context = []\n",
    "        y = []\n",
    "\n",
    "        for sent in sentences:\n",
    "            for i in range(len(sent)):\n",
    "                word = sent[i]\n",
    "                context = sent[max(0, i - window):i] + \\\n",
    "                    sent[i + 1:min(len(sent), i + window + 1)]\n",
    "\n",
    "                if context:\n",
    "                    X_target.append(word)\n",
    "                    X_context.append(context)\n",
    "                    y.append(1)\n",
    "\n",
    "                    X_target.append(np.random.randint(vocab_size))\n",
    "                    X_context.append(context)\n",
    "                    y.append(0)\n",
    "\n",
    "                    if len(X_target) >= batch_size:\n",
    "                        X_target_arr = np.array(X_target, dtype='int64')\n",
    "                        X_context_arr = pad_sequences(\n",
    "                            X_context, maxlen=window * 2, padding='post', value=0)\n",
    "                        y_arr = np.array(y, dtype='float32')\n",
    "                        yield (X_target_arr, X_context_arr), y_arr\n",
    "                        X_target, X_context, y = [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dba874",
   "metadata": {},
   "source": [
    "### SkipGram с negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66b42d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramNegSampling(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.target_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.context_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "\n",
    "    def forward(self, target_ids, context_ids):\n",
    "        t = self.target_emb(target_ids)\n",
    "        c = self.context_emb(context_ids)\n",
    "        dot = (t * c).sum(dim=1)\n",
    "        return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0a5a4b",
   "metadata": {},
   "source": [
    "### CBOW с negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b789082",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWNegSampling(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, window_size):\n",
    "        super().__init__()\n",
    "        self.target_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.context_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def forward(self, target_ids, context_ids):\n",
    "        t = self.target_emb(target_ids)\n",
    "        c = self.context_emb(context_ids)\n",
    "        c_sum = c.sum(dim=1)\n",
    "        dot = (t * c_sum).sum(dim=1)\n",
    "        return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69141ee7",
   "metadata": {},
   "source": [
    "### Нахождение похожих слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08f9dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "\n",
    "def most_similar(word, embeddings, top_n=10):\n",
    "    if word not in word2id:\n",
    "        return f\"Слово '{word}' не найдено в словаре\"\n",
    "\n",
    "    similar = [id2word[i] for i in cosine_distances(\n",
    "        embeddings[word2id[word]].reshape(1, -1), embeddings).argsort()[0][:top_n]]\n",
    "    return similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ddbe0d",
   "metadata": {},
   "source": [
    "## 3. Обучаем SkipGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcafd81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce98556e78d44349ab6285044283206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Эпохи:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6aee3dcc7d74d8f8eaa1ef805745b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - train loss: 5.4505, val loss: 4.9698\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63de625de75e4a98b419ef7d77cd0140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - train loss: 3.9419, val loss: 4.1126\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a677447f7740c4838f498c4249dfa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - train loss: 3.0690, val loss: 2.5660\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5feecc7a5a734a22ba5bda218dfacd18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - train loss: 2.4283, val loss: 2.6706\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb3d75c4e6a49749569a32b2c8c2f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - train loss: 2.1237, val loss: 1.7386\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56721a36b5194dab8a22dcdfe1ad211c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - train loss: 1.9011, val loss: 1.6136\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d6f6743a37493890708ea9990d3f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - train loss: 1.6461, val loss: 1.8475\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154484c79bf84e7a8b7ba106f9468eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - train loss: 1.5150, val loss: 1.4822\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e509c3996204b2aaa6299612ff756c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - train loss: 1.4486, val loss: 1.2219\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b76e0ba5a44b2db47004666775c564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - train loss: 1.3644, val loss: 1.3133\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe85006de4c4130862efc30c5056614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - train loss: 1.0895, val loss: 1.1805\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "355f324072a745f4b04480b1bef504df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - train loss: 1.1280, val loss: 1.2299\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afcf880f8fdb473e8bb4fdd9e4d98b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - train loss: 1.0855, val loss: 0.9918\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a28215eef784fdcab85b1c418f0ee26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - train loss: 1.0266, val loss: 1.1208\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd043797822404d8aa6544f1c5a012d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - train loss: 0.9767, val loss: 1.0675\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a20d4f18123410d939ac8a455a20fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - train loss: 0.9112, val loss: 0.8662\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9382608f62b489a8586ee1aa71e7638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - train loss: 0.8777, val loss: 0.7777\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ee4913e1ce4f9cb4b83c74bd4152a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - train loss: 0.8577, val loss: 0.9616\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f36616f02c841c091acbeec02db4429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - train loss: 0.7897, val loss: 0.9749\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "337dd3ce483244c8b4de83fa6f920cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - train loss: 0.7762, val loss: 0.6533\n"
     ]
    }
   ],
   "source": [
    "model_sg = SkipGramNegSampling(vocab_size, EMB_DIM).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model_sg.parameters(), lr=1e-3)\n",
    "\n",
    "train_gen = gen_batches_skipgram(\n",
    "    sentences[:int(0.95 * len(sentences))], window=WINDOW_SIZE, batch_size=1000)\n",
    "valid_gen = gen_batches_skipgram(\n",
    "    sentences[int(0.95 * len(sentences)):], window=WINDOW_SIZE, batch_size=1000)\n",
    "\n",
    "steps_per_epoch = 5000\n",
    "validation_steps = 30\n",
    "\n",
    "for epoch in tqdm(range(NUM_EPOCHS), desc=\"Эпохи\"):\n",
    "    model_sg.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for step in tqdm(range(steps_per_epoch), desc=\"Шаги\", leave=False):\n",
    "        (X_t, X_c), y = next(train_gen)\n",
    "        X_t = torch.LongTensor(X_t).to(device)\n",
    "        X_c = torch.LongTensor(X_c).to(device)\n",
    "        y_t = torch.FloatTensor(y).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model_sg(X_t, X_c)\n",
    "        loss = criterion(logits, y_t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= steps_per_epoch\n",
    "\n",
    "    model_sg.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for _ in range(validation_steps):\n",
    "            (X_t, X_c), y = next(valid_gen)\n",
    "            X_t = torch.LongTensor(X_t).to(device)\n",
    "            X_c = torch.LongTensor(X_c).to(device)\n",
    "            y_t = torch.FloatTensor(y).to(device)\n",
    "\n",
    "            logits = model_sg(X_t, X_c)\n",
    "            loss = criterion(logits, y_t)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= validation_steps\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {\n",
    "            epoch + 1}/{NUM_EPOCHS} - train loss: {\n",
    "            epoch_loss:.4f}, val loss: {\n",
    "                val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c356b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_sg = model_sg.context_emb.weight.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02684c6a",
   "metadata": {},
   "source": [
    "## 4. Обучаем CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05f35ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b19ecd27b9741bea8d249e4d8f9d513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Эпохи:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1bf79ced6f45babbf57050fee41401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - train loss: 17.7555, val loss: 13.4106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fbab0b583c74f1d8a3e46ed28d96b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - train loss: 11.6826, val loss: 10.7289\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09026c7d3c247428a14b5455b58065d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - train loss: 8.8686, val loss: 9.0054\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f20c9d800eb44832adbe0f82a3247d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - train loss: 7.0722, val loss: 7.4302\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc0719d5699422383fd98c3fc4fd806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - train loss: 5.6345, val loss: 7.1905\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a3e251e91e4bcaa4cdd82c6d160f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - train loss: 4.4743, val loss: 6.7340\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358bb461d6844d7fa2081bfe786ce9cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - train loss: 3.7647, val loss: 6.4249\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e7117a91024faab7714c91e33bb903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - train loss: 3.0605, val loss: 5.2440\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "988e4f8752fb4625888f36de8b6a587b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - train loss: 2.5720, val loss: 5.0170\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bed27b6f39746bba617c1e7f2993156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - train loss: 2.1800, val loss: 5.1925\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd7b3dbf8a44e87b97971c308e41444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - train loss: 1.8223, val loss: 3.6105\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec886f578c1424286cd13a138a51cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - train loss: 1.6149, val loss: 4.1311\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b24d4a5bb2634cbaa35ce803e6468ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - train loss: 1.3542, val loss: 4.1080\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c6080575374a3ea626d4306b2afd64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - train loss: 1.1970, val loss: 4.5963\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea2690d4f5241dbabc9df6845a4ebb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - train loss: 1.0574, val loss: 3.9719\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af2c4673c374865bf4c92d159f1a778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - train loss: 0.9211, val loss: 3.9929\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d272d7f066435f8d5c98192c9fe822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - train loss: 0.8472, val loss: 3.9575\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af08473b5ef24b4abdb84ac65f79de6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - train loss: 0.7449, val loss: 3.6001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533606a1a51f4adaaa45c11f100c1447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - train loss: 0.6890, val loss: 3.4820\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c398c642b3c4e1b84908e87189c81a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - train loss: 0.6160, val loss: 3.6627\n"
     ]
    }
   ],
   "source": [
    "model_cbow = CBOWNegSampling(vocab_size, EMB_DIM, WINDOW_SIZE * 2).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model_cbow.parameters(), lr=1e-3)\n",
    "\n",
    "train_gen = gen_batches_cbow(\n",
    "    sentences[:int(0.95 * len(sentences))], window=WINDOW_SIZE, batch_size=1000)\n",
    "valid_gen = gen_batches_cbow(\n",
    "    sentences[int(0.95 * len(sentences)):], window=WINDOW_SIZE, batch_size=1000)\n",
    "\n",
    "for epoch in tqdm(range(NUM_EPOCHS), desc=\"Эпохи\"):\n",
    "    model_cbow.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for step in tqdm(range(steps_per_epoch), desc=\"Шаги\", leave=False):\n",
    "        (X_t, X_c), y = next(train_gen)\n",
    "        X_t = torch.LongTensor(X_t).to(device)\n",
    "        X_c = torch.LongTensor(X_c).to(device)\n",
    "        y_t = torch.FloatTensor(y).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model_cbow(X_t, X_c)\n",
    "        loss = criterion(logits, y_t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= steps_per_epoch\n",
    "\n",
    "    model_cbow.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for _ in range(validation_steps):\n",
    "            (X_t, X_c), y = next(valid_gen)\n",
    "            X_t = torch.LongTensor(X_t).to(device)\n",
    "            X_c = torch.LongTensor(X_c).to(device)\n",
    "            y_t = torch.FloatTensor(y).to(device)\n",
    "\n",
    "            logits = model_cbow(X_t, X_c)\n",
    "            loss = criterion(logits, y_t)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= validation_steps\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {\n",
    "            epoch + 1}/{NUM_EPOCHS} - train loss: {\n",
    "            epoch_loss:.4f}, val loss: {\n",
    "                val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04671904",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_cbow = model_cbow.context_emb.weight.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a9b060",
   "metadata": {},
   "source": [
    "# 5. Тестим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8554349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "компьютер: ['компьютер', 'ведьма', 'Sun', 'глинистый', 'воронеж', 'воспринимать', 'уния', 'инновационный', 'лесистый', 'джейми']\n",
      "музыка: ['музыка', 'она', 'для', 'группа', 'это', 'называть', 'через', 'создавать', 'с', 'иметь']\n",
      "война: ['война', 'во', 'к', 'с', 'из', 'но', 'он', 'время', 'быть', 'и']\n",
      "смерть: ['смерть', 'но', 'за', 'со', 'он', 'не', 'как', 'а', 'из', 'время']\n",
      "язык: ['язык', 'с', 'к', 'в', 'который', 'на', 'из', 'не', 'по', 'и']\n",
      "лингвистика: ['лингвистика', 'райтман', 'шоколад', 'перевес', 'брилев', 'итого', 'And', 'Hurts', 'Fanuc', 'висконта']\n",
      "самолет: ['самолет', 'вид', 'главный', 'посвящать', 'боевой', 'оставаться', '18', 'белый', 'гора', 'книга']\n",
      "торт: ['торт', 'радовицкий', 'ретель', 'осборн', 'пири', 'мегатрон', 'алоис', 'Boys', 'Jordan', 'регби']\n"
     ]
    }
   ],
   "source": [
    "# SkipGram\n",
    "\n",
    "test_words = [\n",
    "    'компьютер',\n",
    "    'музыка',\n",
    "    'война',\n",
    "    'смерть',\n",
    "    'язык',\n",
    "    'лингвистика',\n",
    "    'самолет',\n",
    "    'торт']\n",
    "\n",
    "for word in test_words:\n",
    "    print(f\"{word}: {most_similar(word, embeddings_sg)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd836036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "компьютер: ['компьютер', 'набор', 'кельтский', 'карате', 'интерфейс', '2000', 'регулировать', 'замена', 'сервер', 'оптический']\n",
      "музыка: ['музыка', 'утренний', 'певец', 'и', '1882', 'неизменный', 'калибр', 'трезвучие', 'звучание', 'театр']\n",
      "война: ['война', 'генрих', 'месть', 'And', '1944', 'варзи', 'полк', 'пасть', 'кампания', 'конфликт']\n",
      "смерть: ['смерть', 'умирать', 'сын', 'житие', 'президент', 'назик', 'сюжетный', 'граф', '1856', 'брак']\n",
      "язык: ['язык', 'способность', 'ходовой', 'достижение', 'новелла', 'книга', 'государство', 'село', 'поэт', 'печать']\n",
      "лингвистика: ['лингвистика', 'коннектикут', 'овраг', 'язык', 'какой', 'нация', 'исток', 'кристаллический', 'мур', 'исследование']\n",
      "самолет: ['самолет', 'катер', '60', 'максимальный', 'автомобиль', 'экипаж', 'отсек', 'ракета', 'австралия', 'снабжение']\n",
      "торт: ['торт', 'ординарный', 'принципиально', 'служанка', 'струнный', 'качественный', 'переписывать', 'болезнь', 'справедливый', 'интеллект']\n"
     ]
    }
   ],
   "source": [
    "# CBOW\n",
    "\n",
    "for word in test_words:\n",
    "    print(f\"{word}: {most_similar(word, embeddings_cbow)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64d104d",
   "metadata": {},
   "source": [
    "Получается так себе. Кажется, надо удалять стоп-слова, по крайней мере для SkipGramm. CBOW с определением похожих слов справляется ощутимо лучше."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f8af07",
   "metadata": {},
   "source": [
    "А если попробовать взять эмбеддинги из других матриц?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7aad4add",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_sg = model_sg.target_emb.weight.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e6a2330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "компьютер: ['компьютер', 'загораться', 'краков', 'передача', 'линкор', 'разрабатывать', 'оценивать', 'показ', 'данные', 'кран']\n",
      "музыка: ['музыка', 'написать', 'известный', 'автор', 'фильм', 'песня', 'джон', 'основывать', 'роль', 'искусство']\n",
      "война: ['война', 'после', 'во', 'военный', 'быть', 'войско', 'год', 'г', 'армия', 'в']\n",
      "смерть: ['смерть', 'сын', 'свой', 'отец', 'его', 'он', 'тот', 'она', 'что', 'не']\n",
      "язык: ['язык', 'являться', 'другой', 'многий', 'как', 'существовать', 'некоторый', 'автор', 'их', 'слово']\n",
      "лингвистика: ['лингвистика', 'дисплей', 'пожилой', 'эпидермис', 'коста', 'водяной', '157', 'покупка', 'сигнал', 'совать']\n",
      "самолет: ['самолет', '27', 'брянский', 'пункт', 'дивизия', 'японский', 'финский', 'ильич', 'го', 'щит']\n",
      "торт: ['торт', 'упрощенный', 'руины', 'вылетать', 'Alaska', 'понижать', 'вот', 'обстреливать', 'завершение', 'зрительный']\n"
     ]
    }
   ],
   "source": [
    "for word in test_words:\n",
    "    print(f\"{word}: {most_similar(word, embeddings_sg)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7b1fc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_cbow = model_cbow.target_emb.weight.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3b82e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "компьютер: ['компьютер', 'Microsoft', 'оперетта', 'плазма', 'памир', 'AT', 'дельта', 'лабораторный', 'выстрел', 'ильич']\n",
      "музыка: ['музыка', 'дядя', 'инструментальный', 'аквариум', 'игрушка', 'драма', 'автор', 'оркестр', 'творчество', 'иудей']\n",
      "война: ['война', 'время', 'взятие', 'ветеран', 'изменять', 'совместно', 'действие', 'ответ', 'восстановление', 'приготовление']\n",
      "смерть: ['смерть', '1170', 'канонический', 'эмилий', 'гостиница', 'муж', 'уходить', 'называться', 'убийство', 'убежденный']\n",
      "язык: ['язык', 'математический', 'традиция', 'прошедший', 'строка', 'термин', 'конкурент', 'инвестор', 'диалект', 'население']\n",
      "лингвистика: ['лингвистика', 'психология', 'инсбрук', 'стимулировать', 'вепсский', 'малышка', 'факультет', 'монография', 'акмолинский', 'стремительно']\n",
      "самолет: ['самолет', 'истребитель', 'аэродром', 'лодка', 'лайнер', 'авиабаза', 'General', 'заказ', 'производить', 'киль']\n",
      "торт: ['торт', 'масляный', 'юнайтед', 'биохимия', 'охлаждать', 'структурный', 'коростенское', 'грамматика', 'чудовище', 'узбекский']\n"
     ]
    }
   ],
   "source": [
    "for word in test_words:\n",
    "    print(f\"{word}: {most_similar(word, embeddings_cbow)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1ca349",
   "metadata": {},
   "source": [
    "Я подумал, что для SkipGramm (как минимум) логичнее ожидать правильные эмбеддинги в матрице с таргетным словом, а не контекстом — и да, стало получше, хоть и всё равно не очень. Что интересно — изменение матрицы, откуда мы брали эмбеддинги, для CBOW тоже как будто улучшило результат (и он всё ещё лучше SkipGramm'а). Будем считать, что здесь нужна ещё какая-то умная оптимизация. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b61b7c",
   "metadata": {},
   "source": [
    "# Задание 2 (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eff080",
   "metadata": {},
   "source": [
    "Обучите 1 word2vec и 1 fastext модель в gensim. В каждой из модели нужно задать все параметры, которые мы разбирали на семинаре. Заданные значения должны отличаться от дефолтных и от тех, что мы использовали на семинаре."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "986c2018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f796aea4",
   "metadata": {},
   "source": [
    "## 1. Готовим тексты\n",
    "\n",
    "Те же самые тексты, токенизируем и лемматизируем так же, как и для первого задания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3a73937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6210b7ec4b7f4eaca3d4bdfa8acbaeb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "texts = [preprocess(text) for text in tqdm(wiki)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1eaf3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество текстов для обучения: 20003\n",
      "Пример первого текста: ['новостройка', 'нижегородский', 'область', 'новостройка', 'сельский', 'поселок', 'в', 'дивеевский', 'район', 'нижегородский']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Количество текстов для обучения: {len(texts)}\")\n",
    "print(f\"Пример первого текста: {texts[0][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82acf79",
   "metadata": {},
   "source": [
    "## 2. Обучаем обычную W2V-модель (CBOW)\n",
    "\n",
    "(потому что он как будто получше похожие слова ищет)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5035bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.Word2Vec(\n",
    "    texts,\n",
    "    vector_size=500,\n",
    "    window=8,\n",
    "    min_count=20,\n",
    "    max_vocab_size=20000,\n",
    "    sg=0,\n",
    "    negative=20,\n",
    "    epochs=20,\n",
    "    workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "773d132e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря Word2Vec: 7321\n"
     ]
    }
   ],
   "source": [
    "print(f\"Размер словаря Word2Vec: {len(w2v.wv)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e96e91",
   "metadata": {},
   "source": [
    "## 3. Обучаем Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08bc482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = gensim.models.FastText(\n",
    "    texts,\n",
    "    vector_size=500,\n",
    "    window=8,\n",
    "    min_count=20,\n",
    "    # мне лень гуглить, включены ли сюда символьные N-граммы, поэтому пусть\n",
    "    # будет побольше\n",
    "    max_vocab_size=50000,\n",
    "    sg=0,\n",
    "    negative=20,\n",
    "    min_n=2,\n",
    "    max_n=5,\n",
    "    epochs=20,\n",
    "    workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "568ce177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря FastText: 16404\n"
     ]
    }
   ],
   "source": [
    "print(f\"Размер словаря FastText: {len(ft.wv)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54975c5",
   "metadata": {},
   "source": [
    "## 4. Тестим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b932e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "компьютер:\n",
      "  USB: 0.6055\n",
      "  процессор: 0.6025\n",
      "  совместимый: 0.5925\n",
      "  клавиатура: 0.5782\n",
      "  PC: 0.5723\n",
      "  интерфейс: 0.5701\n",
      "  разъем: 0.5579\n",
      "  встроенный: 0.5558\n",
      "  битный: 0.5384\n",
      "  Apple: 0.5334\n",
      "\n",
      "музыка:\n",
      "  аранжировка: 0.5526\n",
      "  пение: 0.5434\n",
      "  композитор: 0.5218\n",
      "  музыкальный: 0.5145\n",
      "  хоровой: 0.5114\n",
      "  инструментальный: 0.5059\n",
      "  мелодия: 0.5037\n",
      "  джазовый: 0.5033\n",
      "  вокальный: 0.4888\n",
      "  репертуар: 0.4813\n",
      "\n",
      "война:\n",
      "  вторжение: 0.4432\n",
      "  кампания: 0.4282\n",
      "  оккупация: 0.4155\n",
      "  воевать: 0.3907\n",
      "  поход: 0.3807\n",
      "  восстание: 0.3660\n",
      "  сражение: 0.3432\n",
      "  бомбардировка: 0.3402\n",
      "  осада: 0.3386\n",
      "  революция: 0.3338\n",
      "\n",
      "смерть:\n",
      "  гибель: 0.5423\n",
      "  отъезд: 0.4668\n",
      "  племянник: 0.3974\n",
      "  умирать: 0.3947\n",
      "  внук: 0.3912\n",
      "  сын: 0.3889\n",
      "  уход: 0.3860\n",
      "  самоубийство: 0.3846\n",
      "  графиня: 0.3809\n",
      "  похороны: 0.3776\n",
      "\n",
      "язык:\n",
      "  диалект: 0.5092\n",
      "  перевод: 0.4156\n",
      "  словарь: 0.4126\n",
      "  языковой: 0.4093\n",
      "  поэзия: 0.4067\n",
      "  литература: 0.3966\n",
      "  славянский: 0.3719\n",
      "  фольклор: 0.3471\n",
      "  википедия: 0.3434\n",
      "  текст: 0.3409\n",
      "\n",
      "лингвистика: слово не найдено в словаре\n",
      "самолет:\n",
      "  истребитель: 0.6464\n",
      "  бомбардировщик: 0.6176\n",
      "  вертолет: 0.6121\n",
      "  P2V: 0.5470\n",
      "  ракета: 0.5172\n",
      "  ил: 0.5161\n",
      "  аэродром: 0.5108\n",
      "  Fw: 0.5050\n",
      "  посадка: 0.5004\n",
      "  реактивный: 0.4962\n",
      "\n",
      "торт:\n",
      "  пирог: 0.4916\n",
      "  тесто: 0.4816\n",
      "  сыр: 0.4430\n",
      "  блюдо: 0.4175\n",
      "  фрукт: 0.4067\n",
      "  кухня: 0.4007\n",
      "  кусок: 0.3942\n",
      "  напиток: 0.3859\n",
      "  платье: 0.3733\n",
      "  сахар: 0.3551\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# W2V\n",
    "\n",
    "test_words = [\n",
    "    'компьютер',\n",
    "    'музыка',\n",
    "    'война',\n",
    "    'смерть',\n",
    "    'язык',\n",
    "    'лингвистика',\n",
    "    'самолет',\n",
    "    'торт']\n",
    "\n",
    "for word in test_words:\n",
    "    try:\n",
    "        similar = w2v.wv.most_similar(word, topn=10)\n",
    "        print(f\"{word}:\")\n",
    "        for w, score in similar:\n",
    "            print(f\"  {w}: {score:.4f}\")\n",
    "        print()\n",
    "    except KeyError:\n",
    "        print(f\"{word}: слово не найдено в словаре\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b458f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "компьютер:\n",
      "  компьютерный: 0.8005\n",
      "  компилятор: 0.6718\n",
      "  лютер: 0.6098\n",
      "  шутер: 0.5795\n",
      "  компиляция: 0.5495\n",
      "  интерфейс: 0.5440\n",
      "  PC: 0.5410\n",
      "  Microsoft: 0.5369\n",
      "  интер: 0.5228\n",
      "  тернер: 0.5216\n",
      "\n",
      "музыка:\n",
      "  музыкально: 0.8438\n",
      "  музыкант: 0.8277\n",
      "  музыкальный: 0.7597\n",
      "  музыковед: 0.7391\n",
      "  композитор: 0.5693\n",
      "  муза: 0.5679\n",
      "  скрипка: 0.5383\n",
      "  фортепиано: 0.5091\n",
      "  композиция: 0.4968\n",
      "  гармоника: 0.4915\n",
      "\n",
      "война:\n",
      "  бойня: 0.5069\n",
      "  тайна: 0.4902\n",
      "  хвойный: 0.4331\n",
      "  войсковой: 0.4082\n",
      "  оккупация: 0.4010\n",
      "  войско: 0.3995\n",
      "  вторжение: 0.3950\n",
      "  разбойник: 0.3890\n",
      "  восстание: 0.3857\n",
      "  военнопленный: 0.3844\n",
      "\n",
      "смерть:\n",
      "  паперть: 0.6217\n",
      "  бессмертие: 0.5912\n",
      "  смертность: 0.5757\n",
      "  смертный: 0.5728\n",
      "  смертельно: 0.5553\n",
      "  посмертно: 0.5513\n",
      "  гибель: 0.4887\n",
      "  четверть: 0.4657\n",
      "  смертельный: 0.4608\n",
      "  умирать: 0.4388\n",
      "\n",
      "язык:\n",
      "  языковый: 0.7215\n",
      "  языкознание: 0.7193\n",
      "  язычник: 0.7055\n",
      "  языковой: 0.6902\n",
      "  языческий: 0.5003\n",
      "  бык: 0.4884\n",
      "  клык: 0.4689\n",
      "  англоязычный: 0.4677\n",
      "  язва: 0.4460\n",
      "  диалект: 0.4403\n",
      "\n",
      "лингвистика:\n",
      "  лингвист: 0.7539\n",
      "  этика: 0.7490\n",
      "  лингвистический: 0.7423\n",
      "  стилистика: 0.7347\n",
      "  эстетика: 0.7261\n",
      "  кинематика: 0.7172\n",
      "  публицистика: 0.7075\n",
      "  статистика: 0.7028\n",
      "  математика: 0.6959\n",
      "  грамматика: 0.6952\n",
      "\n",
      "самолет:\n",
      "  самолетный: 0.7680\n",
      "  вертолет: 0.7512\n",
      "  полет: 0.7455\n",
      "  пролет: 0.6766\n",
      "  взлет: 0.6686\n",
      "  вылет: 0.6133\n",
      "  самоа: 0.5867\n",
      "  налет: 0.5588\n",
      "  пистолет: 0.5447\n",
      "  билет: 0.5326\n",
      "\n",
      "торт:\n",
      "  норт: 0.7268\n",
      "  сорт: 0.7248\n",
      "  порт: 0.6903\n",
      "  корт: 0.6883\n",
      "  ньюпорт: 0.6843\n",
      "  аборт: 0.6742\n",
      "  импорт: 0.6326\n",
      "  борт: 0.6251\n",
      "  комфорт: 0.6224\n",
      "  транспорт: 0.6041\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fasttext\n",
    "\n",
    "for word in test_words:\n",
    "    try:\n",
    "        similar = ft.wv.most_similar(word, topn=10)\n",
    "        print(f\"{word}:\")\n",
    "        for w, score in similar:\n",
    "            print(f\"  {w}: {score:.4f}\")\n",
    "        print()\n",
    "    except KeyError:\n",
    "        print(f\"{word}: слово не найдено в словаре\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667931fd",
   "metadata": {},
   "source": [
    "Получилось намного лучше, чем в наших самописных велосипедах выше. Видно, что FastText чаще выдаёт похожие по написанию (или просто с общими подсловами) слова, хотя и близкие по смыслу тоже частотны — например, для слова *война*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bb928c",
   "metadata": {},
   "source": [
    "# Задание 3 (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3019b0d1",
   "metadata": {},
   "source": [
    "Используя датасет для классификации (labeled.csv), обучите классификатор на базе эмбеддингов. Оцените качество на отложенной выборке.   \n",
    "В качестве эмбеддинг модели вы можете использовать одну из моделей обученных в предыдущем задании или использовать одну из предобученных моделей с rusvectores (удостоверьтесь что правильно воспроизводите предобработку в этом случае!)  \n",
    "Для того, чтобы построить эмбединг целого текста, усредните вектора отдельных слов в один общий вектор. \n",
    "В качестве алгоритма классификации используйте LogisicticRegression (можете попробовать SGDClassifier, чтобы было побыстрее)  \n",
    "F1 мера должна быть выше 20%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed908832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ca5e7e",
   "metadata": {},
   "source": [
    "## 1. Загружаем и готовим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af9fdc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер датасета: 14412\n",
      "Распределение классов:\n",
      "toxic\n",
      "0.0    9586\n",
      "1.0    4826\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-d3e8dd78-d4a9-40c0-9980-7c2433796a5e\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Верблюдов-то за что? Дебилы, бл...\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Хохлы, это отдушина затюканого россиянина, мол...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Собаке - собачья смерть\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Страницу обнови, дебил. Это тоже не оскорблени...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>тебя не убедил 6-страничный пдф в том, что Скр...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d3e8dd78-d4a9-40c0-9980-7c2433796a5e')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-d3e8dd78-d4a9-40c0-9980-7c2433796a5e button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-d3e8dd78-d4a9-40c0-9980-7c2433796a5e');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                             comment  toxic\n",
       "0               Верблюдов-то за что? Дебилы, бл...\\n    1.0\n",
       "1  Хохлы, это отдушина затюканого россиянина, мол...    1.0\n",
       "2                          Собаке - собачья смерть\\n    1.0\n",
       "3  Страницу обнови, дебил. Это тоже не оскорблени...    1.0\n",
       "4  тебя не убедил 6-страничный пдф в том, что Скр...    1.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/notebooks/word_embeddings/labeled.csv')\n",
    "\n",
    "print(f\"Размер датасета: {len(data)}\")\n",
    "print(f\"Распределение классов:\\n{data['toxic'].value_counts()}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0eba78",
   "metadata": {},
   "source": [
    "Я буду использовать предобученную модель с леммами, поэтому препроцессинг тот же, что и в первых двух заданиях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0b1e16c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69afdbd858724275a4f6059d2de3ef00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14412 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Лемматизируем\n",
    "data['lemmas'] = data['comment'].progress_apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7e788b",
   "metadata": {},
   "source": [
    "## 2. Загружаем предобученную модель\n",
    "\n",
    "Вместо плохеньких моделей, что мы накрафтили в первых двух заданиях, возьмём (надеюсь) хорошую: \"geowac_lemmas_none_fasttextskipgram_300_5_2020\". Я выбрал эту модель, а не, например, более новую, обученную на НКРЯ, потому что в тех моделях к лемме присобачили части речи. Тегать 14к примеров тегами UD мне сейчас что-то совсем не хочется, поэтому нашёл модель, которая с леммами, но без POS-тегов — и по метрикам она даже хорошая. Кроме того, это FastText-моделька, а значит и для большего количества слов мы сможем получить эмбеддинги."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3485108c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-27 15:54:09--  https://vectors.nlpl.eu/repository/20/213.zip\n",
      "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.200, 2001:700:112::200\n",
      "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.200|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1485270300 (1.4G) [application/zip]\n",
      "Saving to: ‘213.zip’\n",
      "\n",
      "213.zip             100%[===================>]   1.38G  28.2MB/s    in 52s     \n",
      "\n",
      "2025-12-27 15:55:01 (27.5 MB/s) - ‘213.zip’ saved [1485270300/1485270300]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://vectors.nlpl.eu/repository/20/213.zip\n",
    "\n",
    "!mkdir -p model\n",
    "!unzip -q 213.zip -d model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ef70e1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_pretrained = gensim.models.fasttext.FastTextKeyedVectors.load(\n",
    "    'model/model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "16e57a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря: 154923\n",
      "Размерность векторов: 300\n"
     ]
    }
   ],
   "source": [
    "print(f\"Размер словаря: {len(ft_pretrained)}\")\n",
    "print(f\"Размерность векторов: {ft_pretrained.vector_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685d7fc0",
   "metadata": {},
   "source": [
    "## 3. Векторизация текстов\n",
    "\n",
    "Просто усредняем. Если ни одного слова нет, придётся занулять."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8363e0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_vector(tokens, model):\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        if token in model:\n",
    "            vectors.append(model[token])\n",
    "\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        # Придётся занулить\n",
    "        return np.zeros(model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "793f6153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "185111e69b864080bc5f02e0897b6e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14412 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.array([text_to_vector(tokens, ft_pretrained)\n",
    "             for tokens in tqdm(data['lemmas'])])\n",
    "y = data['toxic'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3fd1ad21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерность X: (14412, 300)\n",
      "Размерность y: (14412,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Размерность X: {X.shape}\")\n",
    "print(f\"Размерность y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df09be5",
   "metadata": {},
   "source": [
    "## 4. Делим на выборки\n",
    "\n",
    "90-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f762d8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обучающей выборки: 12970\n",
      "Размер тестовой выборки: 1442\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Размер обучающей выборки: {len(X_train)}\")\n",
    "print(f\"Размер тестовой выборки: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78409122",
   "metadata": {},
   "source": [
    "## 5. Обучаем классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "586ae05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=42)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9c473c",
   "metadata": {},
   "source": [
    "## 6. Оцениваем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f1280628",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "beea78da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.8342\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f7b543fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-toxic       0.90      0.94      0.92       959\n",
      "       toxic       0.88      0.80      0.83       483\n",
      "\n",
      "    accuracy                           0.89      1442\n",
      "   macro avg       0.89      0.87      0.88      1442\n",
      "weighted avg       0.89      0.89      0.89      1442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report:\")\n",
    "print(\n",
    "    classification_report(\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        target_names=[\n",
    "            'non-toxic',\n",
    "            'toxic']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cff3e36",
   "metadata": {},
   "source": [
    "Получилось вроде достойно, хотя, кажется, обычный TF-IDF с логрегом давал больше.\n",
    "\n",
    "В целом, усреднять векторы слов для получения вектора предложения — не самая лучшая идея, особенно в случае статических эмбеддингов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c18c5a",
   "metadata": {},
   "source": [
    "# Задание 4 (2 доп балла)\n",
    "\n",
    "В тетрадку с фастекстом добавьте код для обучения с negative sampling (задача сводится к бинарной классификации) и обучите модель. Проверьте полученную модель на нескольких словах. Похожие слова должны быть похожими по смыслу и по форме."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7995e50c",
   "metadata": {},
   "source": [
    "## 1. Создаём N-граммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d437a8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для создания символьных N-грамм\n",
    "\n",
    "def ngrammer(raw_string, n=2):\n",
    "    ngrams = []\n",
    "    raw_string = ''.join(['<', raw_string, '>'])\n",
    "    for i in range(0, len(raw_string) - n + 1):\n",
    "        ngram = ''.join(raw_string[i:i + n])\n",
    "        if ngram == '<' or ngram == '>':\n",
    "            continue\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "def split_tokens(tokens, min_ngram_size, max_ngram_size):\n",
    "    tokens_with_subwords = []\n",
    "    for token in tokens:\n",
    "        subtokens = []\n",
    "        for i in range(min_ngram_size, max_ngram_size + 1):\n",
    "            if len(token) > i:\n",
    "                subtokens.extend(ngrammer(token, i))\n",
    "        tokens_with_subwords.append(subtokens)\n",
    "    return tokens_with_subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0414503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubwordTokenizer:\n",
    "    def __init__(self, ngram_range=(1, 1), min_count=5):\n",
    "        self.min_ngram_size, self.max_ngram_size = ngram_range\n",
    "        self.min_count = min_count\n",
    "        self.subword_vocab = None\n",
    "        self.fullword_vocab = None\n",
    "        self.vocab = None\n",
    "        self.id2word = None\n",
    "        self.word2id = None\n",
    "\n",
    "    def build_vocab(self, texts):\n",
    "        unfiltered_subword_vocab = Counter()\n",
    "        unfiltered_fullword_vocab = Counter()\n",
    "        for text in tqdm(texts):\n",
    "            # Используем нашу функцию с лемматизацией\n",
    "            tokens = preprocess(text)\n",
    "            unfiltered_fullword_vocab.update(tokens)\n",
    "            subwords_per_token = split_tokens(\n",
    "                tokens, self.min_ngram_size, self.max_ngram_size)\n",
    "            for subwords in subwords_per_token:\n",
    "                unfiltered_subword_vocab.update(set(subwords))\n",
    "\n",
    "        self.fullword_vocab = set()\n",
    "        self.subword_vocab = set()\n",
    "\n",
    "        for word, count in unfiltered_fullword_vocab.items():\n",
    "            if count >= self.min_count:\n",
    "                self.fullword_vocab.add(word)\n",
    "\n",
    "        for word, count in unfiltered_subword_vocab.items():\n",
    "            if count >= (self.min_count * 100):\n",
    "                self.subword_vocab.add(word)\n",
    "\n",
    "        self.vocab = self.fullword_vocab | self.subword_vocab\n",
    "        self.id2word = {i: word for i, word in enumerate(self.vocab)}\n",
    "        self.word2id = {word: i for i, word in self.id2word.items()}\n",
    "\n",
    "    def subword_tokenize(self, text):\n",
    "        if self.vocab is None:\n",
    "            raise AttributeError('Vocabulary is not built!')\n",
    "\n",
    "        tokens = preprocess(text)\n",
    "        tokens_with_subwords = split_tokens(\n",
    "            tokens, self.min_ngram_size, self.max_ngram_size)\n",
    "        only_vocab_tokens_with_subwords = []\n",
    "\n",
    "        for full_token, sub_tokens in zip(tokens, tokens_with_subwords):\n",
    "            filtered = []\n",
    "            if full_token in self.vocab:\n",
    "                filtered.append(full_token)\n",
    "            filtered.extend([subtoken for subtoken in set(\n",
    "                sub_tokens) if subtoken in self.vocab])\n",
    "            only_vocab_tokens_with_subwords.append(filtered)\n",
    "\n",
    "        return only_vocab_tokens_with_subwords\n",
    "\n",
    "    def encode(self, subword_tokenized_text):\n",
    "        encoded_text = []\n",
    "        for token in subword_tokenized_text:\n",
    "            if not token:\n",
    "                continue\n",
    "            encoded_text.append([self.word2id[token[0]]] + [self.word2id[t]\n",
    "                                for t in set(token[1:]) if t in self.word2id and t != token[0]])\n",
    "        return encoded_text\n",
    "\n",
    "    def __call__(self, text):\n",
    "        return self.encode(self.subword_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "327787f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29a5c0b3c5ae46629b4125c257746aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер полного словаря: 17167\n",
      "Размер словаря полных слов: 12942\n",
      "Размер словаря N-грамм: 4620\n"
     ]
    }
   ],
   "source": [
    "# Делаем словарь\n",
    "tokenizer = SubwordTokenizer(ngram_range=(2, 5), min_count=30)\n",
    "\n",
    "tokenizer.build_vocab(wiki)\n",
    "\n",
    "print(f\"Размер полного словаря: {len(tokenizer.vocab)}\")\n",
    "print(f\"Размер словаря полных слов: {len(tokenizer.fullword_vocab)}\")\n",
    "print(f\"Размер словаря N-грамм: {len(tokenizer.subword_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9b27d4",
   "metadata": {},
   "source": [
    "## 2. Создание датасета для Fasttext с negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7758ee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batches_ft_neg(\n",
    "        sentences,\n",
    "        tokenizer,\n",
    "        window=5,\n",
    "        batch_size=1000,\n",
    "        maxlen=20):\n",
    "    vocab_size = len(tokenizer.vocab)\n",
    "\n",
    "    while True:\n",
    "        X_target = []\n",
    "        X_context = []\n",
    "        y = []\n",
    "\n",
    "        for sent in sentences:\n",
    "            sent_encoded = tokenizer(sent)\n",
    "\n",
    "            for i in range(len(sent_encoded)):\n",
    "                word_with_subtokens = sent_encoded[i]\n",
    "                context = sent_encoded[max(\n",
    "                    0, i - window):i] + sent_encoded[i + 1:min(len(sent_encoded), i + window + 1)]\n",
    "\n",
    "                for context_word_with_subtokens in context:\n",
    "                    # Положительный пример\n",
    "                    only_full_word_context_token = context_word_with_subtokens[0]\n",
    "                    X_target.append(word_with_subtokens)\n",
    "                    X_context.append(only_full_word_context_token)\n",
    "                    y.append(1)\n",
    "\n",
    "                    # Негативный пример\n",
    "                    X_target.append(word_with_subtokens)\n",
    "                    X_context.append(np.random.randint(vocab_size))\n",
    "                    y.append(0)\n",
    "\n",
    "                    if len(X_target) >= batch_size:\n",
    "                        X_target_arr = pad_sequences(\n",
    "                            X_target, maxlen=maxlen, padding='post', value=0)\n",
    "                        X_context_arr = np.array(X_context, dtype='int64')\n",
    "                        y_arr = np.array(y, dtype='float32')\n",
    "                        yield (X_target_arr, X_context_arr), y_arr\n",
    "                        X_target, X_context, y = [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f5de85",
   "metadata": {},
   "source": [
    "## 3. Модель Fasttext с negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a0ff0a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastTextNegSampling(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.target_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.context_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "\n",
    "    def forward(self, target_with_subwords, context_ids):\n",
    "        # target_with_subwords: (batch, maxlen) - слово + его нграммы\n",
    "        # context_ids: (batch,) - слово из контекста\n",
    "        t = self.target_emb(target_with_subwords)  # (batch, maxlen, emb_dim)\n",
    "        # (batch, emb_dim) - усредняем слово и N-граммы\n",
    "        t_mean = t.mean(dim=1)\n",
    "        c = self.context_emb(context_ids)          # (batch, emb_dim)\n",
    "        # (batch,) - скалярное произведение\n",
    "        dot = (t_mean * c).sum(dim=1)\n",
    "        return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "31d62899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря: 17167\n",
      "Размерность эмбеддингов: 300\n"
     ]
    }
   ],
   "source": [
    "vocab_size_ft = len(tokenizer.vocab)\n",
    "emb_dim_ft = 300\n",
    "\n",
    "print(f\"Размер словаря: {vocab_size_ft}\")\n",
    "print(f\"Размерность эмбеддингов: {emb_dim_ft}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57ce4af",
   "metadata": {},
   "source": [
    "## 4. Обучаем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a351817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = FastTextNegSampling(vocab_size_ft, emb_dim_ft).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model_ft.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fc87e2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_ft = gen_batches_ft_neg(\n",
    "    wiki[:int(0.95 * len(wiki))], tokenizer, window=7, batch_size=1000, maxlen=20)\n",
    "valid_gen_ft = gen_batches_ft_neg(\n",
    "    wiki[int(0.95 * len(wiki)):], tokenizer, window=7, batch_size=1000, maxlen=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a84fd6a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d590395bac924f8bbf4a73dc2cba86f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Эпохи:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "395c39ca04be4fd381551699304ee609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - train loss: 1.1494, val loss: 0.8766\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ad02e28418454395d8d6b83d644479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - train loss: 0.6388, val loss: 0.7084\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc1c2db3931430e9322508f3abdfbab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - train loss: 0.5337, val loss: 0.5119\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adfec524b586404db636d2e0615eb039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - train loss: 0.4952, val loss: 0.5172\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fdc31029d084a73aea94c5d933f6760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - train loss: 0.4762, val loss: 0.4035\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be54d829013e4b618968f08c0ebfd7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - train loss: 0.4689, val loss: 0.4392\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9dfaacf566c451fbf829e127681202c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - train loss: 0.4422, val loss: 0.4834\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695af73a98b54e0ebb739194a612edf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - train loss: 0.4359, val loss: 0.4500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec39649a654342e29d7e7e99698bee0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - train loss: 0.4364, val loss: 0.4371\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ec3ace88e94972b8c63eb4355aa281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Шаги:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - train loss: 0.4350, val loss: 0.4349\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = 5000\n",
    "validation_steps = 30\n",
    "num_epochs_ft = 10\n",
    "\n",
    "for epoch in tqdm(range(num_epochs_ft), desc=\"Эпохи\"):\n",
    "    model_ft.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for step in tqdm(range(steps_per_epoch), desc=\"Шаги\", leave=False):\n",
    "        (X_t, X_c), y = next(train_gen_ft)\n",
    "        X_t = torch.LongTensor(X_t).to(device)\n",
    "        X_c = torch.LongTensor(X_c).to(device)\n",
    "        y_t = torch.FloatTensor(y).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model_ft(X_t, X_c)\n",
    "        loss = criterion(logits, y_t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= steps_per_epoch\n",
    "\n",
    "    model_ft.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for _ in range(validation_steps):\n",
    "            (X_t, X_c), y = next(valid_gen_ft)\n",
    "            X_t = torch.LongTensor(X_t).to(device)\n",
    "            X_c = torch.LongTensor(X_c).to(device)\n",
    "            y_t = torch.FloatTensor(y).to(device)\n",
    "\n",
    "            logits = model_ft(X_t, X_c)\n",
    "            loss = criterion(logits, y_t)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= validation_steps\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {\n",
    "            epoch + 1}/{num_epochs_ft} - train loss: {\n",
    "            epoch_loss:.4f}, val loss: {\n",
    "                val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7da21ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_ft = model_ft.target_emb.weight.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5677497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем эмбеддинги для полных слов (с учетом N-грамм)\n",
    "full_word_embeddings_ft = np.zeros((len(tokenizer.fullword_vocab), emb_dim_ft))\n",
    "id2word_ft = list(tokenizer.fullword_vocab)\n",
    "\n",
    "for i, word in enumerate(tokenizer.fullword_vocab):\n",
    "    subwords = tokenizer(word)[0]\n",
    "    if subwords:\n",
    "        full_word_embeddings_ft[i] = embeddings_ft[subwords].mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733fd48b",
   "metadata": {},
   "source": [
    "## 5. Тестим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b75458c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "\n",
    "def most_similar_ft(word, embeddings, tokenizer, top_n=10):\n",
    "    if word not in tokenizer.fullword_vocab:\n",
    "        return f\"Слово '{word}' не найдено в словаре\"\n",
    "\n",
    "    subwords = tokenizer(word)[0]\n",
    "    if not subwords:\n",
    "        return f\"Не удалось получить нграммы для слова '{word}'\"\n",
    "\n",
    "    word_embedding = embeddings[subwords].mean(axis=0)\n",
    "    similar = [id2word_ft[i] for i in cosine_distances(\n",
    "        word_embedding.reshape(1, -1), full_word_embeddings_ft).argsort()[0][:top_n]]\n",
    "    return similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2bac365d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "компьютер: ['компьютер', 'компьютерный', 'композиция', 'компиляция', 'суперкомпьютер', 'компилятор', 'комплект', 'комплектация', 'компенсация', 'компонент']\n",
      "музыка: ['музыка', 'музыкант', 'музыкально', 'музыкальный', 'концерт', 'язык', 'белка', 'родригес', 'марка', 'языковой']\n",
      "война: ['война', 'армия', 'год', 'есть', 'быть', 'генерал', '1918', '1943', '1919', '1942']\n",
      "смерть: ['смерть', 'после', 'говорить', 'свой', 'поддерживать', 'четверть', 'содержать', 'жить', 'смертный', 'знать']\n",
      "язык: ['язык', 'для', 'и', 'как', 'есть', 'быть', 'в', 'книга', 'этот', 'языковой']\n",
      "лингвистика: ['лингвистика', 'лингвист', 'лингвистический', 'характеристика', 'стилистика', 'гимнастика', 'статистика', 'журналистика', 'диагностика', 'практика']\n",
      "самолет: ['самолет', 'пролет', 'фиолетовый', 'пистолет', 'такой', 'болеть', 'полет', 'вертолет', 'случай', 'для']\n",
      "торт: ['торт', 'корт', 'торпедо', 'торпедный', 'сорт', 'норт', 'ортон', 'торпеда', 'тора', 'тор']\n"
     ]
    }
   ],
   "source": [
    "test_words = [\n",
    "    'компьютер',\n",
    "    'музыка',\n",
    "    'война',\n",
    "    'смерть',\n",
    "    'язык',\n",
    "    'лингвистика',\n",
    "    'самолет',\n",
    "    'торт']\n",
    "\n",
    "for word in test_words:\n",
    "    result = most_similar_ft(word, embeddings_ft, tokenizer)\n",
    "    print(f\"{word}: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78ac32f",
   "metadata": {},
   "source": [
    "Видно, что в основном выбираются близкие по написанию, или производные слова. Но не всё так плохо, и близкие семантически слова тоже есть: *армия* для слова *война*, *концерт* для слова *музыка* и т.д. В целом результат как будто лучше, чем с моделями в 1 задании, но стоп-слова всё равно надо убирать.\n",
    "\n",
    "P.S. Я решил оставить свой препроцессинг и работать с леммами, а не со словоформами, поэтому \"близкие по форме\" заменяются \"близкими по написанию\" и \"производными\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
