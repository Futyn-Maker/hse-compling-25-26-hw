{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание № 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1 (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдите 1 любой способ сломать токенизацию на предложения функцией sentenize из библиотеки razdel. Придумайте (или найдите на каком-то корпусе) такое предложение (или несколько предложений), которое будет некорректно разобрано sentenize, но при этом будет грамматически корректным. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из Лурка (старого — у меня локальный дамп)\n",
    "\n",
    "> «Кто хочет стать миллионером?» — педерача, в которой актеры гонятся за 3кк рублей (1кк евро/долларов/стерлингов, в странах с меньшей валютой 5кк), отвечая на вопросы и разводя зрителя на отправку смс. Является излюбленной едой фотожабберов и развлекательных педерач.  В 2019 году в одном из выпусков должны были играть звёзды «Что? Где? Когда?» Друзь и Сиднев. Сыграли они так себе, забрав всего 200 тысяч рублёв.\n",
    "\n",
    "Здесь два момента, которые могут потенциально сломать правиловое разделение на предложения. Во-первых, вопросительный знак в конце названия передачи «Кто хочет стать миллионером?», который, тем не менее, не заканчивает предложение, а, во-вторых, название передачи «Что? Где? Когда», которое как бы и состоит из трёх коротких предложений, но тем не менее в данном случае является именем собственным в другом предложении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from razdel import sentenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"«Кто хочет стать миллионером?» — педерача, в которой актеры гонятся за 3кк рублей (1кк евро/долларов/стерлингов, в странах с меньшей валютой 5кк), отвечая на вопросы и разводя зрителя на отправку смс. Является излюбленной едой фотожабберов и развлекательных педерач. В 2019 году в одном из выпусков должны были играть звёзды «Что? Где? Когда?» Друзь и Сиднев. Сыграли они так себе, забрав всего 200 тысяч рублёв.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = list(sentenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['«Кто хочет стать миллионером?» — педерача, в которой актеры гонятся за 3кк рублей (1кк евро/долларов/стерлингов, в странах с меньшей валютой 5кк), отвечая на вопросы и разводя зрителя на отправку смс.', 'Является излюбленной едой фотожабберов и развлекательных педерач.', 'В 2019 году в одном из выпусков должны были играть звёзды «Что?', 'Где?', 'Когда?»', 'Друзь и Сиднев.', 'Сыграли они так себе, забрав всего 200 тысяч рублёв.']\n"
     ]
    }
   ],
   "source": [
    "print([sent.text for sent in sents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сломалось, но частично. Кейс с «миллионером» обработался корректно — видимо, из-за того, что дальше слова шли с маленькой буквы, а вот «Что? Где? Когда?» разбилось на три предложения, и тот факт, что вокруг стояли кавычки, не помог токенизатору: видим даже, что эти кавычки «прилипли» к соответствующим разделённым предложениям.\n",
    "\n",
    "> Вообще, razdel очень легко сломать просто тем, что предложение начинается с маленькой буквы, что распространено в сети, или же инициалами, при условии, что между ними стоит пробел (типографически это некорректно, там надо неразрывный ставить, но на практике это постоянно смешивается)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Токенизация Mystem vs razdel.tokenize (2 балла)\n",
    "\n",
    "\n",
    "Токенизируйте текст с помощью razdel и с помощью Mystem. Найдите различия в токенизациях. Что по вашему работает лучше на приведенном тексте?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кажется, что у нас нет доступа к методу токенизации напрямую ни через обёртку, ни через Mystem напрямую, поэтому получим грамматический анализ для всего текста и достанем токены из поля `text`.\n",
    "\n",
    "Стоит отметить, что Mystem оставляет и пробельные токены (те, которые состоят только из пробела или перехода на новую строку). На самом деле, это удобно, когда мы лемматизируем текст и потом хотим соединить его обратно как есть, но в данном случае просто очень сильно загромождает вывод. Это единственная эвристика на Python, которую мы применим: не будем выводить чисто пробельные токены. Остальное оставляем без изменений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Вторым и третьим открытыми белыми карликами стали Сириус B и Процион B. В 1844 году директор Кёнигсбергской обсерватории Фридрих Бессель, анализируя данные наблюдений, которые велись с 1755 года, обнаружил, что Сириус, ярчайшая звезда земного неба, и Процион периодически, хотя и весьма слабо, отклоняются от прямолинейной траектории движения по небесной сфере[5]. Бессель пришёл к выводу, что у каждой из них должен быть близкий спутник. Сообщение было встречено скептически, поскольку слабый спутник оставался ненаблюдаемым, а его масса должна была быть достаточно велика — сравнимой с массой Сириуса и Проциона, соответственно.\n",
    "\n",
    "В январе 1862 года Элвин Грэхэм Кларк, юстируя 18-дюймовый рефрактор, самый большой на то время телескоп в мире (Dearborn Telescope), впоследствии поставленный семейной фирмой Кларков в обсерваторию Чикагского университета, обнаружил в непосредственной близости от Сириуса тусклую звёздочку. Это был спутник Сириуса, Сириус B, предсказанный Бесселем[6]. А в 1896 году американский астроном Д. М. Шеберле открыл Процион B, подтвердив тем самым и второе предсказание Бесселя.\n",
    "\n",
    "В 1915 году американский астроном Уолтер Сидней Адамс измерил спектр Сириуса B. Из измерений следовало, что его температура не ниже, чем у Сириуса A (по современным данным, температура поверхности Сириуса B составляет 25 000 K, а Сириуса A — 10 000 К), что, с учётом его в 10 000 раз меньшей, чем у Сириуса A, светимости указывает на очень малый радиус и, соответственно, высокую плотность — 106 г/см3 (плотность Сириуса ~0,25 г/см3, плотность Солнца ~1,4 г/см3).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from razdel import tokenize\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "\n",
    "mystem = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_mystem(text_input, mystem_instance):\n",
    "    analysis = mystem_instance.analyze(text_input)\n",
    "    return [item['text']\n",
    "            for item in analysis if 'text' in item and item['text'].strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_razdel(text_input):\n",
    "    tokens_substrings = list(tokenize(text_input))\n",
    "    tokens = [token.text for token in tokens_substrings]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Вторым', 'и', 'третьим', 'открытыми', 'белыми', 'карликами', 'стали', 'Сириус', 'B', 'и', 'Процион', 'B', '. ', 'В', '1844', 'году', 'директор', 'Кёнигсбергской', 'обсерватории', 'Фридрих', 'Бессель', ', ', 'анализируя', 'данные', 'наблюдений', ', ', 'которые', 'велись', 'с', '1755', 'года', ', ', 'обнаружил', ', ', 'что', 'Сириус', ', ', 'ярчайшая', 'звезда', 'земного', 'неба', ', ', 'и', 'Процион', 'периодически', ', ', 'хотя', 'и', 'весьма', 'слабо', ', ', 'отклоняются', 'от', 'прямолинейной', 'траектории', 'движения', 'по', 'небесной', 'сфере', '[', '5', ']', '. ', 'Бессель', 'пришёл', 'к', 'выводу', ', ', 'что', 'у', 'каждой', 'из', 'них', 'должен', 'быть', 'близкий', 'спутник', '. ', 'Сообщение', 'было', 'встречено', 'скептически', ', ', 'поскольку', 'слабый', 'спутник', 'оставался', 'ненаблюдаемым', ', ', 'а', 'его', 'масса', 'должна', 'была', 'быть', 'достаточно', 'велика', ' — ', 'сравнимой', 'с', 'массой', 'Сириуса', 'и', 'Проциона', ', ', 'соответственно', '.', 'В', 'январе', '1862', 'года', 'Элвин', 'Грэхэм', 'Кларк', ', ', 'юстируя', '18', '-', 'дюймовый', 'рефрактор', ', ', 'самый', 'большой', 'на', 'то', 'время', 'телескоп', 'в', 'мире', ' (', 'Dearborn', 'Telescope', '), ', 'впоследствии', 'поставленный', 'семейной', 'фирмой', 'Кларков', 'в', 'обсерваторию', 'Чикагского', 'университета', ', ', 'обнаружил', 'в', 'непосредственной', 'близости', 'от', 'Сириуса', 'тусклую', 'звёздочку', '. ', 'Это', 'был', 'спутник', 'Сириуса', ', ', 'Сириус', 'B', ', ', 'предсказанный', 'Бесселем', '[', '6', ']', '. ', 'А', 'в', '1896', 'году', 'американский', 'астроном', 'Д', '. ', 'М', '. ', 'Шеберле', 'открыл', 'Процион', 'B', ', ', 'подтвердив', 'тем', 'самым', 'и', 'второе', 'предсказание', 'Бесселя', '.', 'В', '1915', 'году', 'американский', 'астроном', 'Уолтер', 'Сидней', 'Адамс', 'измерил', 'спектр', 'Сириуса', 'B', '. ', 'Из', 'измерений', 'следовало', ', ', 'что', 'его', 'температура', 'не', 'ниже', ', ', 'чем', 'у', 'Сириуса', 'A', ' (', 'по', 'современным', 'данным', ', ', 'температура', 'поверхности', 'Сириуса', 'B', 'составляет', '25', '000', 'K', ', ', 'а', 'Сириуса', 'A', ' — ', '10', '000', 'К', '), ', 'что', ', ', 'с', 'учётом', 'его', 'в', '10', '000', 'раз', 'меньшей', ', ', 'чем', 'у', 'Сириуса', 'A', ', ', 'светимости', 'указывает', 'на', 'очень', 'малый', 'радиус', 'и', ', ', 'соответственно', ', ', 'высокую', 'плотность', ' — ', '106', 'г', '/', 'см3', ' (', 'плотность', 'Сириуса', ' ~', '0', ',', '25', 'г', '/', 'см3', ', ', 'плотность', 'Солнца', ' ~', '1', ',', '4', 'г', '/', 'см3', ')', '.']\n",
      "CPU times: user 485 μs, sys: 3.25 ms, total: 3.73 ms\n",
      "Wall time: 494 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Токенизируем с Mystem\n",
    "mystem_tokens = tokenize_mystem(text, mystem)\n",
    "\n",
    "print(mystem_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Вторым', 'и', 'третьим', 'открытыми', 'белыми', 'карликами', 'стали', 'Сириус', 'B', 'и', 'Процион', 'B', '.', 'В', '1844', 'году', 'директор', 'Кёнигсбергской', 'обсерватории', 'Фридрих', 'Бессель', ',', 'анализируя', 'данные', 'наблюдений', ',', 'которые', 'велись', 'с', '1755', 'года', ',', 'обнаружил', ',', 'что', 'Сириус', ',', 'ярчайшая', 'звезда', 'земного', 'неба', ',', 'и', 'Процион', 'периодически', ',', 'хотя', 'и', 'весьма', 'слабо', ',', 'отклоняются', 'от', 'прямолинейной', 'траектории', 'движения', 'по', 'небесной', 'сфере', '[', '5', ']', '.', 'Бессель', 'пришёл', 'к', 'выводу', ',', 'что', 'у', 'каждой', 'из', 'них', 'должен', 'быть', 'близкий', 'спутник', '.', 'Сообщение', 'было', 'встречено', 'скептически', ',', 'поскольку', 'слабый', 'спутник', 'оставался', 'ненаблюдаемым', ',', 'а', 'его', 'масса', 'должна', 'была', 'быть', 'достаточно', 'велика', '—', 'сравнимой', 'с', 'массой', 'Сириуса', 'и', 'Проциона', ',', 'соответственно', '.', 'В', 'январе', '1862', 'года', 'Элвин', 'Грэхэм', 'Кларк', ',', 'юстируя', '18-дюймовый', 'рефрактор', ',', 'самый', 'большой', 'на', 'то', 'время', 'телескоп', 'в', 'мире', '(', 'Dearborn', 'Telescope', ')', ',', 'впоследствии', 'поставленный', 'семейной', 'фирмой', 'Кларков', 'в', 'обсерваторию', 'Чикагского', 'университета', ',', 'обнаружил', 'в', 'непосредственной', 'близости', 'от', 'Сириуса', 'тусклую', 'звёздочку', '.', 'Это', 'был', 'спутник', 'Сириуса', ',', 'Сириус', 'B', ',', 'предсказанный', 'Бесселем', '[', '6', ']', '.', 'А', 'в', '1896', 'году', 'американский', 'астроном', 'Д', '.', 'М', '.', 'Шеберле', 'открыл', 'Процион', 'B', ',', 'подтвердив', 'тем', 'самым', 'и', 'второе', 'предсказание', 'Бесселя', '.', 'В', '1915', 'году', 'американский', 'астроном', 'Уолтер', 'Сидней', 'Адамс', 'измерил', 'спектр', 'Сириуса', 'B', '.', 'Из', 'измерений', 'следовало', ',', 'что', 'его', 'температура', 'не', 'ниже', ',', 'чем', 'у', 'Сириуса', 'A', '(', 'по', 'современным', 'данным', ',', 'температура', 'поверхности', 'Сириуса', 'B', 'составляет', '25', '000', 'K', ',', 'а', 'Сириуса', 'A', '—', '10', '000', 'К', ')', ',', 'что', ',', 'с', 'учётом', 'его', 'в', '10', '000', 'раз', 'меньшей', ',', 'чем', 'у', 'Сириуса', 'A', ',', 'светимости', 'указывает', 'на', 'очень', 'малый', 'радиус', 'и', ',', 'соответственно', ',', 'высокую', 'плотность', '—', '106', 'г', '/', 'см', '3', '(', 'плотность', 'Сириуса', '~', '0,25', 'г', '/', 'см', '3', ',', 'плотность', 'Солнца', '~', '1,4', 'г', '/', 'см', '3', ')', '.']\n",
      "CPU times: user 1.7 ms, sys: 0 ns, total: 1.7 ms\n",
      "Wall time: 1.69 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Токенизируем с razdel\n",
    "razdel_tokens = tokenize_razdel(text)\n",
    "\n",
    "print(razdel_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В большинстве случаев токенизация почти одинаковая, но есть несколько сложных случаев, которые позволяют сказать, что токенизация у razdel нам подходит больше.\n",
    "\n",
    "Во-первых, слово *18-дюймовый*. Mystem ошибочно разделил его на три токена, тогда как здесь всё-таки вернее считать его одним словом, как полнобуквенную форму \"восемнадцатидюймовый\" — это прилагательное. Такое разделение, вероятно, и ошибочно повлияет на анализ и лемматизацию.\n",
    "\n",
    "Во-вторых, Mystem не умеет токенизировать десятичные дроби с запятой (типа 1,4), а razdel — умеет. У razdel десятичная дробь — это один токен, у Mystem их три.\n",
    "\n",
    "Следующий недостаток, который очень мелкий и на самом деле исправляется простым `strip.()` — то, что Mystem оставляет пробелы вместе со знаками препинания. В прочем, как я уже сказал, в данном случае это скорее не баг, а фича, потому что так потом очень удобно собирать текст обратно. Токенизатор из razdel возвращает «чистые» токены.\n",
    "\n",
    "Тем не менее, есть один кейс, в котором Mystem сработал лучше, чем razdel: это обозначение кубического сантиметра *см3*. Mystem оставил это как один токен, а razdel отделил тройку от сокращения — здесь первый вариант представляется более правильным.\n",
    "\n",
    "То есть получается, что Mystem наивно работает с пунктуацией и отделяет её от слова в любом случае, не обращая внимание ни на что, а razdel с этой пунктуацией работает чуть умнее. Однако razdel, возможно, наивнее работает с числами.\n",
    "\n",
    "Ни один из токенизаторов не справился с тем, чтобы оставить *10 000* и другие большие числа как один токен, но, наверное, это уже не их проблема: по правилам там должен быть не пробел, а какой-нибудь разделитель типа апострофа.\n",
    "\n",
    "Ну и, разумеется, Mystem токенизировал текст намного медленнее, чем razdel, но это нормально, потому что Mystem — не такинизатор, а анализатор, и токены мы просто достали из анализа как побочный продукт."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Лемматизация Mystem vs Pymorphy (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизируйте текст с помощью mystem и pymorphy. Найдите различия в лемматизации. Что по вашему работает лучше на приведенном тексте?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важно: для пайморфи используйте токенизацию из mystem, чтобы исключить влияние токенизации на результат. Анализируйте только значимые различия, а не технические особенности (не сравнивайте скорость работы и удобность интерфейса).\n",
    "\n",
    "В майстеме убедитесь, что используется дизамбигуация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy3 import MorphAnalyzer\n",
    "\n",
    "\n",
    "mystem = Mystem(disambiguation=True)\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_pymorphy(tokens_list, morph_analyzer):\n",
    "    lemmas = []\n",
    "    for token in tokens_list:\n",
    "        p = morph_analyzer.parse(token)[0]\n",
    "        lemmas.append(p.normal_form)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['второй', 'и', 'третий', 'открытый', 'белый', 'карлик', 'становиться', 'сириус', 'B', 'и', 'процион', 'B', '. ', 'в', '1844', 'год', 'директор', 'кенигсбергский', 'обсерватория', 'фридрих', 'бессель', ', ', 'анализировать', 'данные', 'наблюдение', ', ', 'который', 'вестись', 'с', '1755', 'год', ', ', 'обнаруживать', ', ', 'что', 'сириус', ', ', 'яркий', 'звезда', 'земной', 'небо', ', ', 'и', 'процион', 'периодически', ', ', 'хотя', 'и', 'весьма', 'слабо', ', ', 'отклоняться', 'от', 'прямолинейный', 'траектория', 'движение', 'по', 'небесный', 'сфера', '[', '5', ']', '. ', 'бессель', 'приходить', 'к', 'вывод', ', ', 'что', 'у', 'каждый', 'из', 'они', 'должный', 'быть', 'близкий', 'спутник', '. ', 'сообщение', 'быть', 'встречать', 'скептически', ', ', 'поскольку', 'слабый', 'спутник', 'оставаться', 'ненаблюдаемый', ', ', 'а', 'его', 'масса', 'должный', 'быть', 'быть', 'достаточно', 'большой', ' — ', 'сравнимый', 'с', 'масса', 'сириус', 'и', 'процион', ', ', 'соответственно', '.', 'в', 'январь', '1862', 'год', 'элвин', 'грэхэм', 'кларк', ', ', 'юстировать', '18', '-', 'дюймовый', 'рефрактор', ', ', 'самый', 'большой', 'на', 'то', 'время', 'телескоп', 'в', 'мир', ' (', 'Dearborn', 'Telescope', '), ', 'впоследствии', 'поставлять', 'семейный', 'фирма', 'кларк', 'в', 'обсерватория', 'чикагский', 'университет', ', ', 'обнаруживать', 'в', 'непосредственный', 'близость', 'от', 'сириус', 'тусклый', 'звездочка', '. ', 'это', 'быть', 'спутник', 'сириус', ', ', 'сириус', 'B', ', ', 'предсказывать', 'бессель', '[', '6', ']', '. ', 'а', 'в', '1896', 'год', 'американский', 'астроном', 'д', '. ', 'м', '. ', 'шеберль', 'открывать', 'процион', 'B', ', ', 'подтвердить', 'то', 'самый', 'и', 'второй', 'предсказание', 'бессель', '.', 'в', '1915', 'год', 'американский', 'астроном', 'уолтер', 'сидней', 'адамс', 'измерять', 'спектр', 'сириус', 'B', '. ', 'из', 'измерение', 'следовать', ', ', 'что', 'его', 'температура', 'не', 'низкий', ', ', 'чем', 'у', 'сириус', 'A', ' (', 'по', 'современный', 'данные', ', ', 'температура', 'поверхность', 'сириус', 'B', 'составлять', '25', '000', 'K', ', ', 'а', 'сириус', 'A', ' — ', '10', '000', 'к', '), ', 'что', ', ', 'с', 'учет', 'он', 'в', '10', '000', 'раз', 'меньший', ', ', 'чем', 'у', 'сириус', 'A', ', ', 'светимость', 'указывать', 'на', 'очень', 'малый', 'радиус', 'и', ', ', 'соответственно', ', ', 'высокий', 'плотность', ' — ', '106', 'г', '/', 'см3', ' (', 'плотность', 'сириус', ' ~', '0', ',', '25', 'г', '/', 'см3', ', ', 'плотность', 'солнце', ' ~', '1', ',', '4', 'г', '/', 'см3', ')', '.']\n",
      "CPU times: user 0 ns, sys: 3.75 ms, total: 3.75 ms\n",
      "Wall time: 453 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Лемматизация с Mystem\n",
    "mystem_lemmas_raw = mystem.lemmatize(text)\n",
    "\n",
    "mystem_lemmas = [lemma for lemma in mystem_lemmas_raw if lemma.strip()]\n",
    "\n",
    "print(mystem_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['второй', 'и', 'третий', 'открытый', 'белый', 'карлик', 'стать', 'сириус', 'b', 'и', 'процион', 'b', '. ', 'в', '1844', 'год', 'директор', 'кёнигсбергский', 'обсерватория', 'фридрих', 'бессель', ', ', 'анализировать', 'дать', 'наблюдение', ', ', 'который', 'вестись', 'с', '1755', 'год', ', ', 'обнаружить', ', ', 'что', 'сириус', ', ', 'яркий', 'звезда', 'земной', 'небо', ', ', 'и', 'процион', 'периодически', ', ', 'хотя', 'и', 'весьма', 'слабо', ', ', 'отклоняться', 'от', 'прямолинейный', 'траектория', 'движение', 'по', 'небесный', 'сфера', '[', '5', ']', '. ', 'бессель', 'прийти', 'к', 'вывод', ', ', 'что', 'у', 'каждый', 'из', 'они', 'должный', 'быть', 'близкий', 'спутник', '. ', 'сообщение', 'быть', 'встретить', 'скептически', ', ', 'поскольку', 'слабый', 'спутник', 'оставаться', 'ненаблюдаемый', ', ', 'а', 'он', 'масса', 'должный', 'быть', 'быть', 'достаточно', 'большой', ' — ', 'сравнимый', 'с', 'масса', 'сириус', 'и', 'процион', ', ', 'соответственно', '.', 'в', 'январь', '1862', 'год', 'элвин', 'грэхэма', 'кларк', ', ', 'юстировать', '18', '-', 'дюймовый', 'рефрактор', ', ', 'самый', 'большой', 'на', 'то', 'время', 'телескоп', 'в', 'мир', ' (', 'dearborn', 'telescope', '), ', 'впоследствии', 'поставить', 'семейный', 'фирма', 'кларк', 'в', 'обсерватория', 'чикагский', 'университет', ', ', 'обнаружить', 'в', 'непосредственный', 'близость', 'от', 'сириус', 'тусклый', 'звёздочка', '. ', 'это', 'быть', 'спутник', 'сириус', ', ', 'сириус', 'b', ', ', 'предсказать', 'бессель', '[', '6', ']', '. ', 'а', 'в', '1896', 'год', 'американский', 'астроном', 'далее', '. ', 'м', '. ', 'шеберл', 'открыть', 'процион', 'b', ', ', 'подтвердить', 'тем', 'самый', 'и', 'второе', 'предсказание', 'бессель', '.', 'в', '1915', 'год', 'американский', 'астроном', 'уолтер', 'сидней', 'адамс', 'измерить', 'спектр', 'сириус', 'b', '. ', 'из', 'измерение', 'следовать', ', ', 'что', 'он', 'температура', 'не', 'ниже', ', ', 'чем', 'у', 'сириус', 'a', ' (', 'по', 'современный', 'данные', ', ', 'температура', 'поверхность', 'сириус', 'b', 'составлять', '25', '000', 'k', ', ', 'а', 'сириус', 'a', ' — ', '10', '000', 'к', '), ', 'что', ', ', 'с', 'учёт', 'он', 'в', '10', '000', 'раз', 'малый', ', ', 'чем', 'у', 'сириус', 'a', ', ', 'светимость', 'указывать', 'на', 'очень', 'малый', 'радиус', 'и', ', ', 'соответственно', ', ', 'высокий', 'плотность', ' — ', '106', 'г', '/', 'см3', ' (', 'плотность', 'сириус', ' ~', '0', ',', '25', 'г', '/', 'см3', ', ', 'плотность', 'солнце', ' ~', '1', ',', '4', 'г', '/', 'см3', ')', '.']\n",
      "CPU times: user 34.3 ms, sys: 362 μs, total: 34.7 ms\n",
      "Wall time: 40.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Лемматизация с Pymorphy3\n",
    "mystem_tokens = tokenize_mystem(text, mystem)\n",
    "\n",
    "pymorphy_lemmas = lemmatize_pymorphy(mystem_tokens, morph)\n",
    "\n",
    "print(pymorphy_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('становиться', 'стать'), ('B', 'b'), ('B', 'b'), ('кенигсбергский', 'кёнигсбергский'), ('данные', 'дать'), ('обнаруживать', 'обнаружить'), ('приходить', 'прийти'), ('встречать', 'встретить'), ('его', 'он'), ('грэхэм', 'грэхэма'), ('Dearborn', 'dearborn'), ('Telescope', 'telescope'), ('поставлять', 'поставить'), ('обнаруживать', 'обнаружить'), ('звездочка', 'звёздочка'), ('B', 'b'), ('предсказывать', 'предсказать'), ('д', 'далее'), ('шеберль', 'шеберл'), ('открывать', 'открыть'), ('B', 'b'), ('то', 'тем'), ('второй', 'второе'), ('измерять', 'измерить'), ('B', 'b'), ('его', 'он'), ('низкий', 'ниже'), ('A', 'a'), ('B', 'b'), ('K', 'k'), ('A', 'a'), ('учет', 'учёт'), ('меньший', 'малый'), ('A', 'a')]\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим только на отличающееся, чтобы сэкономить время\n",
    "different_lemmas = [(m_lemma, p_lemma) for m_lemma, p_lemma in zip(\n",
    "    mystem_lemmas, pymorphy_lemmas) if m_lemma != p_lemma]\n",
    "print(different_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом, лемматизация Mystem лучше, потому что есть попытка разрешения неоднозначности, которая иногда работает, что позволило, например, правильно лемматизировать *второе* как *второй* (PyMorphy3 не справился). Хотя со словоформой *данные* не справился никто: Mystem оставил как существительное plurale tantum, а PyMorphy3 проанализировал как глагол *дать* — хотя в данном случае это всё же не причастие, а отглагольное прилагательное, то есть правильным анализом была бы лемма *данный*.\n",
    "\n",
    "Есть и несколько субъективных критериев, по которым я считаю, что Mystem лучше справился с задачей. Mystem оставляет вид глагола, который был у лемматизируемой словоформы, а Pymorphy3 приводит всё к совершенному виду. С точки зрения лингвистики парадигма Mystem правильная, потому что вид — это словоклассифицирующая, а не словоизменительная категория, и два глагола в видовой паре имеют разные лексемы. Видимо, парадигма авторов PyMorphy и OpenCorpora в данном вопросе отличается. Ещё один такой случай — притяжательные местоимения, где Mystem оставляет их форму (*его*), тогда как Pymorphy3 самым вероятным анализом выбирает *он*. Хотя, скорее всего, это как раз не вопрос разных лингвистических верований, а результат невозможности разрешения омонимии у Pymorphy3, так как форма *его* ещё и форма р.п. и в.п. для личного *он*.\n",
    "\n",
    "Но есть одна вещь, в которой Pymorphy3 лучше: он сохраняет букву *ё* в леммах, тогда как Mystem либо оставляет, как было в словоформе, либо и вовсе «деёфицирует» все леммы. А ещё очень удивило, что Pymorphy3 единственную букву *д* лемматизировал как *далее*! По всей видимости — результат того самого вероятностного выбора на основе словаря (OpenCorpora), где оно, очевидно, так и размечено. Очень смелое и рискованное предположение, но в данном случае сработавшее идеально!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Лемматизация в SpaCy (2 балла)\n",
    "\n",
    "С помощью Spacy (модель для русского языка) лемматизируйте тот же текст. Проверьте есть ли различия с Mystem и Pymoprhy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать большую модельку, потому что а зачем намеренно снижать качество, когда можно лучше?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"ru_core_news_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Я попытался найти способ передавать в SpaCy не текст, а готовый список токенов, чтобы исключить влияние токенизации, однако адекватного и простого не нашёл — а найденные не удовлетворяли моим критериям красоты и качества кода и адекватности затрачиваемых усилий. Поэтому, увы. Stanza с этим справляется намного изящнее, а сейчас мы будем страдать от несоответствия токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 74.8 ms, sys: 3.49 ms, total: 78.3 ms\n",
      "Wall time: 91.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'второй', 'и', 'третий', 'открытый', 'белый', 'карлик', 'стать', 'сириус', 'b', 'и', 'процион', 'b.', 'в', '1844', 'год', 'директор', 'кёнигсбергский', 'обсерватория', 'фридрих', 'бессель', ',', 'анализировать', 'данные', 'наблюдение', ',', 'которые', 'вестись', 'с', '1755', 'год', ',', 'обнаружить', ',', 'что', 'сириус', ',', 'ярчайшая', 'звезда', 'земной', 'небо', ',', 'и', 'процион', 'периодически', ',', 'хотя', 'и', 'весьма', 'слабый', ',', 'отклоняться', 'от', 'прямолинейный', 'траектория', 'движение', 'по', 'небесный', 'сфере[5', ']', '.', 'бессель', 'прийти', 'к', 'вывод', ',', 'что', 'у', 'каждый', 'из', 'них', 'должный', 'быть', 'близкий', 'спутник', '.', 'сообщение', 'было', 'встретить', 'скептически', ',', 'поскольку', 'слабый', 'спутник', 'оставаться', 'ненаблюдаемый', ',', 'а', 'он', 'масса', 'должный', 'быть', 'быть', 'достаточно', 'большой', '—', 'сравнимый', 'с', 'масса', 'сириус', 'и', 'проциона', ',', 'соответственно', '.', '\\n\\n', 'в', 'январь', '1862', 'год', 'элвин', 'грэхэм', 'кларк', ',', 'юстировать', '18-дюймовый', 'рефрактор', ',', 'самый', 'большой', 'на', 'тот', 'время', 'телескоп', 'в', 'мир', '(', 'dearborn', 'telescope', ')', ',', 'впоследствии', 'поставить', 'семейный', 'фирма', 'кларк', 'в', 'обсерватория', 'чикагского', 'университет', ',', 'обнаружить', 'в', 'непосредственный', 'близость', 'от', 'сириус', 'тусклый', 'звёздочка', '.', 'это', 'быть', 'спутник', 'сириус', ',', 'сириус', 'b', ',', 'предсказать', 'бесселем[6', ']', '.', 'а', 'в', '1896', 'год', 'американский', 'астроном', 'д.', 'м.', 'шеберле', 'открыть', 'процион', 'b', ',', 'подтвердить', 'тем', 'самым', 'и', 'второй', 'предсказание', 'бесселя', '.', '\\n\\n', 'в', '1915', 'год', 'американский', 'астроном', 'уолтер', 'сидней', 'адамс', 'измерить', 'спектр', 'сириус', 'b.', 'Из', 'измерение', 'следовать', ',', 'что', 'он', 'температура', 'не', 'низкий', ',', 'чем', 'у', 'сириус', 'a', '(', 'по', 'современный', 'данным', ',', 'температура', 'поверхность', 'сириус', 'b', 'составлять', '25', '000', 'k', ',', 'а', 'сириус', 'a', '—', '10', '000', 'к', ')', ',', 'что', ',', 'с', 'учёт', 'его', 'в', '10', '000', 'раз', 'малый', ',', 'чем', 'у', 'сириус', 'a', ',', 'светимость', 'указывать', 'на', 'очень', 'малый', 'радиус', 'и', ',', 'соответственно', ',', 'высокий', 'плотность', '—', '106', 'г', '/', 'см3', '(', 'плотность', 'сириус', '~0,25', 'г', '/', 'см3', ',', 'плотность', 'солнце', '~1,4', 'г', '/', 'см3', ')', '.', '\\n']\n"
     ]
    }
   ],
   "source": [
    "spacy_lemmas = [\n",
    "    token.lemma_ for token in doc]\n",
    "\n",
    "print(spacy_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну, в целом, ужасненько...\n",
    "\n",
    "Прежде всего стоит напомнить, что в SpaCy в качестве лемматизатора для русского языка используется Pymorphy3, в который передаются разборы из обученной модели SpaCy: то есть Pymorphy3 лемматизирует словоформы уже зная их грамматические характеристики, задачу по разрешению неоднозначности берёт на себя SpaCy. Поэтому лемматизации присуще некоторые особенности лемматизации Pymorphy3: склонность переводить глаголы к лексемам несовершенного вида, например.\n",
    "\n",
    "Однако мы можем наблюдать, что разбор SpaCy часто мешает, а не помогает. Многие слова просто остаются в своей исходной форме: *которые*, *ярчайшая*, *чикагского*, *данным*. Не помог анализ SpaCy и в выявлении притяжательных местоимений.\n",
    "\n",
    "Ещё есть последствия кривой токенизации: из-за того, что квадратная скобочка от сноски приклеилась к словоформе *сфере*, она правильно не разобралась и не лемматизировалась.\n",
    "\n",
    "Лемматизация Mystem из этих трёх всё ещё остаётся самой аккуратной, лемматизации SpaCy я бы присвоил второе место (но помним, кто там выполняет всю черновую работу), и, наконец, «чистый» Pymorphy3 всё же справляется хуже всех по объективным причинам, но не намного хуже SpaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Вообще, самая лучшая лемматизация (и парсинг) для русского языка у моделей из Stanza и UDPipe2 (есть на Синтагрусе и на Тайге и там, и там) — но вы нам его почему-то не дали, а перевыполнять мне лень."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5*. LSH (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*необязательное задание чтобы получить 10 баллов\n",
    "\n",
    "Попробуйте искать дубликаты в настоящих текстах. Например, можете взять https://github.com/mannefedov/compling_nlp_hse_course/blob/master/data/anna_karenina.txt или https://github.com/mannefedov/compling_nlp_hse_course/blob/master/data/besy_dostoevsky.txt (или любой другой корпус)\n",
    "\n",
    "Используйте код из семинара для нахождения кандидатов в дубликаты (шинглы -> минхэш - lsh) и рассчитайте реальную меру Жаккара между полученными кандидатами. Настройте параметры k, num_hash_functions, bands так чтобы результаты получались адекватные (мера Жаккара хотя бы больше нуля). \n",
    "\n",
    "(Можете взять 500-1000 текстов если весь корпус обрабатывается слишком долго)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Загружаем данные и собираем тексты\n",
    "\n",
    "Будем использовать тестовый сет корпуса Синтагрус в UD - просто достанем все предложения как тексты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-10-26 13:58:24--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/refs/heads/master/ru_syntagrus-ud-test.conllu\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15253211 (15M) [text/plain]\n",
      "Saving to: ‘ru_syntagrus-ud-test.conllu’\n",
      "\n",
      "ru_syntagrus-ud-tes 100%[===================>]  14.55M  4.58MB/s    in 3.2s    \n",
      "\n",
      "2025-10-26 13:58:28 (4.58 MB/s) - ‘ru_syntagrus-ud-test.conllu’ saved [15253211/15253211]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/refs/heads/master/ru_syntagrus-ud-test.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего извлечено предложений: 8800\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "file_path = 'ru_syntagrus-ud-test.conllu'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if line.startswith('# text ='):\n",
    "            # Извлекаем текст, убираем префикс пробелы\n",
    "            texts.append(line.replace('# text =', '').strip())\n",
    "\n",
    "print(f\"Всего извлечено предложений: {len(texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Функции и алгоритм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shingles(text, k=5):\n",
    "    \"\"\"генерирует список шинглов из строки\"\"\"\n",
    "    shingles = set()\n",
    "    for i in range(len(text) - k + 1):\n",
    "        shingle = text[i:i + k]\n",
    "        shingles.add(shingle)\n",
    "    return shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_string(s):\n",
    "    \"\"\"хеширует строку и возвращает число\"\"\"\n",
    "    return int(hashlib.md5(s.encode('utf8')).hexdigest(), 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hash_functions(k):\n",
    "    \"\"\"генерирует k хеш-функций\"\"\"\n",
    "    functions = []\n",
    "    for i in range(k):\n",
    "        functions.append(lambda x, i=i: hash_string(x + str(i)))\n",
    "    return functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_minhash_signature(shingles, hash_funcs):\n",
    "    \"\"\"вычисляет minhash-сигнатуру для списка шинглов\"\"\"\n",
    "    signature = []\n",
    "    for hash_func in hash_funcs:\n",
    "        # Применяем хэш-функцию ко всем шинглам и берём минимум\n",
    "        min_hash = min(hash_func(shingle) for shingle in shingles)\n",
    "        signature.append(min_hash)\n",
    "    return signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В функции ниже сделаем одно исправление по сравнению с семинарским кодом. Для хранения бакетов будем использовать не списки, а множества, чтобы одни и те же тексты не попадали в один бакет. Иначе получится, что кандидаты могут содержать пары из одного и того же текста (с одним индексом)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsh(signatures, bands):\n",
    "    \"\"\"Разрезает сигнатуры на куски (bands) и группирует\"\"\"\n",
    "    buckets = defaultdict(\n",
    "        set)  # не используем list, чтобы одинаковые тексты не попали в один бакет\n",
    "\n",
    "    # Проверяем, что число хэшей делится на число бэндов\n",
    "    if len(signatures[0]) % bands != 0:\n",
    "        raise ValueError(\n",
    "            \"Number of hash functions must be divisible by the number of bands\")\n",
    "\n",
    "    band_length = len(signatures[0]) // bands\n",
    "\n",
    "    for idx, sig in enumerate(signatures):\n",
    "        for b in range(bands):\n",
    "            start = b * band_length\n",
    "            end = start + band_length\n",
    "            band = tuple(sig[start:end])\n",
    "            buckets[band].add(idx)\n",
    "\n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_texts(texts_list, k, num_hashes, bands):\n",
    "    \"\"\"Находит кандидатов в дубликаты используя MinHash и LSH.\"\"\"\n",
    "    hash_funcs = generate_hash_functions(num_hashes)\n",
    "    signatures = []\n",
    "    shingles_list = []\n",
    "\n",
    "    for text in tqdm(texts_list):\n",
    "        shingles = get_shingles(text, k)\n",
    "        shingles_list.append(shingles)\n",
    "\n",
    "        # Обрабатываем пустые тексты, если они есть\n",
    "        if shingles:\n",
    "            signature = compute_minhash_signature(shingles, hash_funcs)\n",
    "            signatures.append(signature)\n",
    "        else:\n",
    "            # Добавляем \"пустую\" сигнатуру\n",
    "            signatures.append([0] * num_hashes)\n",
    "\n",
    "    buckets = lsh(signatures, bands)\n",
    "\n",
    "    candidates = set()\n",
    "    for bucket in buckets.values():\n",
    "        if len(bucket) > 1:\n",
    "            unique_indices = sorted(list(bucket))\n",
    "            # Создаем все уникальные пары (i, j) где i < j\n",
    "            for i in range(len(unique_indices)):\n",
    "                for j in range(i + 1, len(unique_indices)):\n",
    "                    candidates.add((unique_indices[i], unique_indices[j]))\n",
    "\n",
    "    return candidates, shingles_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В функции расчёта меры Жаккарда сделаем одну важную проверку: будем возвращать 0, если оба сета шинглов пустые. Иначе будет много ложноположительных результатов на очень коротких предложениях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jaccard_from_shingles(shingles_x, shingles_y):\n",
    "    \"\"\"Считает меру Жаккара для двух готовых множеств шинглов.\"\"\"\n",
    "\n",
    "    # Обработка пустых множеств\n",
    "    if not shingles_x or not shingles_y:\n",
    "        return 0.0\n",
    "\n",
    "    intersection = len(shingles_x & shingles_y)\n",
    "    union = len(shingles_x | shingles_y)\n",
    "\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Запускаем: подбираем параметры и определяем лучшее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запуск LSH на 8800 текстах...\n",
      "Параметры: k=5, Хэш-функций=100, Бэнды=20\n"
     ]
    }
   ],
   "source": [
    "# Параметры\n",
    "SHINGLE_K = 5\n",
    "NUM_HASHES = 100\n",
    "BANDS = 20\n",
    "\n",
    "print(f\"Запуск LSH на {len(texts)} текстах...\")\n",
    "print(f\"Параметры: k={SHINGLE_K}, Хэш-функций={NUM_HASHES}, Бэнды={BANDS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80834429ee1d402ca6571cb731913dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено 86 пар-кандидатов.\n",
      "CPU times: user 1min 7s, sys: 493 ms, total: 1min 7s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "candidates, all_shingles = find_similar_texts(\n",
    "    texts,\n",
    "    k=SHINGLE_K,\n",
    "    num_hashes=NUM_HASHES,\n",
    "    bands=BANDS\n",
    ")\n",
    "\n",
    "print(f\"Найдено {len(candidates)} пар-кандидатов.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представляется, что проверять просто больше нуля не очень эффективно, поэтому возьмём порог 0.5, чтобы находить настоящие дубликаты, а не просто что-то отдалённо похожее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Проверка кандидатов (Мера Жаккара > 0.5) ---\n",
      "\n",
      "Пара: (5937, 5943), Мера Жаккара: 1.0000\n",
      "  Текст 5937: Фокин.\n",
      "  Текст 5943: Фокин.\n",
      "\n",
      "Пара: (3405, 4824), Мера Жаккара: 0.8571\n",
      "  Текст 3405: Ничего подобного.\n",
      "  Текст 4824: Ничего подобного!\n",
      "\n",
      "Пара: (2031, 2036), Мера Жаккара: 0.7328\n",
      "  Текст 2031: На прошлой неделе Министерство образования опубликовало список вузов, допущенных к проведению образовательного эксперимента.\n",
      "  Текст 2036: А на прошлой неделе Министерство образования опубликовало список из 112 вузов, допущенных к проведению эксперимента.\n",
      "\n",
      "Пара: (414, 420), Мера Жаккара: 0.5294\n",
      "  Текст 414: - Иметь-то имеют, - сказала Марья Поликарповна.\n",
      "  Текст 420: - Да, да, - сказала Марья Поликарповна.\n",
      "\n",
      "Пара: (474, 8790), Мера Жаккара: 1.0000\n",
      "  Текст 474: - Пожалуйста.\n",
      "  Текст 8790: - Пожалуйста.\n",
      "\n",
      "Пара: (8212, 8360), Мера Жаккара: 0.7091\n",
      "  Текст 8212: КТО СДАЕТ ПРОДУКТ ВТОРИЧНЫЙ, ТОТ СНАБЖАЕТСЯ ОТЛИЧНО.\n",
      "  Текст 8360: КТО СДАЕТ ПРОДУКТ ВТОРИЧНЫЙ, ТОТ ПИТАЕТСЯ ОТЛИЧНО.\n",
      "\n",
      "Пара: (1190, 8019), Мера Жаккара: 0.6000\n",
      "  Текст 1190: Почему?\n",
      "  Текст 8019: - Почему?\n",
      "\n",
      "Пара: (2728, 2729), Мера Жаккара: 0.6024\n",
      "  Текст 2728: Личный зачет - 10-8-6-5-4-3-2-1, учитывались результаты всех гонок;\n",
      "  Текст 2729: Кубок конструкторов - 10-8-6-5-4-3-2-1, учитывались результаты всех гонок.\n",
      "\n",
      "Пара: (8175, 8471), Мера Жаккара: 0.5882\n",
      "  Текст 8175: \"Чепуха, - сказал я себе самому.\n",
      "  Текст 8471: - Ого! - сказал я себе самому.\n",
      "\n",
      "Пара: (3691, 3792), Мера Жаккара: 1.0000\n",
      "  Текст 3691: - Конечно.\n",
      "  Текст 3792: - Конечно.\n",
      "\n",
      "Пара: (8385, 8491), Мера Жаккара: 0.9429\n",
      "  Текст 8385: 2. Играть на музыкальных инструментах.\n",
      "  Текст 8491: 5. Играть на музыкальных инструментах.\n",
      "\n",
      "Пара: (8360, 8480), Мера Жаккара: 0.6909\n",
      "  Текст 8360: КТО СДАЕТ ПРОДУКТ ВТОРИЧНЫЙ, ТОТ ПИТАЕТСЯ ОТЛИЧНО.\n",
      "  Текст 8480: КТО СДАЕТ ПРОДУКТ ВТОРИЧНЫЙ, ТОТ СЕКСУЕТСЯ ОТЛИЧНО.\n",
      "\n",
      "Пара: (2728, 2731), Мера Жаккара: 0.7941\n",
      "  Текст 2728: Личный зачет - 10-8-6-5-4-3-2-1, учитывались результаты всех гонок;\n",
      "  Текст 2731: Личный зачет - 10-6-4-3-2-1, учитывались результаты всех гонок;\n",
      "\n",
      "Пара: (8430, 8434), Мера Жаккара: 0.5714\n",
      "  Текст 8430: - Штаны длинные носит, говно сдавать не хочет, пищей нашей брезгует.\n",
      "  Текст 8434: - Штаны длинные носит, говно сдавать не хочет, пищей брезгует, да еще бога какого-то поминает!\n",
      "\n",
      "Пара: (6751, 7832), Мера Жаккара: 1.0000\n",
      "  Текст 6751: - Да.\n",
      "  Текст 7832: - Да.\n",
      "\n",
      "Пара: (474, 7969), Мера Жаккара: 0.8000\n",
      "  Текст 474: - Пожалуйста.\n",
      "  Текст 7969: - Пожалуйста!\n",
      "\n",
      "Пара: (2729, 2735), Мера Жаккара: 0.7532\n",
      "  Текст 2729: Кубок конструкторов - 10-8-6-5-4-3-2-1, учитывались результаты всех гонок.\n",
      "  Текст 2735: Кубок конструкторов - 9-6-4-3-2-1, учитывались результаты всех гонок.\n",
      "\n",
      "Пара: (7602, 8019), Мера Жаккара: 0.6000\n",
      "  Текст 7602: Почему?\n",
      "  Текст 8019: - Почему?\n",
      "\n",
      "Пара: (6970, 8769), Мера Жаккара: 0.7368\n",
      "  Текст 6970: - Ладно, - сказал он.\n",
      "  Текст 8769: - Ладно, - сказал я.\n",
      "\n",
      "Пара: (8557, 8561), Мера Жаккара: 0.8333\n",
      "  Текст 8557: - Вот он!!\n",
      "  Текст 8561: - Вот он!\n",
      "\n",
      "Пара: (8212, 8480), Мера Жаккара: 0.6964\n",
      "  Текст 8212: КТО СДАЕТ ПРОДУКТ ВТОРИЧНЫЙ, ТОТ СНАБЖАЕТСЯ ОТЛИЧНО.\n",
      "  Текст 8480: КТО СДАЕТ ПРОДУКТ ВТОРИЧНЫЙ, ТОТ СЕКСУЕТСЯ ОТЛИЧНО.\n",
      "\n",
      "Пара: (6502, 6606), Мера Жаккара: 0.7143\n",
      "  Текст 6502: Энтузиазм?\n",
      "  Текст 6606: Энтузиазм!\n",
      "\n",
      "Пара: (1190, 7602), Мера Жаккара: 1.0000\n",
      "  Текст 1190: Почему?\n",
      "  Текст 7602: Почему?\n",
      "\n",
      "Пара: (2729, 2732), Мера Жаккара: 0.8133\n",
      "  Текст 2729: Кубок конструкторов - 10-8-6-5-4-3-2-1, учитывались результаты всех гонок.\n",
      "  Текст 2732: Кубок конструкторов - 10-6-4-3-2-1, учитывались результаты всех гонок.\n",
      "\n",
      "Пара: (650, 3792), Мера Жаккара: 0.7143\n",
      "  Текст 650: - Конечно!\n",
      "  Текст 3792: - Конечно.\n",
      "\n",
      "Пара: (2731, 2732), Мера Жаккара: 0.5823\n",
      "  Текст 2731: Личный зачет - 10-6-4-3-2-1, учитывались результаты всех гонок;\n",
      "  Текст 2732: Кубок конструкторов - 10-6-4-3-2-1, учитывались результаты всех гонок.\n",
      "\n",
      "Пара: (650, 3691), Мера Жаккара: 0.7143\n",
      "  Текст 650: - Конечно!\n",
      "  Текст 3691: - Конечно.\n",
      "\n",
      "Пара: (7969, 8790), Мера Жаккара: 0.8000\n",
      "  Текст 7969: - Пожалуйста!\n",
      "  Текст 8790: - Пожалуйста.\n",
      "\n",
      "Пара: (2732, 2735), Мера Жаккара: 0.8451\n",
      "  Текст 2732: Кубок конструкторов - 10-6-4-3-2-1, учитывались результаты всех гонок.\n",
      "  Текст 2735: Кубок конструкторов - 9-6-4-3-2-1, учитывались результаты всех гонок.\n",
      "\n",
      "Пара: (6052, 6126), Мера Жаккара: 0.9167\n",
      "  Текст 6052: Привет, старик!\n",
      "  Текст 6126: \"Привет, старик!\n",
      "Всего найдено дубликатов: 30\n",
      "CPU times: user 1.01 ms, sys: 28 μs, total: 1.03 ms\n",
      "Wall time: 1.03 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"--- Проверка кандидатов (Мера Жаккара > 0.5) ---\")\n",
    "\n",
    "found_duplicates = 0\n",
    "\n",
    "for i, j in candidates:\n",
    "    # Получаем меру Жаккара из уже посчитанных шинглов\n",
    "    shingles_i = all_shingles[i]\n",
    "    shingles_j = all_shingles[j]\n",
    "    jaccard_score = get_jaccard_from_shingles(shingles_i, shingles_j)\n",
    "\n",
    "    # Выводим только если схожесть больше 0.5\n",
    "    if jaccard_score > 0.5:\n",
    "        found_duplicates += 1\n",
    "        print(f\"\\nПара: ({i}, {j}), Мера Жаккара: {jaccard_score:.4f}\")\n",
    "        print(f\"  Текст {i}: {texts[i]}\")\n",
    "        print(f\"  Текст {j}: {texts[j]}\")\n",
    "\n",
    "print(\"Всего найдено дубликатов:\", found_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом, заданные параметры сработали неплохо: мы нашли 86 кандидатов, из них 30 оказались адекватными дубликатами. Считать меру Жаккарда для 86 пар намного лучше, чем для 8800 пар.\n",
    "\n",
    "Но всё же кажется, что много потенциальных дубликатов мы упускаем, поэтому сделаем алгоритм более чувствительным, заставим его генерировать больше кандидатов. Для этого увеличим band и немного снизим длину шинглов (мы видели, что тут есть много очень коротких предложений). Это должно немного замедлить проверку, но по идеи дать лучший результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запуск LSH на 8800 текстах...\n",
      "Параметры: k=4, Хэш-функций=100, Бэнды=25\n"
     ]
    }
   ],
   "source": [
    "# Параметры (второй набор)\n",
    "SHINGLE_K = 4\n",
    "NUM_HASHES = 100\n",
    "BANDS = 25\n",
    "\n",
    "print(f\"Запуск LSH на {len(texts)} текстах...\")\n",
    "print(f\"Параметры: k={SHINGLE_K}, Хэш-функций={NUM_HASHES}, Бэнды={BANDS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e85cb2737e421c87626e0fbb82811b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено 328 пар-кандидатов.\n",
      "CPU times: user 1min 16s, sys: 513 ms, total: 1min 16s\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "candidates, all_shingles = find_similar_texts(\n",
    "    texts,\n",
    "    k=SHINGLE_K,\n",
    "    num_hashes=NUM_HASHES,\n",
    "    bands=BANDS\n",
    ")\n",
    "\n",
    "print(f\"Найдено {len(candidates)} пар-кандидатов.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Проверка кандидатов (Мера Жаккара > 0.5) ---\n",
      "\n",
      "Пара: (2031, 2036), Мера Жаккара: 0.7656\n",
      "  Текст 2031: На прошлой неделе Министерство образования опубликовало список вузов, допущенных к проведению образовательного эксперимента.\n",
      "  Текст 2036: А на прошлой неделе Министерство образования опубликовало список из 112 вузов, допущенных к проведению эксперимента.\n",
      "\n",
      "Пара: (2728, 2731), Мера Жаккара: 0.8235\n",
      "  Текст 2728: Личный зачет - 10-8-6-5-4-3-2-1, учитывались результаты всех гонок;\n",
      "  Текст 2731: Личный зачет - 10-6-4-3-2-1, учитывались результаты всех гонок;\n",
      "\n",
      "Пара: (6751, 7832), Мера Жаккара: 1.0000\n",
      "  Текст 6751: - Да.\n",
      "  Текст 7832: - Да.\n",
      "\n",
      "Пара: (474, 7969), Мера Жаккара: 0.8182\n",
      "  Текст 474: - Пожалуйста.\n",
      "  Текст 7969: - Пожалуйста!\n",
      "\n",
      "Пара: (6970, 8769), Мера Жаккара: 0.7500\n",
      "  Текст 6970: - Ладно, - сказал он.\n",
      "  Текст 8769: - Ладно, - сказал я.\n",
      "\n",
      "Пара: (6502, 6606), Мера Жаккара: 0.7500\n",
      "  Текст 6502: Энтузиазм?\n",
      "  Текст 6606: Энтузиазм!\n",
      "\n",
      "Пара: (2731, 2732), Мера Жаккара: 0.5875\n",
      "  Текст 2731: Личный зачет - 10-6-4-3-2-1, учитывались результаты всех гонок;\n",
      "  Текст 2732: Кубок конструкторов - 10-6-4-3-2-1, учитывались результаты всех гонок.\n",
      "\n",
      "Пара: (6052, 6126), Мера Жаккара: 0.9231\n",
      "  Текст 6052: Привет, старик!\n",
      "  Текст 6126: \"Привет, старик!\n",
      "\n",
      "Пара: (2632, 2707), Мера Жаккара: 0.5113\n",
      "  Текст 2632: За первые 10 мест на финише гонки пилоты и команды получают очки по системе 25-18-15-12-10-8-6-4-2-1.\n",
      "  Текст 2707: По окончании гонки первые 10 пилотов, а также их команды, получают очки по системе 25-18-15-12-10-8-6-4-2-1.\n",
      "\n",
      "Пара: (8212, 8360), Мера Жаккара: 0.7455\n",
      "  Текст 8212: КТО СДАЕТ ПРОДУКТ ВТОРИЧНЫЙ, ТОТ СНАБЖАЕТСЯ ОТЛИЧНО.\n",
      "  Текст 8360: КТО СДАЕТ ПРОДУКТ ВТОРИЧНЫЙ, ТОТ ПИТАЕТСЯ ОТЛИЧНО.\n",
      "\n",
      "Пара: (3691, 3792), Мера Жаккара: 1.0000\n",
      "  Текст 3691: - Конечно.\n",
      "  Текст 3792: - Конечно.\n",
      "\n",
      "Пара: (8430, 8434), Мера Жаккара: 0.5979\n",
      "  Текст 8430: - Штаны длинные носит, говно сдавать не хочет, пищей нашей брезгует.\n",
      "  Текст 8434: - Штаны длинные носит, говно сдавать не хочет, пищей брезгует, да еще бога какого-то поминает!\n",
      "\n",
      "Пара: (846, 8318), Мера Жаккара: 0.6000\n",
      "  Текст 846: Правильно.\n",
      "  Текст 8318: - Правильно!\n",
      "\n",
      "Пара: (8212, 8480), Мера Жаккара: 0.7321\n",
      "  Текст 8212: КТО СДАЕТ ПРОДУКТ ВТОРИЧНЫЙ, ТОТ СНАБЖАЕТСЯ ОТЛИЧНО.\n",
      "  Текст 8480: КТО СДАЕТ ПРОДУКТ ВТОРИЧНЫЙ, ТОТ СЕКСУЕТСЯ ОТЛИЧНО.\n",
      "\n",
      "Пара: (1190, 7602), Мера Жаккара: 1.0000\n",
      "  Текст 1190: Почему?\n",
      "  Текст 7602: Почему?\n",
      "\n",
      "Пара: (2729, 2732), Мера Жаккара: 0.8400\n",
      "  Текст 2729: Кубок конструкторов - 10-8-6-5-4-3-2-1, учитывались результаты всех гонок.\n",
      "  Текст 2732: Кубок конструкторов - 10-6-4-3-2-1, учитывались результаты всех гонок.\n",
      "\n",
      "Пара: (650, 3792), Мера Жаккара: 0.7500\n",
      "  Текст 650: - Конечно!\n",
      "  Текст 3792: - Конечно.\n",
      "\n",
      "Пара: (5937, 5943), Мера Жаккара: 1.0000\n",
      "  Текст 5937: Фокин.\n",
      "  Текст 5943: Фокин.\n",
      "\n",
      "Пара: (3405, 4824), Мера Жаккара: 0.8667\n",
      "  Текст 3405: Ничего подобного.\n",
      "  Текст 4824: Ничего подобного!\n",
      "\n",
      "Пара: (474, 8790), Мера Жаккара: 1.0000\n",
      "  Текст 474: - Пожалуйста.\n",
      "  Текст 8790: - Пожалуйста.\n",
      "\n",
      "Пара: (2728, 2729), Мера Жаккара: 0.6071\n",
      "  Текст 2728: Личный зачет - 10-8-6-5-4-3-2-1, учитывались результаты всех гонок;\n",
      "  Текст 2729: Кубок конструкторов - 10-8-6-5-4-3-2-1, учитывались результаты всех гонок.\n",
      "\n",
      "Пара: (8385, 8491), Мера Жаккара: 0.9444\n",
      "  Текст 8385: 2. Играть на музыкальных инструментах.\n",
      "  Текст 8491: 5. Играть на музыкальных инструментах.\n",
      "\n",
      "Пара: (8557, 8561), Мера Жаккара: 0.8571\n",
      "  Текст 8557: - Вот он!!\n",
      "  Текст 8561: - Вот он!\n",
      "\n",
      "Пара: (662, 6881), Мера Жаккара: 0.6000\n",
      "  Текст 662: Ничего?\n",
      "  Текст 6881: Ничего.\n",
      "\n",
      "Пара: (7969, 8790), Мера Жаккара: 0.8182\n",
      "  Текст 7969: - Пожалуйста!\n",
      "  Текст 8790: - Пожалуйста.\n",
      "\n",
      "Пара: (414, 420), Мера Жаккара: 0.5385\n",
      "  Текст 414: - Иметь-то имеют, - сказала Марья Поликарповна.\n",
      "  Текст 420: - Да, да, - сказала Марья Поликарповна.\n",
      "\n",
      "Пара: (1190, 8019), Мера Жаккара: 0.6667\n",
      "  Текст 1190: Почему?\n",
      "  Текст 8019: - Почему?\n",
      "\n",
      "Пара: (8175, 8471), Мера Жаккара: 0.6000\n",
      "  Текст 8175: \"Чепуха, - сказал я себе самому.\n",
      "  Текст 8471: - Ого! - сказал я себе самому.\n",
      "\n",
      "Пара: (8360, 8480), Мера Жаккара: 0.7273\n",
      "  Текст 8360: КТО СДАЕТ ПРОДУКТ ВТОРИЧНЫЙ, ТОТ ПИТАЕТСЯ ОТЛИЧНО.\n",
      "  Текст 8480: КТО СДАЕТ ПРОДУКТ ВТОРИЧНЫЙ, ТОТ СЕКСУЕТСЯ ОТЛИЧНО.\n",
      "\n",
      "Пара: (2729, 2735), Мера Жаккара: 0.7792\n",
      "  Текст 2729: Кубок конструкторов - 10-8-6-5-4-3-2-1, учитывались результаты всех гонок.\n",
      "  Текст 2735: Кубок конструкторов - 9-6-4-3-2-1, учитывались результаты всех гонок.\n",
      "\n",
      "Пара: (7602, 8019), Мера Жаккара: 0.6667\n",
      "  Текст 7602: Почему?\n",
      "  Текст 8019: - Почему?\n",
      "\n",
      "Пара: (2627, 2742), Мера Жаккара: 0.5133\n",
      "  Текст 2627: За соответствием машин техническому регламенту следят стюарды Международной федерации автоспорта.\n",
      "  Текст 2742: Характеристики болида определяются техническим регламентом, за соответствием которому следят стюарды Международной федерации автоспорта.\n",
      "\n",
      "Пара: (8606, 8769), Мера Жаккара: 0.5926\n",
      "  Текст 8606: - Ладно, - сказал я, - ребята.\n",
      "  Текст 8769: - Ладно, - сказал я.\n",
      "\n",
      "Пара: (650, 3691), Мера Жаккара: 0.7500\n",
      "  Текст 650: - Конечно!\n",
      "  Текст 3691: - Конечно.\n",
      "\n",
      "Пара: (2732, 2735), Мера Жаккара: 0.8732\n",
      "  Текст 2732: Кубок конструкторов - 10-6-4-3-2-1, учитывались результаты всех гонок.\n",
      "  Текст 2735: Кубок конструкторов - 9-6-4-3-2-1, учитывались результаты всех гонок.\n",
      "Всего найдено дубликатов: 35\n",
      "CPU times: user 5.72 ms, sys: 0 ns, total: 5.72 ms\n",
      "Wall time: 5.64 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"--- Проверка кандидатов (Мера Жаккара > 0.5) ---\")\n",
    "\n",
    "found_duplicates = 0\n",
    "\n",
    "for i, j in candidates:\n",
    "    # Получаем меру Жаккара из уже посчитанных шинглов\n",
    "    shingles_i = all_shingles[i]\n",
    "    shingles_j = all_shingles[j]\n",
    "    jaccard_score = get_jaccard_from_shingles(shingles_i, shingles_j)\n",
    "\n",
    "    # Выводим только если схожесть больше 0.5\n",
    "    if jaccard_score > 0.5:\n",
    "        found_duplicates += 1\n",
    "        print(f\"\\nПара: ({i}, {j}), Мера Жаккара: {jaccard_score:.4f}\")\n",
    "        print(f\"  Текст {i}: {texts[i]}\")\n",
    "        print(f\"  Текст {j}: {texts[j]}\")\n",
    "\n",
    "print(\"Всего найдено дубликатов:\", found_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы нашли немного больше дубликатов, причём, кажется, немного более релевантных (то есть мера Жаккара в целом выше), но при этом сгенерировали почти в 4 раза больше кандидатов. Видимо, параметры можно подбирать и дальше, но без золотого стандарта — то есть размеченных вручную «истинных» дубликатов — это будет трудно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. А если всё-таки брутфорс?\n",
    "\n",
    "Алгоритм, который мы применили выше, спасает нас от брутфорса — сравнения каждого текста с каждым. И всё-таки интересно: если это сделать, сколько это займёт времени и какой будет результат?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запуск полного перебора для 8800 текстов...\n",
      "Будет выполнено 38715600 сравнений.\n"
     ]
    }
   ],
   "source": [
    "bruteforce_duplicates = []\n",
    "JACCARD_THRESHOLD = 0.5\n",
    "\n",
    "n = len(texts)\n",
    "\n",
    "print(f\"Запуск полного перебора для {n} текстов...\")\n",
    "print(f\"Будет выполнено {n * (n - 1) // 2} сравнений.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c09a6169a804eab94f5767b7d1a97d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min, sys: 1.37 s, total: 5min 1s\n",
      "Wall time: 5min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    for j in range(i + 1, n):\n",
    "        # Получаем уже готовые множества шинглов\n",
    "        shingles_i = all_shingles[i]\n",
    "        shingles_j = all_shingles[j]\n",
    "\n",
    "        score = get_jaccard_from_shingles(shingles_i, shingles_j)\n",
    "\n",
    "        if score > JACCARD_THRESHOLD:\n",
    "            bruteforce_duplicates.append((i, j, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Полный перебор нашел 36 пар с Jaccard > 0.5\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Полный перебор нашел {len(bruteforce_duplicates)} пар с Jaccard > {JACCARD_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что ж, LSH отработал чуть больше минуты (не считая жалкие миллисекунды для проверки кандидатов), а брутфорс работал 5 минут. При этом мы убедились, что очень хорошо настроили наши параметры — с помощью LSH мы нашли 35 пар текстов с мерой Жаккара > 0.5, а брутфорс (который точный) нашёл 36, то есть мы упустили всего одну пару.\n",
    "\n",
    "При этом надо понимать, что сложность у алгоритма с брутфорсом квадратичная, и, например, если мы удвоим количество текстов, то время выполнения увеличится в 4 раза, а у LSH сложность линейная — так что всё работает и достаточно эффективно!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
