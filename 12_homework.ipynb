{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bf6f873",
   "metadata": {},
   "source": [
    "# Домашнее задание № 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4bd487",
   "metadata": {},
   "source": [
    "## Задание 1 (4 балла) \n",
    "\n",
    "Обучите 2 модели похожую по архитектуре на модель из ULMFit для задачи классификации текста (датасет - lenta_40k )\n",
    "В моделях должно быть как минимум два рекуррентных слоя, а финальный вектор для классификации составляться из последнего состояния RNN (так делалось в семинаре), а также AveragePooling и MaxPooling из всех векторов последовательности (конкатенируйте последнее состояния и результаты пулинга). В первой модели используйте обычные слои, а во второй Bidirectional. Рассчитайте по классовую точность/полноту/f-меру для каждой из модели (результаты не должны быть совсем близкие к нулю после обучения на хотя бы нескольких эпохах). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0710dd",
   "metadata": {},
   "source": [
    "> Так как мы работаем с тем же датасетом, что и в прошлом задании, я во многом буду переиспользовать тот код, что написал ранее. В частности, это относится к препроцессингу с лемматизацией и удалением стоп-слов, удалением двух последних классов из выборки и разделением на выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1790788f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/futyn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c338e29f",
   "metadata": {},
   "source": [
    "### 1. Загружаем и готовим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b12ec5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic\n",
      "Россия               9622\n",
      "Мир                  8193\n",
      "Экономика            4768\n",
      "Спорт                3894\n",
      "Наука и техника      3204\n",
      "Культура             3183\n",
      "Бывший СССР          3182\n",
      "Интернет и СМИ       2643\n",
      "Из жизни             1679\n",
      "Дом                  1315\n",
      "Силовые структуры    1203\n",
      "Ценности              460\n",
      "Бизнес                433\n",
      "Путешествия           418\n",
      "69-я параллель         82\n",
      "Крым                   43\n",
      "Культпросвет           25\n",
      "Легпром                 6\n",
      "Библиотека              3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "url = \"https://github.com/mannefedov/compling_nlp_hse_course/raw/master/data/lenta_40k.csv.zip\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "data.dropna(subset=['topic', 'text'], inplace=True)\n",
    "\n",
    "print(data['topic'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8135f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Убираем почти непредставленные классы\n",
    "data = data[~data['topic'].isin(['Легпром', 'Библиотека'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f88b5c",
   "metadata": {},
   "source": [
    "### 2. Препроцессинг\n",
    "\n",
    "Как и в прошлый раз — с лемматизацией и удалением стоп-слов, чтобы результат имел шанс быть получше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82d9e367",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem = Mystem()\n",
    "russian_stopwords = set(stopwords.words(\"russian\"))\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    lemmas = mystem.lemmatize(text)\n",
    "    tokens = [token for token in lemmas if token.isalnum()\n",
    "              and token not in russian_stopwords]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c3cdf42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a101579a8621441b8917cacd377707df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing text:   0%|          | 0/44347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Препроцессим весь датасет сразу\n",
    "tqdm.pandas(desc=\"Preprocessing text\")\n",
    "data['processed_tokens'] = data['text'].progress_apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cf63d9",
   "metadata": {},
   "source": [
    "### 3. Делаем словарь и датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75aeaa15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocab size: 140210\n",
      "Filtered vocab size: 20441\n"
     ]
    }
   ],
   "source": [
    "vocab = Counter()\n",
    "for tokens in data['processed_tokens']:\n",
    "    vocab.update(tokens)\n",
    "\n",
    "filtered_vocab = {word for word, count in vocab.items() if count > 15}\n",
    "\n",
    "print(f\"Original vocab size: {len(vocab)}\")\n",
    "print(f\"Filtered vocab size: {len(filtered_vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70523b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Индексируем\n",
    "word2id = {'PAD': 0, 'UNK': 1}\n",
    "for word in filtered_vocab:\n",
    "    word2id[word] = len(word2id)\n",
    "\n",
    "# Маппим лейблы\n",
    "id2label = {i: l for i, l in enumerate(set(data.topic))}\n",
    "label2id = {l: i for i, l in id2label.items()}\n",
    "\n",
    "texts_tokens = data['processed_tokens'].values\n",
    "targets = [label2id[l] for l in data.topic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9219435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Делаем трейн и дев выборки\n",
    "train_tokens, valid_tokens, train_targets, valid_targets = train_test_split(\n",
    "    texts_tokens, targets, test_size=0.05, stratify=targets, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df594206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Датасет\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, word2id, max_len, tokens_list, targets):\n",
    "        self.texts = [torch.LongTensor(\n",
    "            [word2id.get(w, 1) for w in t][:max_len]) for t in tokens_list]\n",
    "        self.texts = torch.nn.utils.rnn.pad_sequence(\n",
    "            self.texts, batch_first=True, padding_value=0)\n",
    "        self.target = torch.LongTensor(targets)\n",
    "        self.length = len(tokens_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.texts[index], self.target[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdd63454",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 200\n",
    "BATCH_SIZE = 256\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_dataset = TextDataset(word2id, MAX_LEN, train_tokens, train_targets)\n",
    "valid_dataset = TextDataset(word2id, MAX_LEN, valid_tokens, valid_targets)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a6c062",
   "metadata": {},
   "source": [
    "### 4. Архитектура RNN\n",
    "\n",
    "Мы определим один класс модели, который в зависимости от параметра направленности будет добавлять в нужные места либо однонаправленные, либо двунаправленные LSTM-слои. LSTM просто потому что оно из RNN кажется самым нормальным :) В линейных слоях классификации будем использовать батчнорм и дропаут."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3248feae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNULMFitClassifier(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size,\n",
    "            emb_dim,\n",
    "            hidden_size,\n",
    "            output_dim,\n",
    "            dropout=0.3,\n",
    "            bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "\n",
    "        # Основной LSTM, 2 слоя\n",
    "        self.rnn = nn.LSTM(\n",
    "            emb_dim,\n",
    "            hidden_size,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=bidirectional)\n",
    "\n",
    "        # Множитель для размерности в зависимости от направленности\n",
    "        self.direction_factor = 2 if bidirectional else 1\n",
    "\n",
    "        # Размерность после конкатенации\n",
    "        self.concat_dim = hidden_size * self.direction_factor * 3\n",
    "\n",
    "        # Полносвязный слой с батчнормом и дропаутом\n",
    "        self.fc1 = nn.Linear(self.concat_dim, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(256, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # Применяем эмбеддинги\n",
    "        embedded = self.embedding(text)\n",
    "\n",
    "        # Прогоняем через RNN\n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "\n",
    "        # Avg Pooling по всем скрытым состояниям\n",
    "        avg_pool = torch.mean(output, dim=1)\n",
    "\n",
    "        # Макспуллинг по всем скрытым состояниям\n",
    "        max_pool, _ = torch.max(output, dim=1)\n",
    "\n",
    "        # Последнее скрытое состояние\n",
    "        last_hidden = output[:, -1, :]\n",
    "\n",
    "        # Конкатенируем три вектора\n",
    "        cat = torch.cat((last_hidden, avg_pool, max_pool), dim=1)\n",
    "\n",
    "        # Классификация\n",
    "        x = self.dropout(F.relu(self.bn1(self.fc1(cat))))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8f84bb",
   "metadata": {},
   "source": [
    "### 5. Цикл обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b81f347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    metrics = {\n",
    "        'loss': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': [],\n",
    "        'accuracy': []}\n",
    "\n",
    "    for texts, ys in tqdm(iterator, desc=\"Training\", leave=False):\n",
    "        texts, ys = texts.to(device), ys.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model(texts)\n",
    "        loss = criterion(predictions, ys)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = predictions.argmax(1).cpu().numpy()\n",
    "        y_true = ys.cpu().numpy()\n",
    "\n",
    "        metrics['loss'].append(loss.item())\n",
    "        metrics['precision'].append(\n",
    "            precision_score(\n",
    "                y_true,\n",
    "                preds,\n",
    "                average='macro',\n",
    "                zero_division=0))\n",
    "        metrics['recall'].append(\n",
    "            recall_score(\n",
    "                y_true,\n",
    "                preds,\n",
    "                average='macro',\n",
    "                zero_division=0))\n",
    "        metrics['f1'].append(\n",
    "            f1_score(\n",
    "                y_true,\n",
    "                preds,\n",
    "                average='macro',\n",
    "                zero_division=0))\n",
    "        metrics['accuracy'].append(accuracy_score(y_true, preds))\n",
    "\n",
    "    return {k: np.mean(v) for k, v in metrics.items()}\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion, print_report=False):\n",
    "    model.eval()\n",
    "    metrics = {\n",
    "        'loss': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': [],\n",
    "        'accuracy': []}\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for texts, ys in iterator:\n",
    "            texts, ys = texts.to(device), ys.to(device)\n",
    "            predictions = model(texts)\n",
    "            loss = criterion(predictions, ys)\n",
    "\n",
    "            preds = predictions.argmax(1).cpu().numpy()\n",
    "            y_true = ys.cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_targets.extend(y_true)\n",
    "\n",
    "            metrics['loss'].append(loss.item())\n",
    "            metrics['precision'].append(\n",
    "                precision_score(\n",
    "                    y_true,\n",
    "                    preds,\n",
    "                    average='macro',\n",
    "                    zero_division=0))\n",
    "            metrics['recall'].append(\n",
    "                recall_score(\n",
    "                    y_true,\n",
    "                    preds,\n",
    "                    average='macro',\n",
    "                    zero_division=0))\n",
    "            metrics['f1'].append(\n",
    "                f1_score(\n",
    "                    y_true,\n",
    "                    preds,\n",
    "                    average='macro',\n",
    "                    zero_division=0))\n",
    "            metrics['accuracy'].append(accuracy_score(y_true, preds))\n",
    "\n",
    "    if print_report:\n",
    "        print(\"\\nClassification Report (Validation):\")\n",
    "        print(\n",
    "            classification_report(\n",
    "                all_targets,\n",
    "                all_preds,\n",
    "                target_names=[\n",
    "                    id2label[i] for i in range(\n",
    "                        len(id2label))]))\n",
    "\n",
    "    return {k: np.mean(v) for k, v in metrics.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9bfc86",
   "metadata": {},
   "source": [
    "### 6. Обучаем однонаправленную RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca72e3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a176deb1134257b76b3eaa1910bdb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 1.432, F1: 0.346\n",
      "Valid - Loss: 1.060, F1: 0.467\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84ba877a351748bfae6141032fca86ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.877, F1: 0.543\n",
      "Valid - Loss: 0.975, F1: 0.536\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "874b11f6216d4c36914d9f3bccf2c10f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.702, F1: 0.621\n",
      "Valid - Loss: 0.738, F1: 0.614\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8332f549ccf14b738d0bc9b4dd48ea84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.581, F1: 0.688\n",
      "Valid - Loss: 0.777, F1: 0.648\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c45836a712469dae93573198a9e9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.491, F1: 0.726\n",
      "Valid - Loss: 0.706, F1: 0.660\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f1b70e7eb3485eb3b3fccedfe30ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.414, F1: 0.763\n",
      "Valid - Loss: 0.806, F1: 0.647\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91267e49b2db4b778fa0a6febb8c6f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.351, F1: 0.791\n",
      "Valid - Loss: 0.759, F1: 0.656\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baadda929b084f0382062ff8dec3e35a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.289, F1: 0.827\n",
      "Valid - Loss: 0.847, F1: 0.655\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf619691ae14a8495f259ae50a4f62e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.232, F1: 0.853\n",
      "Valid - Loss: 0.888, F1: 0.652\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55ee609a4c44926987ba5f926187d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.187, F1: 0.879\n",
      "Valid - Loss: 0.889, F1: 0.646\n"
     ]
    }
   ],
   "source": [
    "model_uni = RNNULMFitClassifier(len(word2id), emb_dim=100, hidden_size=128,\n",
    "                                output_dim=len(label2id), bidirectional=False)\n",
    "model_uni = model_uni.to(device)\n",
    "\n",
    "# Оптимизатор и функция потерь\n",
    "optimizer = optim.AdamW(model_uni.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# Обучаем\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    train_metrics = train_epoch(model_uni, train_loader, optimizer, criterion)\n",
    "    valid_metrics = evaluate(model_uni, valid_loader, criterion)\n",
    "\n",
    "    print(\n",
    "        f\"Train - Loss: {train_metrics['loss']:.3f}, F1: {train_metrics['f1']:.3f}\")\n",
    "    print(\n",
    "        f\"Valid - Loss: {valid_metrics['loss']:.3f}, F1: {valid_metrics['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61439c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report (Validation):\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "         Ценности       0.69      0.87      0.77        23\n",
      "        Экономика       0.80      0.78      0.79       239\n",
      "              Мир       0.73      0.83      0.78       410\n",
      "            Спорт       0.94      0.97      0.95       195\n",
      "           Бизнес       0.47      0.32      0.38        22\n",
      "           Россия       0.78      0.73      0.75       481\n",
      "              Дом       0.84      0.62      0.71        66\n",
      "         Из жизни       0.56      0.61      0.58        84\n",
      "      Путешествия       0.59      0.48      0.53        21\n",
      "         Культура       0.84      0.79      0.82       159\n",
      "    Культпросвет        0.00      0.00      0.00         1\n",
      "   Интернет и СМИ       0.68      0.65      0.67       132\n",
      "             Крым       0.00      0.00      0.00         2\n",
      "Силовые структуры       0.57      0.52      0.54        60\n",
      "   69-я параллель       0.00      0.00      0.00         4\n",
      "  Наука и техника       0.72      0.86      0.79       160\n",
      "      Бывший СССР       0.81      0.77      0.79       159\n",
      "\n",
      "         accuracy                           0.76      2218\n",
      "        macro avg       0.59      0.58      0.58      2218\n",
      "     weighted avg       0.76      0.76      0.76      2218\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/futyn/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/futyn/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/futyn/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': np.float64(0.8893612093395658),\n",
       " 'precision': np.float64(0.6617783932549006),\n",
       " 'recall': np.float64(0.6515196762169871),\n",
       " 'f1': np.float64(0.6460027906486592),\n",
       " 'accuracy': np.float64(0.7664113562091504)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Оцениваем итоговую модель\n",
    "evaluate(model_uni, valid_loader, criterion, print_report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4999a9ea",
   "metadata": {},
   "source": [
    "### 7. Обучаем двунаправленную RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "201b130f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7b30729d334c1eb4a46b92e0458415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 1.319, F1: 0.400\n",
      "Valid - Loss: 0.947, F1: 0.516\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c85d00e21c4538938e8906ac2321d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.792, F1: 0.600\n",
      "Valid - Loss: 0.839, F1: 0.597\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d330b5cc0b34432afa9490436a4b553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.612, F1: 0.683\n",
      "Valid - Loss: 0.785, F1: 0.629\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2532aea8aa4208b8e81205228f816f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.494, F1: 0.732\n",
      "Valid - Loss: 0.801, F1: 0.645\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f838dd31bf405cbee0848f77ed8c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.394, F1: 0.775\n",
      "Valid - Loss: 0.919, F1: 0.635\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf74ab012d1c4de488cf1267088f08f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.316, F1: 0.810\n",
      "Valid - Loss: 0.940, F1: 0.635\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8431cbdec8d4faf955d2dcfdbfcaa4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.244, F1: 0.849\n",
      "Valid - Loss: 0.937, F1: 0.629\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd223956d3b94c3abecff3ca345e5c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.188, F1: 0.884\n",
      "Valid - Loss: 0.859, F1: 0.654\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe30dde58f344b4e83cb8c95513382fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.147, F1: 0.900\n",
      "Valid - Loss: 0.931, F1: 0.660\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68768c2e7ae4319a7554d7dfa925378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.118, F1: 0.926\n",
      "Valid - Loss: 0.955, F1: 0.687\n"
     ]
    }
   ],
   "source": [
    "model_bi = RNNULMFitClassifier(len(word2id), emb_dim=100, hidden_size=128,\n",
    "                               output_dim=len(label2id), bidirectional=True)\n",
    "model_bi = model_bi.to(device)\n",
    "\n",
    "# Оптимизатор и функция потерь\n",
    "optimizer = optim.AdamW(model_bi.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# Обучаем\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    train_metrics = train_epoch(model_bi, train_loader, optimizer, criterion)\n",
    "    valid_metrics = evaluate(model_bi, valid_loader, criterion)\n",
    "\n",
    "    print(\n",
    "        f\"Train - Loss: {train_metrics['loss']:.3f}, F1: {train_metrics['f1']:.3f}\")\n",
    "    print(\n",
    "        f\"Valid - Loss: {valid_metrics['loss']:.3f}, F1: {valid_metrics['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7316ab18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report (Validation):\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "         Ценности       0.91      0.91      0.91        23\n",
      "        Экономика       0.72      0.90      0.80       239\n",
      "              Мир       0.81      0.76      0.78       410\n",
      "            Спорт       0.95      0.94      0.95       195\n",
      "           Бизнес       0.75      0.14      0.23        22\n",
      "           Россия       0.76      0.77      0.77       481\n",
      "              Дом       0.76      0.77      0.77        66\n",
      "         Из жизни       0.55      0.62      0.58        84\n",
      "      Путешествия       0.73      0.52      0.61        21\n",
      "         Культура       0.79      0.90      0.84       159\n",
      "    Культпросвет        0.00      0.00      0.00         1\n",
      "   Интернет и СМИ       0.78      0.55      0.65       132\n",
      "             Крым       0.00      0.00      0.00         2\n",
      "Силовые структуры       0.62      0.47      0.53        60\n",
      "   69-я параллель       1.00      0.50      0.67         4\n",
      "  Наука и техника       0.79      0.88      0.83       160\n",
      "      Бывший СССР       0.80      0.77      0.79       159\n",
      "\n",
      "         accuracy                           0.78      2218\n",
      "        macro avg       0.69      0.61      0.63      2218\n",
      "     weighted avg       0.78      0.78      0.77      2218\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/futyn/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/futyn/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/futyn/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': np.float64(0.9547217620743645),\n",
       " 'precision': np.float64(0.7039130603940047),\n",
       " 'recall': np.float64(0.6905786513734706),\n",
       " 'f1': np.float64(0.6867316981350401),\n",
       " 'accuracy': np.float64(0.7802951388888889)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Оцениваем итоговую модель\n",
    "evaluate(model_bi, valid_loader, criterion, print_report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb411884",
   "metadata": {},
   "source": [
    "Видно, что двунаправленная RNN в целом значительно выигрывает на этой задаче у однонаправленной. При этом обе модели существенно лучше справились, чем свёртки в предыдущем задании.\n",
    "\n",
    "Двунаправленная модель обучается медленнее с точки зрения времени и, видимо, требует больше памяти, однако лучше справляется с дисбалансом классов (что видно по лучшему значению Macro F1 и, например, по лучшему предсказанию очень редкого класса \"69-я параллель\"), ну и быстрее приходит к лучшим метрикам с точки зрения количества эпох. Однако однонаправленная модель меньше переобучается и более стабильная и сбалансированная: precision и recall почти на одном уровне, тогда как двунаправленная модель более precision-biased, то есть более консервативна."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5d6eea",
   "metadata": {},
   "source": [
    "## Задание 2 (6 баллов)\n",
    "\n",
    "На данных википедии (wikiann) обучите и сравните 3 модели:  \n",
    "1) модель в которой как минимум два рекуррентных слоя, причем один из них GRU, а другой LSTM \n",
    "2) модель в которой как минимум 3 рекуррентных слоя идут друг за другом и при этом 2-ой и 3-й слои еще имеют residual connection к изначальным эмбедингам. Для того, чтобы сделать residual connection вам нужно будет использовать одинаковую размерность эмбедингов и количество unit'ов в RNN слоях, чтобы их можно было просуммировать \n",
    "3) модель в которой будут и рекуррентные и сверточные слои (как минимум 2 rnn и как минимум 2 cnn слоя). В cnn слоях будьте аккуратны с укорачиванием последовательности и используйте паддинг\n",
    "\n",
    "\n",
    "\n",
    "Сравните качество по метрикам (точность/полнота/f-мера). Также придумайте несколько сложных примеров и проверьте, какие сущности определяет каждая из моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a588de09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd1e14710f247b2b96800534a51a984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823af987",
   "metadata": {},
   "source": [
    "### 1. Загружаем данные и делаем словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2727cca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikiann\", 'ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23ec9d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 5445\n",
      "Labels: {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6}\n"
     ]
    }
   ],
   "source": [
    "vocab = Counter()\n",
    "for sent in dataset['train']['tokens']:\n",
    "    vocab.update(sent)\n",
    "\n",
    "# Немного почистим, оставляем слова, встретившиеся более 2 раз\n",
    "filtered_vocab = {word for word, count in vocab.items() if count > 2}\n",
    "\n",
    "word2id = {'PAD': 0, 'UNK': 1}\n",
    "for word in filtered_vocab:\n",
    "    word2id[word] = len(word2id)\n",
    "\n",
    "# Теги из датасета (IOB)\n",
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-PER\",\n",
    "    2: \"I-PER\",\n",
    "    3: \"B-ORG\",\n",
    "    4: \"I-ORG\",\n",
    "    5: \"B-LOC\",\n",
    "    6: \"I-LOC\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "print(f\"Vocab size: {len(word2id)}\")\n",
    "print(f\"Labels: {label2id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dc99bc",
   "metadata": {},
   "source": [
    "### 2. Датасет с паддингом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39903cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_split, word2id, max_len=100):\n",
    "        self.dataset = dataset_split\n",
    "        self.word2id = word2id\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tokens = self.dataset[index]['tokens']\n",
    "        tags = self.dataset[index]['ner_tags']\n",
    "\n",
    "        # Конвертируем токены в индексы\n",
    "        ids = [self.word2id.get(t, 1) for t in tokens][:self.max_len]\n",
    "        # Теги тоже обрезаем\n",
    "        labels = tags[:self.max_len]\n",
    "\n",
    "        # Паддинг\n",
    "        pad_len = self.max_len - len(ids)\n",
    "        if pad_len > 0:\n",
    "            ids = ids + [0] * pad_len\n",
    "            # Используем -100 для паддинга меток, чтобы loss их игнорировал\n",
    "            labels = labels + [-100] * pad_len\n",
    "\n",
    "        return torch.LongTensor(ids), torch.LongTensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "956afe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 64\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train_data = NERDataset(dataset['train'], word2id, MAX_LEN)\n",
    "valid_data = NERDataset(\n",
    "    dataset['validation'],\n",
    "    word2id,\n",
    "    MAX_LEN)  # Validation split\n",
    "test_data = NERDataset(dataset['test'], word2id, MAX_LEN)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063e3b0e",
   "metadata": {},
   "source": [
    "### 3. Модифицированный для NER цикл обучения\n",
    "\n",
    "В основном изменения касаются подсчёта метрик, так как теперь мы предсказываем метку для каждого слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aca6a98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ner_epoch(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    metrics = {\n",
    "        'loss': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': [],\n",
    "        'accuracy': []}\n",
    "\n",
    "    for texts, labels in tqdm(iterator, desc=\"Training\", leave=False):\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model(texts)  # [Batch, Seq, Classes]\n",
    "\n",
    "        # Переставляем размерности для CrossEntropyLoss\n",
    "        predictions_flat = predictions.view(-1, predictions.shape[-1])\n",
    "        labels_flat = labels.view(-1)\n",
    "\n",
    "        loss = criterion(predictions_flat, labels_flat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Метрики считаем без паддинга\n",
    "        mask = labels_flat != -100\n",
    "        preds = predictions_flat.argmax(1)[mask].cpu().numpy()\n",
    "        y_true = labels_flat[mask].cpu().numpy()\n",
    "\n",
    "        metrics['loss'].append(loss.item())\n",
    "        metrics['precision'].append(\n",
    "            precision_score(\n",
    "                y_true,\n",
    "                preds,\n",
    "                average='macro',\n",
    "                zero_division=0))\n",
    "        metrics['recall'].append(\n",
    "            recall_score(\n",
    "                y_true,\n",
    "                preds,\n",
    "                average='macro',\n",
    "                zero_division=0))\n",
    "        metrics['f1'].append(\n",
    "            f1_score(\n",
    "                y_true,\n",
    "                preds,\n",
    "                average='macro',\n",
    "                zero_division=0))\n",
    "        metrics['accuracy'].append(accuracy_score(y_true, preds))\n",
    "\n",
    "    return {k: np.mean(v) for k, v in metrics.items()}\n",
    "\n",
    "\n",
    "def evaluate_ner(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    metrics = {\n",
    "        'loss': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': [],\n",
    "        'accuracy': []}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in iterator:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            predictions = model(texts)\n",
    "\n",
    "            predictions_flat = predictions.view(-1, predictions.shape[-1])\n",
    "            labels_flat = labels.view(-1)\n",
    "\n",
    "            loss = criterion(predictions_flat, labels_flat)\n",
    "\n",
    "            mask = labels_flat != -100\n",
    "            preds = predictions_flat.argmax(1)[mask].cpu().numpy()\n",
    "            y_true = labels_flat[mask].cpu().numpy()\n",
    "\n",
    "            metrics['loss'].append(loss.item())\n",
    "            metrics['precision'].append(\n",
    "                precision_score(\n",
    "                    y_true,\n",
    "                    preds,\n",
    "                    average='macro',\n",
    "                    zero_division=0))\n",
    "            metrics['recall'].append(\n",
    "                recall_score(\n",
    "                    y_true,\n",
    "                    preds,\n",
    "                    average='macro',\n",
    "                    zero_division=0))\n",
    "            metrics['f1'].append(\n",
    "                f1_score(\n",
    "                    y_true,\n",
    "                    preds,\n",
    "                    average='macro',\n",
    "                    zero_division=0))\n",
    "            metrics['accuracy'].append(accuracy_score(y_true, preds))\n",
    "\n",
    "    return {k: np.mean(v) for k, v in metrics.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30155f36",
   "metadata": {},
   "source": [
    "### 4. Модель 1: GRU + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8ebd7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRNN(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size,\n",
    "            emb_dim,\n",
    "            hidden_size,\n",
    "            output_dim,\n",
    "            dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # 1 слой - двунаправленный GRU\n",
    "        self.gru = nn.GRU(\n",
    "            emb_dim,\n",
    "            hidden_size,\n",
    "            batch_first=True,\n",
    "            bidirectional=True)\n",
    "\n",
    "        # 2 слой - двунаправленный LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            hidden_size * 2,\n",
    "            hidden_size,\n",
    "            batch_first=True,\n",
    "            bidirectional=True)\n",
    "\n",
    "        # Полносвязный слой\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "\n",
    "        # Проход через GRU\n",
    "        gru_out, _ = self.gru(embedded)\n",
    "\n",
    "        # Проход через LSTM\n",
    "        lstm_out, _ = self.lstm(gru_out)\n",
    "\n",
    "        # Предсказание для каждого токена\n",
    "        return self.fc(self.dropout(lstm_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a41719",
   "metadata": {},
   "source": [
    "### 5. Модель 2: Residual RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bee965d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, output_dim, dropout=0.3):\n",
    "        super().__init__()\n",
    "        # Размер эмбеддинга равен размеру скрытого слоя для сложения\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # 3 последовательных однонаправленных слоя\n",
    "        self.rnn1 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.rnn2 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.rnn3 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # Исходные эмбеддинги\n",
    "        initial_emb = self.dropout(self.embedding(text))\n",
    "\n",
    "        # 1 слой: обычный проход\n",
    "        out1, _ = self.rnn1(initial_emb)\n",
    "\n",
    "        # 2 слой: вход + исходный эмбеддинг (Residual connection)\n",
    "        res_input2 = out1 + initial_emb\n",
    "        out2, _ = self.rnn2(res_input2)\n",
    "\n",
    "        # 3 слой: вход + исходный эмбеддинг\n",
    "        res_input3 = out2 + initial_emb\n",
    "        out3, _ = self.rnn3(res_input3)\n",
    "\n",
    "        return self.fc(self.dropout(out3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e601c8",
   "metadata": {},
   "source": [
    "### 6. Модель 3: CNN + RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab4556cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNRNNModel(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size,\n",
    "            emb_dim,\n",
    "            hidden_size,\n",
    "            output_dim,\n",
    "            dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # CNN слои\n",
    "        self.conv1 = nn.Conv1d(emb_dim, hidden_size, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            hidden_size,\n",
    "            hidden_size,\n",
    "            kernel_size=3,\n",
    "            padding=1)\n",
    "\n",
    "        # RNN слои\n",
    "        self.rnn1 = nn.LSTM(\n",
    "            hidden_size,\n",
    "            hidden_size,\n",
    "            batch_first=True,\n",
    "            bidirectional=True)\n",
    "        self.rnn2 = nn.LSTM(\n",
    "            hidden_size * 2,\n",
    "            hidden_size,\n",
    "            batch_first=True,\n",
    "            bidirectional=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "\n",
    "        x = embedded.permute(0, 2, 1)\n",
    "\n",
    "        # CNN блок\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "\n",
    "        # Возвращаем размерность для RNN\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # RNN блок\n",
    "        x, _ = self.rnn1(x)\n",
    "        x, _ = self.rnn2(x)\n",
    "\n",
    "        return self.fc(self.dropout(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcdb640",
   "metadata": {},
   "source": [
    "### 7. Обучение всех моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb554a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e0c7af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training GRU+LSTM...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09680873ec4545c8a631b874f2054d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train F1: 0.372 | Val F1: 0.608 | Val Acc: 0.779\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f9d1eed8b14c5d95fc9ed1d6f41aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train F1: 0.668 | Val F1: 0.730 | Val Acc: 0.836\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177349f284a64ad786862a742c534dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train F1: 0.734 | Val F1: 0.763 | Val Acc: 0.854\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58dd978b0e5743c1b6d5f1afb11e1908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train F1: 0.763 | Val F1: 0.779 | Val Acc: 0.864\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63da29ef148a4ca8a307b717d3665bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train F1: 0.784 | Val F1: 0.794 | Val Acc: 0.874\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d4a2f5edc5455bbbde2dd53d8de8c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train F1: 0.801 | Val F1: 0.803 | Val Acc: 0.877\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf17fcb3e59449b918bf7e91540dbe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train F1: 0.816 | Val F1: 0.806 | Val Acc: 0.882\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b953804da9ba4cdb8e322c0d542e48c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train F1: 0.829 | Val F1: 0.815 | Val Acc: 0.886\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f901bac07665457cb57e0e557d104130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train F1: 0.835 | Val F1: 0.818 | Val Acc: 0.888\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab93b523b6443b1be708d262b49ca6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train F1: 0.845 | Val F1: 0.824 | Val Acc: 0.890\n",
      "\n",
      "Final Report for GRU+LSTM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.92      0.97      0.94     40499\n",
      "       B-PER       0.93      0.83      0.88      3543\n",
      "       I-PER       0.94      0.90      0.92      7544\n",
      "       B-ORG       0.76      0.65      0.70      4074\n",
      "       I-ORG       0.78      0.82      0.80      8008\n",
      "       B-LOC       0.83      0.74      0.78      4560\n",
      "       I-LOC       0.87      0.73      0.80      3060\n",
      "\n",
      "    accuracy                           0.89     71288\n",
      "   macro avg       0.86      0.81      0.83     71288\n",
      "weighted avg       0.89      0.89      0.89     71288\n",
      "\n",
      "\n",
      "Training Residual RNN...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85423adc632a47f9b743a59c1f47712f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train F1: 0.264 | Val F1: 0.431 | Val Acc: 0.698\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "234f271cc5e94523b4c1a81fc99cbc7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train F1: 0.477 | Val F1: 0.565 | Val Acc: 0.750\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38cd3c2779d45a4bdd3d0a4e0daea59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train F1: 0.562 | Val F1: 0.609 | Val Acc: 0.773\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146ec793471d43e58c6c54cc960c32f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train F1: 0.604 | Val F1: 0.642 | Val Acc: 0.789\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "149be12e098d44b18f6af3de8ebf5b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train F1: 0.635 | Val F1: 0.665 | Val Acc: 0.800\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c00e8ae858343fa8b2c991481bbb9a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train F1: 0.656 | Val F1: 0.671 | Val Acc: 0.808\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d6f671175b4e0c87f346150ac6915b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train F1: 0.674 | Val F1: 0.689 | Val Acc: 0.817\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a06b4cac88054ed9ba9163a401dc850f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train F1: 0.690 | Val F1: 0.697 | Val Acc: 0.822\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc24f18f94be4dc29e573de5b7094660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train F1: 0.700 | Val F1: 0.708 | Val Acc: 0.827\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8178ab3eb384682bcba4985a43a87d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train F1: 0.711 | Val F1: 0.713 | Val Acc: 0.831\n",
      "\n",
      "Final Report for Residual RNN:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.87      0.95      0.91     40499\n",
      "       B-PER       0.51      0.81      0.62      3543\n",
      "       I-PER       0.92      0.86      0.89      7544\n",
      "       B-ORG       0.67      0.38      0.48      4074\n",
      "       I-ORG       0.77      0.71      0.74      8008\n",
      "       B-LOC       0.81      0.47      0.59      4560\n",
      "       I-LOC       0.87      0.63      0.73      3060\n",
      "\n",
      "    accuracy                           0.83     71288\n",
      "   macro avg       0.77      0.69      0.71     71288\n",
      "weighted avg       0.83      0.83      0.82     71288\n",
      "\n",
      "\n",
      "Training CNN + RNN...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8ce2f6d454451583112c48c8cc848f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train F1: 0.224 | Val F1: 0.502 | Val Acc: 0.738\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "befc5a0a240d49bba1ea6ff9545d7c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train F1: 0.631 | Val F1: 0.706 | Val Acc: 0.822\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993871db695b4effba69a7cc20c40387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train F1: 0.715 | Val F1: 0.737 | Val Acc: 0.839\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f56879eb9a849fbaa086fd681b8de33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train F1: 0.747 | Val F1: 0.765 | Val Acc: 0.857\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcbe534a55a844c58e2ce26a1a51e29b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train F1: 0.765 | Val F1: 0.777 | Val Acc: 0.865\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e8db30d8b0440ca77fb77b8d6d1325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train F1: 0.786 | Val F1: 0.786 | Val Acc: 0.864\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86d91679029d4efa80daefaa799ca8c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train F1: 0.796 | Val F1: 0.796 | Val Acc: 0.875\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2731121b998f424ca9790a5fe1d80c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train F1: 0.809 | Val F1: 0.800 | Val Acc: 0.878\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e84f4e692144c48bfffdefd8075dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train F1: 0.818 | Val F1: 0.807 | Val Acc: 0.881\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c144cbc28974fd38e87fd47afd86edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train F1: 0.827 | Val F1: 0.811 | Val Acc: 0.883\n",
      "\n",
      "Final Report for CNN + RNN:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.92      0.96      0.94     40499\n",
      "       B-PER       0.90      0.80      0.85      3543\n",
      "       I-PER       0.94      0.89      0.91      7544\n",
      "       B-ORG       0.70      0.63      0.66      4074\n",
      "       I-ORG       0.75      0.81      0.78      8008\n",
      "       B-LOC       0.83      0.71      0.76      4560\n",
      "       I-LOC       0.81      0.75      0.78      3060\n",
      "\n",
      "    accuracy                           0.88     71288\n",
      "   macro avg       0.84      0.79      0.81     71288\n",
      "weighted avg       0.88      0.88      0.88     71288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models_to_train = {\n",
    "    \"GRU+LSTM\": HybridRNN(len(word2id), 128, 128, len(label2id)),\n",
    "    \"Residual RNN\": ResidualRNN(len(word2id), 128, len(label2id)),\n",
    "    \"CNN + RNN\": CNNRNNModel(len(word2id), 128, 128, len(label2id))\n",
    "}\n",
    "\n",
    "for name, model in models_to_train.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_metrics = train_ner_epoch(\n",
    "            model, train_loader, optimizer, criterion)\n",
    "        valid_metrics = evaluate_ner(model, valid_loader, criterion)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: \"\n",
    "              f\"Train F1: {train_metrics['f1']:.3f} | \"\n",
    "              f\"Val F1: {valid_metrics['f1']:.3f} | \"\n",
    "              f\"Val Acc: {valid_metrics['accuracy']:.3f}\")\n",
    "\n",
    "    print(f\"\\nFinal Report for {name}:\")\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in test_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            preds = model(texts).argmax(2).view(-1).cpu().numpy()\n",
    "            targets = labels.view(-1).cpu().numpy()\n",
    "\n",
    "            mask = targets != -100\n",
    "            all_preds.extend(preds[mask])\n",
    "            all_true.extend(targets[mask])\n",
    "\n",
    "    print(\n",
    "        classification_report(\n",
    "            all_true,\n",
    "            all_preds,\n",
    "            target_names=[\n",
    "                id2label[i] for i in range(\n",
    "                    len(id2label))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb10a1cf",
   "metadata": {},
   "source": [
    "По метрикам лидирует первая модель (GRU+LSTM). Она же и быстрее всех стартует во время обучения, хотя CNN+RNN в дальнейшем обучается быстрее, выходя на ту же F-меру на валидации раньше. Что касается Residual RNN, у модели, похоже, сложности с обучением, параметры оптимизируются очень медленно, и того же качества она достигает уже в самом конце. Признаков переобучения ни одна модель почти не демонстрирует."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372dde87",
   "metadata": {},
   "source": [
    "### 8. Функция инференса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfccd2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def predict_ner(text, model, word2id, id2label):\n",
    "    model.eval()\n",
    "    # Простая токенизация\n",
    "    tokens = re.findall(r'\\w+|[^\\w\\s]+', text)\n",
    "    ids = [word2id.get(t, 1) for t in tokens]\n",
    "    tensor_ids = torch.LongTensor([ids]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(tensor_ids)\n",
    "        preds = logits.argmax(2)[0].cpu().tolist()\n",
    "\n",
    "    result = []\n",
    "    for t, p in zip(tokens, preds):\n",
    "        result.append(f\"{t}({id2label[p]})\")\n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d879c537",
   "metadata": {},
   "source": [
    "### 9. Тестируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "874ae042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model: GRU+LSTM ---\n",
      "Я(O) написал(O) письмо(O) Татьяне(O) Владиславовне(I-PER) ,(O) менеджеру(O) ВШЭ(I-PER) .(O)\n",
      "На(O) Новый(O) год(O) мы(O) в(O) Питере(O) отметили(O) Рождество(O) в(O) баре(O) «(O) Кибер(O) Ель(O) ».(O)\n",
      "Вашингтон(O) объявил(O) новые(O) санкции(O) против(O) Москвы(B-ORG) .(O)\n",
      "Отзывы(O) студентов(O) о(O) питерской(O) Вышке(O) мне(O) не(O) нравятся(O) .(O)\n",
      "\n",
      "--- Model: Residual RNN ---\n",
      "Я(B-PER) написал(O) письмо(O) Татьяне(O) Владиславовне(O) ,(O) менеджеру(O) ВШЭ(O) .(O)\n",
      "На(O) Новый(O) год(O) мы(O) в(O) Питере(O) отметили(O) Рождество(O) в(O) баре(O) «(O) Кибер(B-ORG) Ель(I-ORG) ».(I-ORG)\n",
      "Вашингтон(B-LOC) объявил(I-PER) новые(O) санкции(O) против(O) Москвы(B-LOC) .(O)\n",
      "Отзывы(B-PER) студентов(O) о(O) питерской(O) Вышке(O) мне(O) не(O) нравятся(O) .(O)\n",
      "\n",
      "--- Model: CNN + RNN ---\n",
      "Я(O) написал(O) письмо(O) Татьяне(O) Владиславовне(O) ,(O) менеджеру(O) ВШЭ(O) .(O)\n",
      "На(O) Новый(B-ORG) год(I-ORG) мы(I-ORG) в(O) Питере(O) отметили(O) Рождество(O) в(O) баре(O) «(O) Кибер(O) Ель(O) ».(O)\n",
      "Вашингтон(O) объявил(O) новые(O) санкции(B-ORG) против(I-ORG) Москвы(I-ORG) .(I-ORG)\n",
      "Отзывы(O) студентов(O) о(O) питерской(O) Вышке(I-ORG) мне(I-ORG) не(O) нравятся(O) .(O)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    \"Я написал письмо Татьяне Владиславовне, менеджеру ВШЭ.\",\n",
    "    \"На Новый год мы в Питере отметили Рождество в баре «Кибер Ель».\",\n",
    "    \"Вашингтон объявил новые санкции против Москвы.\",\n",
    "    \"Отзывы студентов о питерской Вышке мне не нравятся.\"\n",
    "]\n",
    "\n",
    "for name, model in models_to_train.items():\n",
    "    print(f\"--- Model: {name} ---\")\n",
    "    for ex in examples:\n",
    "        print(predict_ner(ex, model, word2id, id2label))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7fe858",
   "metadata": {},
   "source": [
    "А вот тут всё очень грустно, все модели отперформили прям плохо. Видимо, потому что предложения максимально не соответствуют тому, что представлено в обучающем датасете — по стилю и названиям (это даже не новости). Но если наши лучшие по метрикам модели (GRU+LSTM и CNN+RNN) скорее консервативные и пропускают названия, то третья (худшая по метрикам) модель наоборот гмассово галлюцинирует их там, где не надо.\n",
    "\n",
    "В целом, для NER как будто либо правила, либо трансформеры, середина тут так себе работает."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
